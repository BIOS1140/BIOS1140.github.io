[["index.html", "R tutorials for the course BIOS1140 at the University of Oslo Preface", " R tutorials for the course BIOS1140 at the University of Oslo Original tutorials written by Mark Ravinet. Revamped in 2021 by Even Sletteng Garvang and Solveig Brochmann. Revamped again in 2024 by Emily Enevoldsen and Marius F. Maurstad 2024-06-28 Preface Effectively handling, processing and analysing large amounts of data is an essential skill for modern evolutionary biologists. Large genomic and phenotypic datasets are now routine for the biosciences and we are no longer at point where a simple desktop program can suit our needs for data curation, statistical analysis and visualisation. To meet the challenge, it is necessary for biologists to learn how to program and the fundamentals of data science. As well as this, exploring, visualising and understanding data in a programming environment can help reinforce understanding of key concepts and mathematical or statistical relationships. These reasons are the motivation for the online sections of our book, Evolutionary Genetics: Concepts, Analysis &amp; Practice. Each of the ten tutorials hosted here are self-contained introductions to key concepts in evolutionary genetics and they are also designed to familiarise you with the basics of the R programming language. Each tutorial comes with a set of study questions which you can use to reflect on your learning and of course, we also provide the answers for you to check your work against. The first two tutorials (Chapters 1 &amp; 2) are genetics-free, providing an introduction to R and also the tidyverse approach. They are intended for as wide an audience as possible. We hope they will be of use to biologists and non-biologists alike. Mark Ravinet &amp; Glenn-Peter Sætre Oslo, October 2018 2021 version This version is a revamped version of Mark Ravinet’s original tutorials, edited by Even Sletteng Garvang and Solveig Brochmann based on the feedback during the three years the course has been run at the University of Oslo. Even contributed with new code, revised sections and wrote a new tutorial for chapter 1. Solveig contributed with edits and feedback, and revised sections. In 2024 the tutorials were shortened to 7 assignments by Emily Enevoldsen and Marius F. Maurstad. The code used to create this book can be found at https://github.com/BIOS1140/BIOS1140.github.io. The R and package versions used can be found in Appendix B. "],["how-to-use-these-tutorials.html", "How to use these tutorials", " How to use these tutorials To get the most out of these tutorials, you should run all the provided code in R on your own computer while going through them. Rather than just copy/pasting all the code, we encourage you to write the code in manually. A great way to force yourself to do this is by changing some object names along the way. We also highly encourage you to experiment with the provided code. Please do change function arguments, values etc. to see what happens. If you ever think: “what happens if I…”, try it! The worst thing that can happen is that you get an error. During the tutorials, there are small exercises that we recommend that you do for your own understanding. Each tutorial also have assignments tied to what you have learned. If you are following the course BIOS1140 at UiO, be sure to deliver these assignments before the deadline. See the appendix for how you should deliver the assignments. We use colour-coded boxes in the tutorials to summarise information and give additional tips. In addition, some code may be too advanced for an introductory course in R, and will also be wrapped in coloured boxes. The colors are explained below. Important concept: Green boxes summarises important concepts from the text. Additional info Blue boxes contain small tips and additional information for those that are interested. What’s in these is hopefully useful, but not mandatory. Feel free to skip these if you want. Advanced code Yellow boxes contain code that you need to run in order to complete the tutorial, but that you don’t necessarily need to understand. Text in bold contains small exercises to do on your own throughout the tutorials. These are for your own understanding only, so you don’t need to hand them in. "],["ch01.html", "Week 1 Introduction to R", " Week 1 Introduction to R For modern evolutionary biologists, handling large amounts of data is a fundamental skill. Familiarity with a programming language, particularly one that makes it straightforward to visualise, explore and filter data is the best way to achieve this ability. There are many different types of programming and scripting languages; the entire concept may seem daunting at first, especially if you have never encountered it before. This is natural, many other biologists applying scripting tools on a daily basis have started from similar first principles. A little patience with the basics of any form of programming and you will soon be able to do much more than you thought possible. For this section, we will be introducing you to R, a statistical programming language and environment that is widely used in the biological sciences. R is flexible, clear and easy to learn. It also extremely good for producing quick, high quality visualisations of data, which makes it very useful for anyone trying to explore their data. Perhaps the greatest strength of R is its focus on statistics - this makes it an excellent tool for carrying out and learning statistical analysis. R is also used for data analysis beyond evolutionary biology - it forms the basis of data science for companies such as Google, Facebook and Twitter, among many others. If you find yourself wondering why you are learning a programming language, it is worth remembering this point - familiarity with R and scripting will provide you with a very flexible and useful skill. We believe the best way to get an idea of what R is and what it is capable of is to dive straight in. There really is no better way to learn and understand than to demonstrate the workings of the language to yourself. As a brief overview, we will show the utmost basics here before moving onto more advanced topics in the next chapter. We will also introduce some basic statistical concepts for which R makes visualisation and understanding straightforward. Together these first two chapters will form the foundations for applying R to more evolutionary genetics focused questions. What to expect In this section we are going to: learn what R is, and start using R as a tool understand how to handle data structures in R get familiar with plotting and visualising data handle datasets and reading in data loading packages "],["start-using-r.html", "1.1 Start using R!", " 1.1 Start using R! 1.1.1 Install R and Rstudio To begin, you should download and install R from the CRAN. This is the online hub for the R language and it stands for Comprehensive R Archive Network. Be sure to download the correct R installation for your operating system. We also strongly recommend you install RStudio, a front-end for R. This utility makes working in the R environment a lot more straightforward, standardises things across operating systems and has many helpful features. For the purposes of these tutorials, we will assume you are using RStudio. With both R and RStudio installed, start Rstudio and we will begin! 1.1.2 Getting familiar with R and the console 1.1.2.1 R as a calculator In the bottom left you will see a &gt; where you can input text. This is called the console, and you can directly interact with R through this. Try inputting a number, or any calculation you can think of, and then press enter. For example: 4 4+3 11-5 5*2 3/2 10^2 You will see that if you input a single number, R will return that same number to you. If you input a calculation, R will return the result of that calculation. You can also use parentheses as you would on a normal scientific calculator: 10*3+4 #&gt; [1] 34 10*(3+4) #&gt; [1] 70 You have now learned how R can be used as a calculator! 1.1.2.2 Text input If you input text into the console, however, you will get the following: hello #&gt; Error in eval(expr, envir, enclos): object &#39;hello&#39; not found The error message reason for this will be apparent later in this tutorial. To get R to return text, you have to enclose the text in single or double quotes (\" or ') like so: &quot;hello&quot; #&gt; [1] &quot;hello&quot; &#39;hello&#39; #&gt; [1] &quot;hello&quot; Important concept: To make R interpret what you write as text, you have to enclose it with single or double quotes: \"hello\" or 'hello' rather than just hello Error messages: Above, you got an error message saying “Error: object ‘hello’ not found”. Errors are common in R, and are nothing to be afraid of. The worst thing that can happen from an error in R is that you have to fix your code and re-run it. You should get used to reading error messages, they are often helpful to understand what is wrong with your code. They can also be extremely unhelpful, but even then, googling the error message can give you some insight into what went wrong. 1.1.2.3 Comments If you input a #, R will ignore whatever comes after on the same line. This means you can write entire lines of comments, or write comments after your calculations: # This entire line is a comment, and that is fine! 6 + 4 # add a meaningful comment on why you&#39;re doing this Comment your code liberally, so it becomes easier to understand for any person reading your code. That person is often your future self, and you’d be surprised to learn how unreadable uncommented code can be, even if you wrote it yourself! Fortunately, you will never end up in that situation, since from now on you will be commenting everything you do (because I said so). 1.1.2.4 Some additional tips about the console use the up and down arrows to cycle through your previous commands if the &gt; in the console turns into a +, it means that you probably forgot to close a parenthesis or a quote (try e.g. running \"hello). You will be unable to do anything while the console is behaving like this. Press the Esc button to get your familiar &gt; back and continue working. Spaces generally don’t mean anything when doing calculations, which means 4+3 is equivalent with 4 + 3. Use spaces to make your code easier to read. 1.1.3 Scripts Doing small operations in the console is all well and good, but once you’ve done something a bit more complex than adding numbers you will want to save what you did in some way. You could use the up-arrow in the console to recall the things you did (even if you close and reopen RStudio), but imagine if the thing you want to recall was a thousand operations ago. That’s a whole lot of button pressing! The most convenient way to store what you have done in R is by writing a script. A script in R is just a text document containing R code, and the file extension is .R (your script can e.g. be called “myscript.R”. In RStudio, you can create a new script either from the menu File &gt; New File &gt; R script or by pressing ctrl+shift+N on Windows or command+shift+N on Mac. To save your script, use ctrl/command+S, or choose save/save as... from the File dropdown menu. Be concious about where you save your script so that you will find it again later! Working in a script is almost exactly the same as working in the console: You write a line of code in the script, run it, and the result is shown in the console. The main difference is that you have to press ctrl/command+enter to run code from a script, rather than just enter. The other big difference is that you can save your scripts, so you have access to all the code you have previously written. For this reason, you should always work in a script rather than in the console. Exercise: Create a new script, and save it with a meaningful name (e.g. “BIOS1140_week01.R” in a folder named BIOS1140). Run the commands you ran in the console earlier, but this time from your script instead. Remember to comment your code with # as you go along! Whenever you start a new project, create a script for that project. Save your script in a meaningful location with a meaningful name so it will be easy to find later. "],["r-essentials.html", "1.2 R essentials", " 1.2 R essentials In this part of the tutorial, we will learn the fundamentals of R programming, while investigating the demographics of the Nordic countries. All numbers are the 2020 populations taken from the International Data Base (IDB) from the US government. 1.2.1 Assigning to objects 1.2.1.1 Problem: numbers are complicated Let’s start by looking at Norway and Sweden. Say you want to find out what proportion the Norwegian population makes up of the total population of Norway plus Sweden. Conceptually, it looks something like this: \\[\\frac{Norwegian\\ population}{Norwegian\\ population+Swedish\\ population}\\] Exercise: The Norwegian population is 5 465 387 and the Swedish is 10 185 555. Use R to calculate the proportion the Norwegian population makes up of the total population. Do the same for the Swedish population. If you were able to do this, great work! Your solution probably looked something like this1: # Norwegian population 5465387 / (5456387 + 10185555) # Swedish population 10185555 / (5465387 + 10185555) However, there are several problems with doing it this way. First of all, 5465387 is a stupidly long number, and the probability of typing it wrong is rather high (I actually mistyped a number in the code above, can you spot the error?). Another problem is that if any of the populations change (e.g. you want to update to 2021 numbers), you would have to update it in four different places to make this simple change. Third: for someone looking at your code, how will they know what’s happening? These numbers have no meaning without their context. Now we’re finally getting to the first taste of why R is more powerful than a regular calculator: we can give our numbers simple names. 1.2.1.2 Solution: assign the complicated numbers to named objects What if you could write some text instead of our stupidly large numbers? Something like norway_pop instead of 5465387? Luckily, you can! You can do the following to give your variables a name: # store the number to an object called norway_pop norway_pop &lt;- 5465387 # store another number to an object called sweden_pop sweden_pop &lt;- 10185555 This is called assignment, and is done using the arrow &lt;-2. You have now created a named object, which is a very powerful tool. Your objects behave exactly like the numbers that are stored in them (try e.g. norway_pop*2 or sweden_pop+4445). This means that we can now simply write: norway_pop / (norway_pop + sweden_pop) #&gt; [1] 0.349205 sweden_pop / (norway_pop + sweden_pop) #&gt; [1] 0.650795 Notice how similar this is to the conceptual version we saw earlier! This is much simpler, easier to read and way less error-prone than writing out the numbers each time. In addition, if you want to change any of the population sizes, you will just have to change it in one place instead of four. To make things even easier, we could store norway_pop + sweden_pop as total_pop, and also store our results with a name as well: # make total population object total_pop &lt;- norway_pop + sweden_pop # make object for Norway&#39;s proportion norway_proportion &lt;- norway_pop / total_pop # print the result norway_proportion #&gt; [1] 0.349205 # make object for Sweden&#39;s population sweden_proportion &lt;- sweden_pop / total_pop # print the result sweden_proportion #&gt; [1] 0.650795 Additional info If you have experience with Python, you would have printed the objects using print(norway_proportion). In R we don’t have to explicitly use print(). An object (or any calculation for that matter) is automatically printed when we run it. R does have a print() function, though, that can be used if you want to be explicit about printing something. 1.2.1.3 Notes about naming variables You should always give your variables sensible names. In the code above, we could have saved ourselves some typing by e.g. calling the populations x and y respectively. However, this quickly becomes a nightmare to read when scripts get long, so even though norway_pop takes longer to write than x, you should go with norway_pop so it’s possible to understand what’s going on in your script. You can name your variables just about anything, but there are some characters you can’t use in their names. Notably, spaces, dashes and a lot of special characters cant’ be used, and the name cannot start with a number. If you stick to letters, underscores and numbers (except in the beginning), you should be fine! Another thing to note about variable names is that they are case sensitive, for instance meaning that A can contain something completely different than a. In the code above, we used all-lowercase variable names, so what happens if we try to run the object Norway_pop3? Norway_pop #&gt; Error in eval(expr, envir, enclos): object &#39;Norway_pop&#39; not found R can’t find Norway_pop, because it doesn’t exist, only norway_pop does. Beware of this, as it’s often a source of annoying bugs in the code. Important concept: Assign any and all variables to named objects. It’s easier to read, and you’re less likely to make mistakes. Use good variable names, and beware of case sensitivity! 1.2.2 Vectors Let’s expand our little Norway-Sweden project to all the Nordic countries. Table 1.1 shows the populations of all the Nordic countries. Table 1.1: Population sizes of the nordic countries. Country Population Denmark 5868927 Finland 5572355 Iceland 350773 Norway 5465387 Sweden 10185555 Now, if we were to calculate the proportion of the total for all countries, like we did for Norway and Sweden earlier, the code would quickly become very long: denmark_pop &lt;- 5868927 finland_pop &lt;- 5572355 iceland_pop &lt;- 350773 nordic_total &lt;- norway_pop + sweden_pop + denmark_pop + finland_pop + iceland_pop norway_proportion &lt;- norway_pop / nordic_total sweden_proportion &lt;- sweden_pop / nordic_total denmark_proportion &lt;- denmark_pop / nordic_total finland_proportion &lt;- finland_pop / nordic_total iceland_proportion &lt;- iceland_pop / nordic_total norway_proportion sweden_proportion denmark_proportion finland_proportion iceland_proportion That is a lot of typing! And remember that this is only for the Nordic countries, imagine doing this for all the countries in the world! In R, we can avoid all this tedious work by storing our data in a vector 1.2.2.1 Store related data in a vector R has a data type called vector, which can store multiple values at once. In our case, this means that we can store the populations of all the Nordic countries in one vector. Let’s look at some different ways to make a vector. The following lines all create a vector containing the numbers 1 through 10. # create a vector with c() c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) # create a vector with : 1:10 # create a vector with seq() seq(from = 1, to = 10, by = 1) You can create a vector manually with c(). All your numbers (or other values) go inside the parentheses, separated by comma. You can create a vector with the colon operator :. This creates a vector containing all the numbers starting from the number before :, and up to and including the number after. You can create a vector with seq(). The numbers go inside the parentheses, separated by comma. The number after from = is the beginning of the vector, after to = is the end of the vector, and after by = is the distance between the values in the vector. As a side note, c() and seq() are what we call functions. We’ll get back to functions in section 1.2.5, so don’t worry if you don’t fully understand how these work yet. Like numbers, vectors can be assigned to objects. This works in exactly the same way, using the arrow &lt;-. arbitrary_numbers &lt;- c(56, 789, 109, 76) arbitrary_numbers #&gt; [1] 56 789 109 76 my_sequence &lt;- seq(from = 0, to = 60, by = 5) my_sequence #&gt; [1] 0 5 10 15 20 25 30 35 40 45 50 55 60 Exercise: Store the population sizes of the Nordic countries in a vector using c(). Input the numbers in alphabetical order of the countries (same as in table 1.1 above) Assign the vector to an object with a good name. nordic &lt;- c(5868927, 5572355, 350773, 5465387, 10185555) Additional information: Those of you who learned programming in BIOS1100 might remember that in python we mainly used lists when we wanted to store many elements in one variable. In R, we will mainly be using vectors. The two are similar in some ways, but there are also some differences between the two: To define a vector in R you need to use the c() function (or the other methods explained above), and with vectors you can perform mathematical operations on all the elements (more on this in the next section!). If you’re familiar with Numpy arrays in Python, vectors in R are quite similar to those. 1.2.2.2 Vector properties 1.2.2.2.1 Mathematical operations When you have a vector of numbers, you can apply all the same mathematical operations that you can on a single number. The operation is then applied to each element of the vector separately. Try it! # create a vector from 1 to 10 my_vector &lt;- 1:10 # multiply each element by 2 my_vector * 2 #&gt; [1] 2 4 6 8 10 12 14 16 18 20 # add 5 to each element my_vector + 5 #&gt; [1] 6 7 8 9 10 11 12 13 14 15 If you have two vectors of equal length, you can do mathematical operations on both, e.g. multiply two vectors. The first element of the first vector is then multiplied with the first element of the second vector, the second element with the second and so on. You can do the same for division, addition, subtraction and any operation you can think of. # create another vector my_vector2 &lt;- 11:20 # mutliply them together my_vector * my_vector2 #&gt; [1] 11 24 39 56 75 96 119 144 171 200 # add them my_vector + my_vector2 #&gt; [1] 12 14 16 18 20 22 24 26 28 30 If the two vectors aren’t of the same length, you will get a warning, but R will still try to perform the operation. The shorter vector will then start over when it runs out of numbers. # multiplying two vectors of unequal length 1:10 * 1:9 #&gt; [1] 1 4 9 16 25 36 49 64 81 10 # this is what R does: #1*1 #2*2 #3*3 #4*4 #5*5 #6*6 #7*7 #8*8 #9*9 # here we use the last element of the vector 1:9 #10*1 # here R &quot;recycles&quot; the vector, using the first element of 1:9 Remember to keep all your vectors the same length, or something unexpected might happen! To check the length of a vector, you can use the length() function. length(my_vector) #&gt; [1] 10 length(my_vector2) #&gt; [1] 10 Exercise: express the Nordic population sizes in millions by dividing all the numbers in the nordic vector by 10^6 nordic / 10^6 1.2.2.2.2 Extracting numbers Having all numbers stored in one place is great, but what if you want to use just one of them? We use square brackets [] to access numbers inside vectors. You put the index of the element you want to extract inside the square brackets like this: my_vector[1]. This will extract the first element from the vector, my_vector[2] will extract the second. On our nordic vector, we could for instance do the following: # extract the third element (Iceland) from the nordic vector nordic[3] #&gt; [1] 350773 We can also select multiple numbers, by inputting a vector inside the square brackets. nordic[c(3, 5)] will extract the third and the fifth element of ournordicvector, andnordic[1:3] will extract elements 1 through 3. Exercise: extract elements 2 through 4 from the nordic vector. Then, extract only the Scandinavian countries, and store them in an object with a good name. Vector indexing: If you’re familiar with Python or another programming language, you are probably used to that counting starts on 0 when indexing. In R, however, we start at 1. This can be annoying when switching between languages, but is unfortunately something you just need to remember. 1.2.2.2.3 Getting the sum and mean of a vector You can do a variety of operations on vectors in addition to using the mathemathical operators +, -, * and /. Two of the most common operations are calculating the sum and the mean of all the numbers in the vector. With the sum() and mean() functions. sum(my_vector) #&gt; [1] 55 mean(my_vector) #&gt; [1] 5.5 There’s a variety of other functions you can apply to a vector, such as max(), min() and median(). Try them out! Exercise: Use what you have learned about vectors to calculate the population proportions of all the Nordic countries in a single operation. Save it to an object with a good name. The solution to the exercise shows how powerful working with vectors can be. nordic_prop &lt;- nordic / sum(nordic) nordic_prop #&gt; [1] 0.21385882 0.20305198 0.01278188 0.19915416 0.37115316 This is way simpler than doing this without using vectors! Also, imagine if you had a hundred, or even a million values in your vector. The code would still look exactly the same, making calculations with any number of values trivial. Important concept If you have many values that go together, store them together in a vector. You can do a variety of mathematical operations on vectors, but make sure that all vectors have the same length! 1.2.3 Strings As mentioned way back in section 1.1.2.2, you have to write \"hello\" rather than hello to get the actual text “hello”. Now you may have figured out that this is because we have to separate objects from plain text in some way. Text within quotes in R (and any programming language) is called strings. These can be stored in objects and combined into vectors just like you can with numbers. &quot;Hello, world!&quot; #&gt; [1] &quot;Hello, world!&quot; greeting &lt;- &quot;Hello, world!&quot; greeting #&gt; [1] &quot;Hello, world!&quot; string_vector &lt;- c(&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;vector&quot;, &quot;of&quot;, &quot;strings!&quot;) string_vector #&gt; [1] &quot;this&quot; &quot;is&quot; &quot;a&quot; &quot;vector&quot; &quot;of&quot; &quot;strings!&quot; To combine several strings into one, or even combine numbers and strings, you can use the function paste(): paste(&quot;These two strings&quot;, &quot;become one&quot;) #&gt; [1] &quot;These two strings become one&quot; nordic_sum &lt;- sum(nordic) paste(&quot;The total population of the nordic countries is&quot;, nordic_sum, &quot;people.&quot;) #&gt; [1] &quot;The total population of the nordic countries is 27442997 people.&quot; Exercise: Combine the names of the Nordic countries into a vector. Make sure the names are in the same order as in table 1.1. nordic_names &lt;- c(&quot;Denmark&quot;, &quot;Finland&quot;, &quot;Iceland&quot;, &quot;Norway&quot;, &quot;Sweden&quot;) One of many uses for strings is to give names to a vector. You can use the names() function to see a vectors names. names(nordic) #&gt; NULL As you can see, we get NULL here, which means that the vector elements don’t have any names. We can set the names like this: names(nordic) &lt;- nordic_names nordic #&gt; Denmark Finland Iceland Norway Sweden #&gt; 5868927 5572355 350773 5465387 10185555 Now, when we print our nordic vector, we can see which population size belongs to which country! Neat! We can also extract values from our vector based on names rather than just position. nordic[&quot;Denmark&quot;] # equivalent to nordic[1] #&gt; Denmark #&gt; 5868927 nordic[c(&quot;Finland&quot;, &quot;Norway&quot;)] # equivalent to nordic[c(2, 4)] #&gt; Finland Norway #&gt; 5572355 5465387 Exercise: Do some calculations on the now named nordic vector (e.g. calculate the proportions again). What happens to the names? nordic / sum(nordic) #&gt; Denmark Finland Iceland Norway Sweden #&gt; 0.21385882 0.20305198 0.01278188 0.19915416 0.37115316 # The names carry over to the new vector 1.2.4 Logical values R has some words that have special meaning, two of these are what we call logical (or boolean) values: TRUE and FALSE. These are often the result of checking if some condition is true or not, using logical operators. The most important logical operators in R are larger than &gt;, smaller than &lt;, equal to ==, equal or larger/smaller &gt;=/&lt;= and not equal to !=. They return TRUE if the condition is true, and FALSE if the condition is false. In its simplest form, we can write: # is 3 smaller than 4? 3 &lt; 4 #&gt; [1] TRUE # is 3 exactly equal to 3.01? 3 == 3.01 #&gt; [1] FALSE # are these two strings the same? &quot;Norway&quot; == &quot;norway&quot; #&gt; [1] FALSE The operators also work on vectors. Then, every element of the vector is checked, and we get a vector of TRUE and FALSE. # which numbers from 1 to 5 are larger than 3? 1:5 &gt; 3 #&gt; [1] FALSE FALSE FALSE TRUE TRUE # which countries has a population larger than 5000000? nordic &gt; 5*10^6 #&gt; Denmark Finland Iceland Norway Sweden #&gt; TRUE TRUE FALSE TRUE TRUE # which countries in our vector are not named &quot;Norway&quot;? names(nordic) != &quot;Norway&quot; #&gt; [1] TRUE TRUE TRUE FALSE TRUE A neat property of logical values is that they behave as numbers in certain contexts. For example, if you use the sum() function on them, each TRUE is counted as 1, and each FALSE is counted as 0. sum(c(TRUE, FALSE, TRUE)) #&gt; [1] 2 We can use this to check how many elements in our vector match a certain condition: # how many numbers from 1 to 5 are larger than 3? sum(1:5 &gt; 3) #&gt; [1] 2 # how many countries has a population larger than 5000000? sum(nordic &gt; 5*10^6) #&gt; [1] 4 # how many countries in our vector are not named &quot;Norway&quot;? sum(names(nordic) != &quot;Norway&quot;) #&gt; [1] 4 Exercise: play around with the logical operators so you get a feel for how they work. Ask questions like: “is sum(3, 4) the same as 3+4?” or “which is larger, 10^6 or 6^10?”, and answer them using logical operators. 1.2.4.1 NA R also has another important special value, NA, which stands for “not available”. It is mostly used to indicate missing data in data sets. One important property of NA is that any operation involving NA returns NA. 5 * NA #&gt; [1] NA sum(c(5, 6, 10, NA, 1)) #&gt; [1] NA mean(c(3, 5, 10, NA)) #&gt; [1] NA You can write na.rm = TRUE within the sum() and mean() functions to ignore NAs. sum(c(5, 6, 10, NA, 1), na.rm = TRUE) #&gt; [1] 22 mean(c(3, 5, 10, NA), na.rm = TRUE) #&gt; [1] 6 Important concept: TRUE, FALSE and NA are examples of special values in R. TRUE and FALSE are called logical (or boolean) values, and are often the result of using logical operators like ==, &gt;, != etc. TRUE and FALSE behave as 1 and 0 respectively when used inside e.g. the sum() function. NA indicates missing data. Any operation involving NA returns NA 1.2.5 Functions So far you have seen a handful of functions being used, like c(), seq() and mean() to name some. Functions are always written in the form functionname(). The text outside the parentheses is the function name, and whatever is inside the parentheses are called arguments. A function can be seen as a series of operations that are applied to the arguments, or simply as somewhere you put something in, and get something else in return. They can often save you a tremendous amount of time, compare for example manually calculating the mean of a 1000 numbers vs. using the mean() function. If you want to know more about what a function does, you can write ? and then the function name, for example ?seq. Then you will get a help page that tells you more about how the function works, and some examples of use in the bottom. Be aware that the help page is written by programmers for programmers, and is often difficult to understand. In the beginning you will often be better off googling what a function does (but the examples are always useful). Functions mostly have 1 or more arguments (sometimes 0), which go inside of the parentheses separated by comma, conceptually: function(arg1, arg2, arg3). The arguments can either be input in a set order, or you can name them. Consider the following: seq(1, 10, 2) # &quot;from&quot; is argument no. 1, &quot;to&quot; is argument no. 2 etc. #&gt; [1] 1 3 5 7 9 seq(from = 1, to = 10, by = 2) #&gt; [1] 1 3 5 7 9 These are exactly the same, but one uses argument order, and the other the argument names. To figure out the order and names of arguments, you have to consult the help pages or the internet. For simple functions like seq() and mean() it’s common to omit the argument names, while for more complicated functions it’s better to include them, so you clearly show what you’re doing. Note that function arguments are never enclosed with quotes \". Line breaks in functions  A tip for making code more readable is that as long as we are inside a parenthesis, we can have line breaks in our code. This means that we can put each argument on a separate line. Instead of the code above, we could also have written: seq(from = 1, to = 10, by = 2) #&gt; [1] 1 3 5 7 9 This may not matter much for such a simple function, but it gets way easier to read long, complicated functions if it’s formatted this way. You will see this formatting in our next section about data frames. 1.2.6 Data frames Let’s return to investigating more aspects of the Nordic countries, after all there’s more to a country than just it’s population size. Table 1.2 shows some additional information about the Nordic countries: Table 1.2: More information on the nordic countries. Country Population size Area (km2) Life expectancy Denmark 5868927 42434 81.24 Finland 5572355 303815 81.33 Iceland 350773 100250 83.26 Norway 5465387 304282 82.14 Sweden 10185555 410335 82.40 What if you want to use some more information about the countries, e.g. the area? One solution is to store the new information in a vector: nordic_area &lt;- c(42434, 303815, 100250, 304282, 410335) However, as the number of variables increase, you can get quite a lot of vectors! Luckily there’s a way to keep everything together, in what R calls a data frame. 1.2.6.1 Creating a data frame A data frame is conceptually similar to the table above. You have columns of different variables, and rows of observations of these variables. If you have a number of vectors (of the same length!), you can combine these into a data frame with the function data.frame():4 nordic_df &lt;- data.frame(country = nordic_names, population = nordic, area = nordic_area) nordic_df #&gt; country population area #&gt; Denmark Denmark 5868927 42434 #&gt; Finland Finland 5572355 303815 #&gt; Iceland Iceland 350773 100250 #&gt; Norway Norway 5465387 304282 #&gt; Sweden Sweden 10185555 410335 The arguments of data.frame() are kind of special in that you can name them whatever you want. Your argument names (i.e. the text before the equal sign) determine the names of the columns in your data frame. 1.2.6.2 Extracting the data within the data frame You previously learned that you can access data in a vector using square brackets [], and the same goes for data frames. However, we now have two dimensions (rows and columns) instead of just 1. The syntax for extracting from a data frame is df[row, column]. For instance, to get the area (column 3) of Finland (row 2), we can run: # extract row 2, column 3 nordic_df[2, 3] #&gt; [1] 303815 You can also use the name of the column instead of the position (remember quotes!). nordic_df[2, &quot;area&quot;] #&gt; [1] 303815 If you leave the row field empty, you will get all rows, and if you leave the column field empty you will get all columns. # Get area of all countries nordic_df[,&quot;area&quot;] #&gt; [1] 42434 303815 100250 304282 410335 # Get all the information about Iceland nordic_df[3,] #&gt; country population area #&gt; Iceland Iceland 350773 100250 # get all rows and columns (useless, but it works!) nordic_df[,] #&gt; country population area #&gt; Denmark Denmark 5868927 42434 #&gt; Finland Finland 5572355 303815 #&gt; Iceland Iceland 350773 100250 #&gt; Norway Norway 5465387 304282 #&gt; Sweden Sweden 10185555 410335 Another way of getting data from your data frame is with the $ operator. Writing something like df$column returns the entire column as a vector. nordic_df$population #&gt; [1] 5868927 5572355 350773 5465387 10185555 This is very useful, as you can do the same operations on these vectors that you can on any vector. nordic_df$population / 10^6 #&gt; [1] 5.868927 5.572355 0.350773 5.465387 10.185555 mean(nordic_df$area) #&gt; [1] 232223.2 nordic_df$area + nordic_df$population #&gt; [1] 5911361 5876170 451023 5769669 10595890 nordic_df$population &gt; 5*10^6 #&gt; [1] TRUE TRUE FALSE TRUE TRUE Note that the $ syntax only works for columns, not for rows. Exercise: Calculate the population density (population/area) of the Nordic countries using the data frame and the $ operator. nordic_df$population / nordic_df$area #&gt; [1] 138.307183 18.341277 3.498983 17.961585 24.822535 1.2.6.3 Adding data to the data frame Adding data to the data frame can also be done with the $ operator. Instead of referencing an existing column, you can simply write a new name after the $, and assign to it as a variable like so: df$newcolumn &lt;- c(1, 2, 3, 4, 5). nordic_df$is_norway &lt;- c(&quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;) nordic_df #&gt; country population area is_norway #&gt; Denmark Denmark 5868927 42434 no #&gt; Finland Finland 5572355 303815 no #&gt; Iceland Iceland 350773 100250 no #&gt; Norway Norway 5465387 304282 yes #&gt; Sweden Sweden 10185555 410335 no Now you have added a column named “is_norway” to your data frame, which is kind of useless, but still cool. Exercise: For a more useful application of creating a new column, add a population density column to nordic_df (using the same calculation as in the previous exercise). nordic_df$pop_density &lt;- nordic_df$population / nordic_df$area 1.2.6.4 Subsetting with logical operators Sometimes you don’t know exactly which rows you want to extract from your data. For example, you may want all countries with an area below 300 000 km2, or all countries that aren’t Norway. For this, we can use the logical operators that you learned about in section 1.2.4. If you want to figure out which countries have an area less than 300 000 km2, you have learned that you can do the following: # which elements have an area less than 300000? nordic_df$area &lt; 300000 #&gt; [1] TRUE FALSE TRUE FALSE FALSE The result shows that element 1 and 3 are TRUE, while the rest are FALSE. If you put this same statement within square brackets (before the comma) to index your data frame, R will return all rows that are TRUE, and discard all rows that are FALSE. # get all countries with an area below 300 000 km^2 nordic_df[nordic_df$area &lt; 300000, ] #&gt; country population area is_norway pop_density #&gt; Denmark Denmark 5868927 42434 no 138.307183 #&gt; Iceland Iceland 350773 100250 no 3.498983 We could do the same to get all rows where the country isn’t Norway: # get all countries except Norway nordic_df[nordic_df$country != &quot;Norway&quot;, ] #&gt; country population area is_norway pop_density #&gt; Denmark Denmark 5868927 42434 no 138.307183 #&gt; Finland Finland 5572355 303815 no 18.341277 #&gt; Iceland Iceland 350773 100250 no 3.498983 #&gt; Sweden Sweden 10185555 410335 no 24.822535 Two important things to note here is that you have to explicitly write nordic_df$ inside the square brackets, and that you have to end with a comma to tell R that you’re filtering rows (i.e. leave the column space empty). Neither nordic_df[area &lt; 300000, ] nor nordic_df[nordic_df$area &lt; 300000] will work. Exercise: make a subset of your data containing all the countries with population density of more than 18 (using the pop_density column you created in the previous exercise). nordic_df[nordic_df$pop_density &gt; 18, ] #&gt; country population area is_norway pop_density #&gt; Denmark Denmark 5868927 42434 no 138.30718 #&gt; Finland Finland 5572355 303815 no 18.34128 #&gt; Sweden Sweden 10185555 410335 no 24.82254 Important concept When you have a lot of data that belongs together, typically something you could store in an excel document and show in a table, make it into a data frame. Many things we will be doing later in this course requires that your data is in a data frame, so learn to recognize this type of object. with the comments, I hope!↩︎ You can also use = instead of &lt;- . If you know another programming language already, like Python, this may feel more natural. I like to use the arrow to remind my muscle memory that I’m working in R, but it makes absolutely no difference which you use, so use whichever you like!↩︎ As a side note, whenever you wonder “what happens if I do …”, try it! The worst thing that can happen if you try something is that you get an error, the best thing is that you learn something useful.↩︎ You can see here that the names of nordic carries over when making the data frame. This results in the names of the countries stored in what seems to be a nameless column. These are the row names, and can be accessed with row.names(nordic_df)↩︎ "],["dataimport.html", "1.3 Data import", " 1.3 Data import This part of the tutorial is a simplified version of this tutorial. If you want to know more about data import in R, check that one out! I’ve frequently had you imagine doing the data input for data from all countries in the world throughout this tutorial. You may already have thought manually inputting data for the entire world like we did with the Nordic countries above is time-consuming and error-prone. This is correct, and we should be able to do something smarter. Luckily, we can download data as a text file and import it into R as a data frame! 1.3.1 Data formats Start by downloading the file worlddata.csv and make sure you know where on your computer it’s downloaded to (suggestion: the same folder as your R script). Open the file in a plain text editor on your computer (e.g. Notepad on Windows or TextEdit on Mac). You can see that we have one row of variable names, and then one row of data for each country. The values are separated by comma making this what we call comma separated values (csv), which is a very common format to store data in. Data entries can also be separated by e.g. spaces, tabs, semicolon and much more. All of these can be imported into R, but you always need to be aware how your data are formatted. 1.3.2 Working directories The next step is to make R look in the same folder that your data is in. An important concept is that R only works in one folder at a time. You can get the folder where R is currently looking, called the working directory, by running the command getwd() This may or may not be the folder where your text file (and/or R script) is located (probably it isn’t). If R is looking in a different folder than you want it to, you need to tell it where to look, this can be done in a variety of ways, the most manual being the function setwd(): # set the working directory to the BIOS1140 folder within &quot;Documents&quot; setwd(&quot;C:/Documents/BIOS1140&quot;) Another way to set the working directory (in RStudio) is by navigating to Session &gt; Set working directory and choosing the folder you want. A third way is to find the “Files” tab in RStudio (in the bottom-right pane), navigate to your folder there, and click More &gt; Set as working directory. Exercise: Download the csv file to your computer (make sure you know where), and set the working directory to the same folder as the file is in. Important concept: R only works in one folder at a time, this folder is called the working directory. Get your current working directory with getwd() and set a new working directory either with setwd() or by navigating in RStudio. To import a file into R it has to be in your current working directory! 1.3.3 Importing When importing data, there are three things you need to consider: Does the first row in the data contain variable names? If so, this is called a header. How are the values separated? (e.g. comma, semicolon, tab) What is the decimal marker? It is usually period . but can sometimes be comma , Look at the file worlddata.csv and answer the three questions above for that file. The function for reading data into R is read.table(). The first argument is the file name in quotes. It also has three more arguments5 to answer the three questions above. set header = TRUE if the data has a header, header = FALSE if it doesn’t. sep =\",\" for comma separated values, \";\" for semicolon, \"\\t\" for tab space, and much more. dec is the decimal marker, either dec = \".\" or dec = \",\" Now you should be able to import the data! I’ve made some skeleton code for you to fill out below to simplfy things. Exercise: import worlddata.csv into R by filling in the blanks in the code below. # fill in the blanks (after &quot;=&quot;) world_data &lt;- read.table(&quot;worlddata.csv&quot;, header = , sep = , dec = ) world_data &lt;- read.table(&quot;worlddata.csv&quot;, header = TRUE, sep = &quot;,&quot;, dec = &quot;.&quot;) If you managed to do this, you now have data for all countries in the world stored in a data frame! Try doing some operations on it to get familiar with the data. Some suggestions (remember that you can write ?functionname to find out more about a function): head() shows the first rows of the data frame summary() gives a summary of key aspects of the data frame use the $ operator to extract columns, add or multiply them together, or whatever you want! see section 1.2.5 if you forgot what an argument is↩︎ "],["plotting.html", "1.4 Plotting", " 1.4 Plotting This part of the tutorial is very brief, since we will focus on plotting with ggplot2 next week. If you want to know more about plotting in so-called base R, check out this tutorial! R is a powerful tool for visualizing your data. You can make almost any kind of plot, revealing connections that are hard to see from summary statistics. In this section, we will only go through some very basic plotting, you will learn more about plotting next week. The basic function for plotting in R is simply plot(). This function will guess what kind of plot to make based on the data you provide. You can supply many arguments to the plot() function to get the visualisation you want, which we will show some examples of here. 1.4.1 Plotting vectors The simplest way of plotting in R is by plotting two vectors of equal length. One vector gives the x-value, and another gives the y-value. # make two vectors of equal length x &lt;- 1:50 y &lt;- 51:100 plot(x, y) Figure 1.1: Two vectors plotted against each other. As you can see, it makes a simple plot of our data, using points as the default. If we want to make a line graph we have to specify type = \"l\": plot(x, y, type = &quot;l&quot;) Figure 1.2: The same plot as above, but with a line graph instead For all the different ´type´ arguments, see the plot-function’s help page by running ?plot. You can use the functions lines() and points() to add elements to an existing plot instead of creating a new one: plot(x, y, type = &quot;l&quot;) lines(x, y - 5, col = &quot;red&quot;) points(c(0, 10, 15), c(60, 80, 70), col = &quot;blue&quot;) Figure 1.3: Plot with added line (red) and added points (blue) You can use the hist() function to create simple histograms, as shown with the Total_Fertility_Rate column of world_data in the example below hist(world_data$Total_Fertility_Rate) Figure 1.4: Distribution of fertility rate in the countries of the world. 1.4.2 Customizing your plots The plot() (and lines() and points()) function contains a lot of arguments to customize your plots. Below is a (non-comprehensive) list of parameters and what they do: col changes the color of the plot pch changes the shape of your points lty changes line type main changes the title xlab and ylab changes the axis labels Below is an example that uses all these arguments in a single plot, see if you can follow what happens. plot(x, y, type = &quot;l&quot;, main = &quot;customization options&quot;, xlab = &quot;x-axis&quot;, ylab = &quot;y-axis&quot;) lines(x, y - 5, col = &quot;red&quot;, lty = 2) points(c(0, 10, 15), c(60, 80, 70), col = &quot;blue&quot;, pch = 10) "],["installing-and-loading-packages.html", "1.5 Installing and loading packages", " 1.5 Installing and loading packages So far, all the functions we have used in this tutorial is part of your R installation. However, people all around the world have developed their own custom functions, and shared them in what we call packages6. During this course, we will need several of these packages, in this section you will learn how to install and load them. Packages in R can be installed with the function install.packages(). You will only have to install a package once per computer. Here is how to install the ggplot2 package, which we will use next week: install.packages(&quot;ggplot2&quot;) When a package has been installed, you have to load it in order to use it. You have to load a package each time you want to use it in a script. You load a package using the function library(): library(ggplot2) Make a habit of loading all the packages you use at the top of your script, that makes it easier to follow. Important concept: Packages contain custom functions that you will need later in this course (and during the rest of your studies and work as a biologist). install a package with install.packages() you only need to do this once per computer load a package with library() you need to do this every time you want to use that package. Load all packages you need at the top of your script There are thousands of packages available for use in R, doing anything imaginable. See for example the package ggbernie which is dedicated to plotting US senator Bernie Sanders, and allows me to make plots like this: ↩︎ "],["going-further.html", "1.6 Going further", " 1.6 Going further Since R is so widely used, there are many excellent resources out there to help you learn your way. Our introduction is just one of them and so here we point you towards some good examples for developing and honing your understanding of working with R. Datacamp has an excellent, free introduction to R The R Core team provide an indepth introduction to R on CRAN Emmanuel Paradis has a good R guide for beginners A beginners guide to R packages A simple guide to base R graphics "],["ch02.html", "Week 2 Building on your foundations: going further with R", " Week 2 Building on your foundations: going further with R In the last chapter, we got to grips with the basics of R. Hopefully after completing the basic introduction, you feel more comfortable with the key concepts of R. Don’t worry if you feel like you haven’t understood everything - this is common and perfectly normal! Learning R is very much like learning a real language in that it takes time and practice to feel ‘fluent’. Even if you do feel comfortable in the language, there is no shame in asking for help or looking for more information to develop your understanding. As regular R users, we still look things up constantly and there are one or two basics which we still forget, even with over a decade of experience of using the R environment! With this in mind, a goal of these R tutorials is to re-emphasise and reinforce basic concepts throughout. We will introduce concepts but through the practical demonstrations of code, we will underline them again and again. In future chapters, we will be using a similar approach to reinforce the evolutionary genetic concepts you have encountered in the book. However, for this chapter we remain solely in the R environment and will instead switch our focus to more advanced features of R. Advanced does not necessarily mean more complicated - but it does mean that you need to have at least been introduced to the basic concepts. We will first ‘level-up’ our approach to handling and manipulating data. For this, we will be borrowing heavily from the tidyverse - a collection of packages and principles for data science in R. We will also introduce you to more advanced plotting, comparing the two most popular apporaches for plots - base and ggplot. What to expect In this section we are going to: explore more advanced methods of handling and manipulating data learn how to plot data using ggplot2 learn how to reshape data with pivot_longer() "],["intro-to-data-manipulation-with-tidyverse.html", "2.1 Intro to data manipulation with tidyverse", " 2.1 Intro to data manipulation with tidyverse Data manipulation might seem quite a boring topic but it is actually a crucial part of data science and increasingly, bioinformatics and evolutionary biology. For the average researcher working with biological data, we would estimate that the vast majority of analysis time is spent handling the data. By handling and manipulation, we mean exploring the data, shaping it into a form we want to work with and extracting information we find important or interesting. Getting to know your data is absolutely fundamental to properly understanding it and that is why we have decided to dedicate time to it in this chapter. At this point in our tutorial, we will use a series of packages collectively known as the tidyverse; in particularly, we will focus on functions from a tidyverse package called dplyr. These packages grew from the approach of Hadley Wickham - a statistician responsible for popularising fresh approaches to R and data science. As with nearly all things in R, there are many, many ways to achieve the same goal and the guidlines we give here are by no means definitive. However, we choose to introduce these principles now because in our experience of data analysis, they have greatly improved our efficiency, the clarity of our R code and the way we work with data. 2.1.1 What is the tidyverse? It’s important to emphasize that the tidyverse set of packages can do mostly the same as base R already can do. So what’s the difference? While base R is a collection of different methods and functions built up over years, tidyverse is designed with a specific philosophy in mind. This leads to having a consistent approach to solving problems that many find appealing. That being said, if you find you prefer the “regular” R-functions over their tidyverse equivalents, go ahead and use those instead, there’s nothing wrong with that. 2.1.2 The dplyr package dplyr is one of the packages in the tidyverse, and is focused on manipulating data in data frames. dplyr at it’s core consists of combining 5 different verbs for data handling: select() select columns from your data filter() filters rows based on certain criteria mutate() creates new columns (not gone through in this tutorial, but included in this list for completeness) group_by() creates groups for summarizing data summarise() summarises data based on the groups you have created We will go through the use of these functions shortly. You may notice that you’ve already learned how to select, filter and mutate data last week using [] and $, which is correct, and exactly what we mean when we say that base R and tidyverse can do the same things. "],["using-dplyr-to-work-with-your-data.html", "2.2 Using dplyr to work with your data", " 2.2 Using dplyr to work with your data First, we have to install and load the tidyverse.7 install.packages(&quot;tidyverse&quot;) library(tidyverse) Together with the tidyverse, you get a built in data set called starwars, containing information about the characters from the Star Wars films. We will use this data set for most of this tutorial. Since it’s already loaded, all you have to do to access it is run: starwars #&gt; # A tibble: 87 × 14 #&gt; name height mass hair_color skin_color eye_color birth_year sex gender #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Luke Sk… 172 77 blond fair blue 19 male mascu… #&gt; 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… #&gt; 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… #&gt; 4 Darth V… 202 136 none white yellow 41.9 male mascu… #&gt; 5 Leia Or… 150 49 brown light brown 19 fema… femin… #&gt; 6 Owen La… 178 120 brown, gr… light blue 52 male mascu… #&gt; 7 Beru Wh… 165 75 brown light blue 47 fema… femin… #&gt; 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… #&gt; 9 Biggs D… 183 84 black light brown 24 male mascu… #&gt; 10 Obi-Wan… 182 77 auburn, w… fair blue-gray 57 male mascu… #&gt; # ℹ 77 more rows #&gt; # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, #&gt; # vehicles &lt;list&gt;, starships &lt;list&gt; Additional info: starwars is a data frame like you learned about last week, but you might notice that it’s printing a bit differently (e.g. with red color for NA, condensing the output). This is because starwars is a special kind of data frame introduced in the tidyverse, called a tibble. For all practical purposes, a tibble and a data frame is the same, so throughout this course we won’t care much whether our data is contained in a tibble or a regular data frame. 2.2.1 The pipe The tidyverse introduces a new operator called the pipe, which looks like this %&gt;%. Conceptually, the pipe allows you to do something with your data, and then send the result to a new function which does more work, sends it to the next function and so on until you’re satisfied. You can for instance use the pipe like this: x &lt;- 1:100 x^2 %&gt;% mean() #&gt; [1] 3383.5 This is equivalent to writing: mean(x^2) #&gt; [1] 3383.5 Basically, you are putting the left hand side of the pipe into the parentheses in the function on the right hand side. This may not seem useful right now, but as we will see later, this can make code much easier to read. You can also assign the results of your pipe to an object like any ordinary calculation: x2_mean &lt;- x^2 %&gt;% mean() There will be a lot of examples of using the pipe throughout this tutorial, showing how it can make quite complex code readable. Important concept: The pipe operator %&gt;% allows you to send an object from the left side of the pipe to a function on the right side. 2.2.2 Selecting columns with select() Lets say we want to choose the name and homeworld columns from our starwars data, how can we do that? With standard R, we might do something like this. # with names starwars[, c(&#39;name&#39;, &#39;homeworld&#39;)] # with indices starwars[, c(1, 9)] With dplyr we can do the following: select(starwars, name, homeworld) #&gt; # A tibble: 87 × 2 #&gt; name homeworld #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Luke Skywalker Tatooine #&gt; 2 C-3PO Tatooine #&gt; 3 R2-D2 Naboo #&gt; 4 Darth Vader Tatooine #&gt; 5 Leia Organa Alderaan #&gt; 6 Owen Lars Tatooine #&gt; 7 Beru Whitesun Lars Tatooine #&gt; 8 R5-D4 Tatooine #&gt; 9 Biggs Darklighter Tatooine #&gt; 10 Obi-Wan Kenobi Stewjon #&gt; # ℹ 77 more rows The first argument here is your data, while the others are the columns you want to select. Note that you don’t need to use the quotes \" here, you generally don’t need those for the dplyr functions. select() becomes even more intuitive when using the pipe: starwars %&gt;% select(name, homeworld) This style of code is closer to how we would write in English: “Take the starwars data, and select the name and homeworld columns”. From now on we will write all our dplyr code using the pipe. If you want to omit a column, you can use - in front of its name: # choose all columns BUT name starwars %&gt;% select(-name) select also has additional ways of selecting columns, some examples of this is shown below: # choose only columns containing an underscore starwars %&gt;% select(contains(&quot;_&quot;)) # choose only columns beginning with &quot;s&quot; starwars %&gt;% select(starts_with(&quot;s&quot;)) # choose only columns ending with &quot;color&quot; starwars %&gt;% select(ends_with(&quot;color&quot;)) Exercise: Use select() to select numeric columns, i.e. all the columns that contain numbers, and save it to an object with a meaningful name. You can do this manually by looking at the columns. If you want a challenge, see if you can figure out how to automatically select numeric columns from your data. # manual method: sw_hmb &lt;- starwars %&gt;% select(height, mass, birth_year) # challenge: sw_hmb &lt;- starwars %&gt;% select(where(is.numeric)) 2.2.3 Filtering colums using filter() Last week, you learned to filter a data set based on some criterion using the square brackets []. To filter out only the humans from the starwars data set, you could write: starwars[starwars$species == &quot;Human&quot;, ] dplyr’s filter() function does the same thing, but with a slightly different syntax: starwars %&gt;% filter(species == &quot;Human&quot;) This looks quite similar to using the square brackets, one notable difference being that you don’t need to use starwars$ within filter(), the function already understands that we are working with starwars. Like with base R, you can use this to filter with the other logical operators as well, like &gt; and !=: # get people lower than 1m tall starwars %&gt;% filter(height &lt; 100) # get all non-humans starwars %&gt;% filter(species != &quot;Human&quot;) You can also filter using several criteria at once, simply separate the logical statements with a comma: # get all non-humans shorter than 1m starwars %&gt;% filter(height &lt; 100, species != &quot;Human&quot;) 2.2.3.1 Combining filter() and select() The real power of the pipe shows when you chain several operations together. To both filter and select from your data, simply first do the filtering and then pipe the result to select:8 starwars %&gt;% filter(height &lt; 100) %&gt;% select(name, height, birth_year) #&gt; # A tibble: 7 × 3 #&gt; name height birth_year #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 R2-D2 96 33 #&gt; 2 R5-D4 97 NA #&gt; 3 Yoda 66 896 #&gt; 4 Wicket Systri Warrick 88 8 #&gt; 5 Ratts Tyerel 79 NA #&gt; 6 Dud Bolt 94 NA #&gt; 7 R4-P17 96 NA Again, the code looks like how you would explain what you’re doing: “take the starwars data, filter based on height, and select the name, height and birth_year columns”. Note that you can have a line break after a pipe like you can inside parentheses, your code will still continue running. Exercise: use filter() to choose all the people that has “Naboo” as homeworld. Select name, skin_color and eye_color and save the result to an object. sw_naboo_color &lt;- starwars %&gt;% filter(homeworld == &quot;Naboo&quot;) %&gt;% select(name, skin_color, eye_color) Important concept: - filter() is used to select specific rows. Example: filter(height &lt; 100) - select() is used to select specific columns. Example: select(name, height) 2.2.4 Grouped summaries with group_by() and summarise() Imagine that you want to calculate the mean height of the people (and droids) in the starwars data set. You could use mean() on the column to achieve this (note the use of na.rm since the height column contains NAs): mean(starwars$height, na.rm = TRUE) #&gt; [1] 174.6049 But what if you want to calculate the mean height separately for e.g. the different species? One way to do this is to do a grouped summary. Your group is the species column, and your summary statistic is mean. We create groups using the group_by() function: starwars %&gt;% group_by(species) #&gt; # A tibble: 87 × 14 #&gt; # Groups: species [38] #&gt; name height mass hair_color skin_color eye_color birth_year sex gender #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Luke Sk… 172 77 blond fair blue 19 male mascu… #&gt; 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… #&gt; 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… #&gt; 4 Darth V… 202 136 none white yellow 41.9 male mascu… #&gt; 5 Leia Or… 150 49 brown light brown 19 fema… femin… #&gt; 6 Owen La… 178 120 brown, gr… light blue 52 male mascu… #&gt; 7 Beru Wh… 165 75 brown light blue 47 fema… femin… #&gt; 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… #&gt; 9 Biggs D… 183 84 black light brown 24 male mascu… #&gt; 10 Obi-Wan… 182 77 auburn, w… fair blue-gray 57 male mascu… #&gt; # ℹ 77 more rows #&gt; # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, #&gt; # vehicles &lt;list&gt;, starships &lt;list&gt; Notice that nothing has changed in the data, but at the top you can see the text # Groups: species [38], showing that you indeed have created a group, and that you have 38 different species in your data. The main use of group_by() is together with summarise(), which does a summary based on the groups you’ve created: starwars %&gt;% group_by(species) %&gt;% #create group summarise(mean_height = mean(height, na.rm = TRUE)) # calculate summary statistic #&gt; # A tibble: 38 × 2 #&gt; species mean_height #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aleena 79 #&gt; 2 Besalisk 198 #&gt; 3 Cerean 198 #&gt; 4 Chagrian 196 #&gt; 5 Clawdite 168 #&gt; 6 Droid 131. #&gt; 7 Dug 112 #&gt; 8 Ewok 88 #&gt; 9 Geonosian 183 #&gt; 10 Gungan 209. #&gt; # ℹ 28 more rows Note how again, like in data.frame, the argument name to summarise becomes the column name in your new data frame. You can use several summary functions inside summarise(), like median(), sd(), sum() and max() to name some. You can also do several summaries within a single summarise() function: starwars %&gt;% group_by(species) %&gt;% summarise(mean_height = mean(height, na.rm = TRUE), median_height = median(height, na.rm = TRUE), sd_height = sd(height, na.rm = TRUE)) #&gt; # A tibble: 38 × 4 #&gt; species mean_height median_height sd_height #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Aleena 79 79 NA #&gt; 2 Besalisk 198 198 NA #&gt; 3 Cerean 198 198 NA #&gt; 4 Chagrian 196 196 NA #&gt; 5 Clawdite 168 168 NA #&gt; 6 Droid 131. 97 49.1 #&gt; 7 Dug 112 112 NA #&gt; 8 Ewok 88 88 NA #&gt; 9 Geonosian 183 183 NA #&gt; 10 Gungan 209. 206 14.2 #&gt; # ℹ 28 more rows We can even group by several variables, creating more detailed summaries: starwars %&gt;% group_by(homeworld, sex) %&gt;% summarise(mean_height = mean(height, na.rm = TRUE)) #&gt; # A tibble: 60 × 3 #&gt; # Groups: homeworld [49] #&gt; homeworld sex mean_height #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Alderaan female 150 #&gt; 2 Alderaan male 190. #&gt; 3 Aleen Minor male 79 #&gt; 4 Bespin male 175 #&gt; 5 Bestine IV &lt;NA&gt; 180 #&gt; 6 Cato Neimoidia male 191 #&gt; 7 Cerea male 198 #&gt; 8 Champala male 196 #&gt; 9 Chandrila female 150 #&gt; 10 Concord Dawn male 183 #&gt; # ℹ 50 more rows Now you get two groups for homeworld Alderaan, one with males and one with females. For the following homeworld groups there are only males (except Chandrila, with only females), so you just get one group for each (giving a pretty accurate picture of the gender balance in Star Wars). 2.2.4.1 Counting how many observations we have in our groups with tally() When we have created our groups, we can also use the tally() function to count the number of observations we have in the groups: starwars %&gt;% group_by(species) %&gt;% tally() #&gt; # A tibble: 38 × 2 #&gt; species n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Aleena 1 #&gt; 2 Besalisk 1 #&gt; 3 Cerean 1 #&gt; 4 Chagrian 1 #&gt; 5 Clawdite 1 #&gt; 6 Droid 6 #&gt; 7 Dug 1 #&gt; 8 Ewok 1 #&gt; 9 Geonosian 1 #&gt; 10 Gungan 3 #&gt; # ℹ 28 more rows This can be useful to get an overview of your data9 Important concept: group_by() can be combined with different functions to give an overview of your data. group_by() %&gt;% summarise() does some calculation in each group. Example: group_by(homeworld, sex) %&gt;% summarise(mean_height = mean(height)) group_by() %&gt;% tally() counts the number of observations in the groups. Example: group_by(species) %&gt;% tally() 2.2.5 Using everything we’ve learned in a single pipe, and a dplyr exercise One advantage of pipes is that you can do everything you want in a single operation10. Below is an example using everything we’ve learned so far in a single pipe. starwars %&gt;% select(-films, -vehicles, -starships) %&gt;% filter(species == &quot;Human&quot;) %&gt;% group_by(sex) %&gt;% summarise(mean_height = mean(height, na.rm = TRUE)) #&gt; # A tibble: 2 × 2 #&gt; sex mean_height #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 female 164. #&gt; 2 male 182. Exercise: Explain to another student what happens in the code above (or write it down if you’re doing this on your own). Exercise: Take the starwars data set, filter so you keep all that are below the mean height. Then, calculate the mean height of these short individuals, grouped by homeworld. Show hint You can supply the mean height to the logical statement inside filter(). Your filtering step should then look like this: filter(height &lt; mean(height, na.rm = TRUE)) starwars %&gt;% filter(height &lt; mean(height, na.rm = TRUE)) %&gt;% group_by(homeworld) %&gt;% summarise(mean_height = mean(height, na.rm = TRUE)) #&gt; # A tibble: 19 × 2 #&gt; homeworld mean_height #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Alderaan 150 #&gt; 2 Aleen Minor 79 #&gt; 3 Chandrila 150 #&gt; 4 Corellia 170 #&gt; 5 Coruscant 168. #&gt; 6 Endor 88 #&gt; 7 Iridonia 171 #&gt; 8 Malastare 112 #&gt; 9 Mirial 168 #&gt; 10 Naboo 147 #&gt; 11 Rodia 173 #&gt; 12 Sullust 160 #&gt; 13 Tatooine 153. #&gt; 14 Toydaria 137 #&gt; 15 Troiken 122 #&gt; 16 Tund 163 #&gt; 17 Vulpter 94 #&gt; 18 Zolan 168 #&gt; 19 &lt;NA&gt; 81 Remember that you only need to install a package once, but that it needs to be loaded with library() every time you want to use it.↩︎ Remember that what the pipe basically does is to put the left hand side of the pipe into the function on the right hand side. Without the pipe, filtering and selecting looks like this: select(filter(starwars, height &lt; 100), name, height, birth_year)↩︎ for example, you could realise that it doesn’t make sense to calculate mean and standard deviation when you only have a single value, like we’ve done quite a bit↩︎ In practice, it’s probably smart to make an object of your intermediary results every now and then.↩︎ "],["plotting-your-data-with-ggplot2.html", "2.3 Plotting your data with ggplot2", " 2.3 Plotting your data with ggplot2 In the last chapter, we learned that R is highly versatile when it comes to plotting and visualising data. Visualisation really cannot be understated - as datasets become larger and more difficult to handle, it is imperative you learn how to effectively plot and explore your data. This obviously takes practice, but plotting and summarising data visually is a key skill for guiding further analysis - this is especially true for evolutionary genomics but is easily applicable to any number of scientific fields. As you may have gathered by now, there are lots of opinions on how to use R - whether you should use base or tidyverse approaches. We want to stress that there is nothing wrong with using base plotting, it is capable of some very impressive plots (use demo(graphics) to have a look). However ggplot2 is extremely flexible and takes quite a different approach to plotting compared to baseR. 2.3.1 The three things you need in a ggplot You need three basic elements to construct a ggplot:11 Data: this is your data set, and it has to be contained in a data frame. Variables: You need variables to plot on the x and y axes (mapping of variables) Geometry: You need some graphics in your plot: points, lines, boxplots, histograms etc. Let’s now use these three elements step-by-step to build up our plot. In our example, we want to make a scatterplot (plot with points) of height vs. mass in our starwars data set. 2.3.1.1 Data First, we try supplying our data, starwars. The data is provided as an argument to the ggplot() function. ggplot(data = starwars) As you can see, this results in a completely empty plot (because, like I said, we need two more things). 2.3.1.2 Variables The variables are provided to the mapping argument of ggplot(). For reasons we won’t discuss here, all variables always have to be contained within the function aes(). Let’s try providing variables to our plot: ggplot(data = starwars, mapping = aes(x = height, y = mass)) Now we’re getting somewhere! We have axes now, but we’re still missing our points. Time to add the geometry. 2.3.1.3 Geometry The geometry of a ggplot aren’t provided to the ggplot() function as arguments. Instead, a separate function is added to the plot using +. All the functions for adding geometry start with geom_, and the one for points is called geom_point(). We add this to our plot: ggplot(data = starwars, mapping = aes(x = height, y = mass)) + geom_point() Wohoo, we now have the plot we set out to make! There’s an obvious outlier in the mass department, which we’ll deal with later. The philosophy behind adding geometry with a + is that you build up your plot, layer by layer. We could for example add a regression line in addition to points in our plot: ggplot(data = starwars, mapping = aes(x = height, y = mass)) + geom_point() + #add points geom_smooth() #add regression line We could keep adding layers like this forever, as long as we felt we had some meaningful stuff to add.12 Notice how we can have line breaks in our code after the +, the plot still executes. Important concept: You need 3 things for a ggplot: data in a data frame (the data argument of ggplot()) variables – which columns of your data do you want to plot? (the mapping argument of ggplot(), needs to be wrapped in aes()) geometry – how do you want to represent your variables (separate functions, starting with geom_). You can add as many layers of geometry as you’d like. 2.3.1.4 Interlude: filtering out the outlier Before we continue, we should investigate our outlier, and remove it from our data to better see the pattern between mass and height. Exercise: Use the dplyr tools you learned earlier to find out who the outlier is, and make a subset of the data without that individual. Then, remake the plot with your subsetted data. Show hint You know that the individual in question is really heavy. Use filter() on the mass column to find it! # find individuals with mass larger than 1000 starwars %&gt;% filter(mass &gt; 1000) %&gt;% select(name, mass) #&gt; # A tibble: 1 × 2 #&gt; name mass #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Jabba Desilijic Tiure 1358 # If you’ve seen Star Wars, the answer to this shouldn’t be a huge surprise… # Let’s filter him out of the data starwars2 &lt;- starwars %&gt;% filter(mass &lt; 1000) # and plot it ggplot(data = starwars2, mapping = aes(x = height, y = mass)) + geom_point() 2.3.2 Storing ggplots in objects A very useful feature of ggplots is that they can be stored in objects just like any other data. We will test this with the starwars2 data frame we created above. sw_plot &lt;- ggplot(data = starwars2, mapping = aes(x = height, y = mass)) sw_plot We can now use this object as a base, and make different plots by adding geoms: # plot with points sw_plot + geom_point() # plot with line sw_plot + geom_line() # plot with regression line sw_plot + geom_smooth() If you plan to make several plots with the same data and variables, you should save the basic plot to an object to avoid repeating yourself. 2.3.3 Customizing your plots 2.3.3.1 General customization So far, we’ve been using the geom_ functions without arguments, but they actually take many of the same arguments as plot(). This means that you can use col to change color, pch to change point shape and lty to change line type: # create basic plot object sw_plot &lt;- ggplot(data = starwars2, mapping = aes(x = height, y = mass)) # add lines and points, and customize these sw_pts_ln &lt;- sw_plot + geom_line(col = &quot;steelblue&quot;, lty = 2) + geom_point(col = &quot;firebrick&quot;, pch = 3) # print plot sw_pts_ln Adding title and labels can be done by adding a separate function, labs(). labs() has, among others, the arguments x, y, title and subtitle, doing exactly what you would expect:13 sw_pts_ln + labs(x = &quot;Height (cm)&quot;, y = &quot;Mass (kg)&quot;, title = &quot;Heigt vs. mass in the Star Wars universe&quot;, subtitle = &quot;A part of the BIOS1140 ggplot tutorial&quot;) 2.3.3.2 Mapping variables to colors, shapes etc. The modifications you’ve learned so far are nice for making plots pretty, but the real power of using colors and other aesthetics comes when they can contain additional information about your data. Here we introduce a powerful concept in ggplot2 for doing this: You can map data to more than just your axis labels. In the following plot, the points are colored by their value in the species column, rather than all having the same color: ggplot(data = starwars2, mapping = aes(x = height, y = mass, col = species)) + geom_point() One important thing to note here is that your variable has to be within aes() in your plot. Note that variable names do not need quotes. It’s easy to get confused about when to put something inside aes() and not, but the general rule is: If you’re mapping color (or shape, linetype) to a variable in your data set, the col argument must be inside aes(). If you’re giving everything the same color (or shape, linetype), the col argument must be outside of aes(). In this sense, mapping variables to e.g. color is no different than mapping to your x and y axes (which you would always wrap inside aes()) As indicated above, other things than color can be mapped to aesthetics: ggplot(data = starwars2, mapping = aes(x = height, y = mass, pch = sex, lty = sex)) + geom_point() + # method=lm creates LINEAR regression, se=FALSE removes the grey confidence intervals geom_smooth(method = &quot;lm&quot;, se = FALSE) If you e.g. want to group your points by sex, but you don’t want that same grouping for your lines, you can use the mapping argument of your geom instead: ggplot(data = starwars2, mapping = aes(x = height, y = mass)) + geom_point(mapping = aes(col = sex)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Important concept: Variables can be mapped to aesthetics like color and point shape the same way they can be mapped to axes. Whenever you do this, you have to have your mapping within the aes() function. You can use the mapping argument of ggplot() to make your mapping global (i.e. for the entire plot), or the mapping argument of a geom to make the mapping exclusive to that geom. Exercise: Make a scatter plot (plot with points) of height vs. birth year in the Star Wars data. Color the points by species. Add a single (linear) regression line that is not colored by species. Show hint Map color within the geom_point() function in order to avoid having your regression line colored by species ggplot(data = starwars, mapping = aes(x = birth_year, y = height)) + geom_point(mapping = aes(col = species)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Tip: From now on, we will no longer explicitly write the names of the data and mapping arguments. Instead, we will go with argument order, as explained in the tutorial last week. data is the first argument of ggplot() and mapping is the second. Remember that you can always recognize the mapping argument since it always contains the aes() function. Similarly, x and y are always the first and second arguments respectively of aes(). This means that ggplot(data = starwars, mapping = aes(x = height, y = mass)) can just as well be written ggplot(starwars, aes(height, mass)) 2.3.4 Saving your plots You can save your ggplot to a file using the function ggsave(). At the very least, you need to provide a plot object and a file name (with extension). # create our plot sw_plot &lt;- ggplot(data = starwars, mapping = aes(x = height, y = mass)) + geom_point() # save it ggsave(&quot;sw_plot.png&quot;, sw_plot) There are more you could use, but these three are the ones that are strictly necessary.↩︎ Like this! ggplot(data = starwars, mapping = aes(x = height, y = mass)) + geom_point() + geom_line() + geom_text(aes(label = name)) + geom_boxplot() + geom_violin() + geom_smooth() I know, I know, I did say “meaningful”↩︎ Notice how our plot is built up layer by layer. Just to remind you, here’s how the code for our plot would look without creating intermediary objects: ggplot(data = starwars2, mapping = aes(x = height, y = mass)) + geom_line(col = &quot;steelblue&quot;, lty = 2) + geom_point(col = &quot;firebrick&quot;, pch = 3) + labs(x = &quot;Height (cm)&quot;, y = &quot;Mass (kg)&quot;, title = &quot;Heigt vs. mass in the Star Wars universe&quot;, subtitle = &quot;A part of the BIOS1140 ggplot tutorial&quot;)  ↩︎ "],["pivot-longer.html", "2.4 Reshaping data with pivot_longer()", " 2.4 Reshaping data with pivot_longer() 2.4.1 Wide and long format Let’s say you have some biological data (finally, wohoo!), and want to plot it using ggplot2. There are (at least) two ways your data can be formatted: Figure 2.1: Data in “wide format” (left) and “long format” (right) These two formats are commonly referred to as “wide” and “long” respectively. If you want to make some plot that is e.g. colored by species in this data, the data needs to be in long format, i.e. the variable you are grouping by has to be contained in a single column. Data can be converted from wide to long using the tidyverse function pivot_longer(). 2.4.2 Import example data Let’s import a data set to use as our example. Download copepods.txt here. The data contains counts of different copepod taxa from outside Drøbak. Exercise: download the data and import it into R. Is this data “wide” or “long”? Show hint Use the read.table() function. The data is tabulator separated with a header. Remember to work in the correct working directory! copepods &lt;- read.table(&quot;copepods.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) Take a look at the data and how it’s structured: copepods #&gt; depth acartia calanus harpacticoida oithona oncaea temora #&gt; 1 0 0 3 0 2 0 0 #&gt; 2 2 1 0 0 6 1 0 #&gt; 3 4 1 0 0 7 0 1 #&gt; 4 6 27 0 1 0 0 2 #&gt; 5 8 11 0 2 6 0 3 #&gt; 6 10 17 0 3 0 0 2 #&gt; 7 12 13 0 1 0 0 1 #&gt; 8 14 7 0 13 0 0 0 #&gt; 9 16 6 0 6 0 0 1 2.4.3 Reshape the data As you hopefully figured out, this data is in so-called wide format, and we need to make it long with pivot_longer(). pivot_longer() has two important arguments called names_to and values_to. In our case names_to is the name of the new column of species, and values_to is the name of the new column where our values go. In addition, you need to provide the columns that you want to reshape. We can reshape this data like this: copepods_long &lt;- copepods %&gt;% pivot_longer(c(acartia, calanus, harpacticoida, oithona, oncaea, temora), names_to = &quot;species&quot;, values_to = &quot;count&quot;) copepods_long #&gt; # A tibble: 54 × 3 #&gt; depth species count #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 acartia 0 #&gt; 2 0 calanus 3 #&gt; 3 0 harpacticoida 0 #&gt; 4 0 oithona 2 #&gt; 5 0 oncaea 0 #&gt; 6 0 temora 0 #&gt; 7 2 acartia 1 #&gt; 8 2 calanus 0 #&gt; 9 2 harpacticoida 0 #&gt; 10 2 oithona 6 #&gt; # ℹ 44 more rows Note that pivot_longer() has the same way of selecting columns as select(), meaning we can use the minus sign to choose all columns except depth. The following code does the same as the one above: copepods_long &lt;- copepods %&gt;% pivot_longer(-depth, names_to = &quot;species&quot;, values_to = &quot;count&quot;) That sure is more convenient than explicitly selecting all the columns we want (in our case, anyway)! 2.4.4 Plot the data Now we can plot the data! By now, you should know enough ggplot to attempt this yourself. Exercise: Make a plot where you have depth on the x-axis and count on the y-axis, and color by species. Experiment with using some different geoms and find the most suitable for visualising your data. When you’ve settled on a geom, create a title and axis labels, and save your finished plot with ggsave() Show some geom ideas Try these, and see how they look for your data! geom_point() geom_jitter() (what is the difference between this and geom_point()?) geom_col() (tip: use fill aesthetic instead of col) geom_boxplot() (does this make sense?) geom_line() geom_area() (use fill for this one too) Show code and plots depthplot &lt;- ggplot(copepods_long, aes(depth, count, col = species)) depthplot + geom_point() depthplot + geom_jitter() depthplot + geom_col(aes(fill = species)) depthplot + geom_boxplot() depthplot + geom_line() depthplot + geom_area(aes(fill = species)) I’m settling on geom_area() since it nicely shows both total abundance and the relationship between the taxa (plus, it looks cool). Some additional tricks I do: flip the coordinates with coord_flip() to get depth on the y-axis, and plotting -depth instead of depth to plot depth downwards. I do this because it is how depth data is usually shown in marine biology, and because I wanted to show you that there are lots of options on customising plots that you will encounter as you learn more about ggplot. copeplot &lt;- ggplot(copepods_long, aes(-depth, count)) + geom_area(aes(fill = species)) + labs(title = &quot;Number of individuals by depth&quot;, subtitle = &quot;Data from Drøbak&quot;, x = &quot;Depth (m)&quot;, y = &quot;Count&quot;) + coord_flip() copeplot "],["study-questions.html", "2.5 Study questions", " 2.5 Study questions The study questions for week 1-2 are found here. Deliver them in Canvas before the deadline as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-1.html", "2.6 Going further", " 2.6 Going further As normal, R has a huge range of freely available resources online that can help you learn more about data manipulation, the tidyverse, ggplot2 and scripting. Here we point you to a few below that you might be interested in. Datacamp has an free introduction to using tidyverse packages to manipulate data Hadley Wickham &amp; Garrett Grolemund have written the definitive, freely available online book on using R for data manipulation - this is the ‘bible’ of the tidyverse approach and includes a section on ggplot2 There is also a Datacamp course on ggpot2 Winston Chang’s R Graphic’s Cookbook is also an excellent resource for using ggplot2 for data visualisation A detailed software carpentry guide to R scripting "],["ch03.html", "Week 3 Changes in Allele and Genotype Frequency", " Week 3 Changes in Allele and Genotype Frequency Over the last two tutorials, we learned the basics of R and also how to manipulate, visualize and explore data. From this tutorial onwards, we will start change direction a little and reinforce some of the concepts of evolutionary genetics that you have been learning during the class sessions. This doesn’t mean we are going to throw you in at the deep-end and expect you to be completely relaxed in R, we will still take the opportunity to work through some of the concepts of the language we have already touched upon. Again remember, it is perfectly fine to ask for assistance or to look up R code you don’t understand - we use R everyday and turn to Google for solutions almost constantly! For the bulk of this session, we will focus on exploring the Hardy-Weinberg (HW) model and testing for deviations from Hardy-Weinberg Expectation (HWE) using R code. You will recall that the HW-model is basically an idealized model of the relationship between allele and genotype frequencies, in the absence of the action of demographic processes such as inbreeding, and evolutionary processes such as genetic drift or selection. The model therefore acts as null model for testing whether such processes have taken place. In other words, we can compare our real data to the expectation and draw inference on what demographic or evolutionary processes might be acting in the real world. As well as allowing us to flex our R abilities, examining the HW-model also requires us to perform some basic statistical analysis, particularly a goodness of fit test. In addition to the HW model and testing for deviations from HWE, we will also learn how to simulate genetic drift in R. Here we will make use of the visualisation skills we learned with ggplot2 in Chapter 2 to recreate Figure 3.7 from the textbook. We hope that this will have the dual benefit of letting you play with population parameters in order to understand genetic drift but also to help you develop your R programming skills in more detail. What to expect In this section we will: learn about for-loops in R explore the concept of Hardy Weinberg Equilibrium using R simulate genetic drift under the Hardy Weinberg expectation take a first look at how we can use R to program The tutorial is divided into two parts: A section focused on general R programming, where you learn concepts that can be used across many fields and even programming languages. A section focused on using R for evolutionary biology, where you apply some of the concepts you’ve learned in this tutorial and earlier to solve problems in evolutionary biology. In the second section, most of the code is quite straightforward, but you might encounter some more complicated code that you haven’t learned enough to understand yet. This will be clearly indicated, and we only expect you to have a general sense of what’s going on, not understand what every bit of the code does. "],["r-programming-for-loops.html", "3.1 R programming: for-loops", " 3.1 R programming: for-loops In this section we will learn about for-loops, which is an important concept for any kind of programming (not just in R!). It is made so you can follow it even if you’ve never heard of loops in programming before, but should also be useful for those who already have experience with loops in another programming language (like Python) and want to learn how it’s done in R. 3.1.1 Motivation: why loop? Let’s start by making a numeric vector that we want do do some operations on. x &lt;- seq(2, 20, 2) x #&gt; [1] 2 4 6 8 10 12 14 16 18 20 Say you want to multiply every element by 5. You’ve already learned to do that in R: x * 5 #&gt; [1] 10 20 30 40 50 60 70 80 90 100 However, what if we want to add two and two elements of the vector together? With our vector x, this would be 2+4, 4+6, 6+8 and so on. There is no simple way to do this that you’ve learned yet. We could use the square brackets to extract individual elements and do 9 calculations like this: x[1] + x[2] x[2] + x[3] x[3] + x[4] x[4] + x[5] x[5] + x[6] x[6] + x[7] x[7] + x[8] x[8] + x[9] x[9] + x[10] But this is a lot of typing for doing a repetitive task, which we want to avoid14. What if we could generalize this, so the computer does the repeated operations for us instead? That’s where for-loops come in. 3.1.2 How a for-loop works Before we go into solving our example, we have to learn a bit about for-loops. A for-loop in R conceptually looks like this: for (variable in vector){ # variable starts as the first element of vector # do something involving variable # the part within the curly brackets is called # the body of the for-loop } # when the curly bracket ends, variable becomes the next element of vector When the loop starts, variable is set to the first element of vector. Within the curly braces (“krølleparentes”) some operation is done using this variable. Then, after the closing curly bracket, variable becomes the next element in vector, and whatever is inside the curly braces gets repeated with this updated variable. This goes on until you’ve been through all elements of vector. Since it repeats an operation for all the elements of vector, we say that it “loops over” vector. This might be easier to understand if you see a real example of a for loop15: for (element in 1:10){ print(element) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 #&gt; [1] 6 #&gt; [1] 7 #&gt; [1] 8 #&gt; [1] 9 #&gt; [1] 10 Here you can see more of what’s actually happening. In the first round, element is the first element of 1:10, i.e. 1, which is printed. Then, element becomes the second element of 1:10, which is 2, and print() prints it. This goes on until you have looped over the entire vector, and the loop ends after printing 10. Below is what actually happens for each round of the loop. print(1) # round 1, element is 1 print(2) # round 2, element is 2 print(3) # round 3, element is 3 print(4) # ... and so on print(5) print(6) print(7) print(8) print(9) print(10) Notice how many lines you saved by writing a loop! The power here comes from that it doesn’t matter how long your vector is, and you can do any operation on the elements of the vector. Say we want to multiply each element of our vector x with 5, which we did in the start of this tutorial. for(element in x){ print(element * 5) } #&gt; [1] 10 #&gt; [1] 20 #&gt; [1] 30 #&gt; [1] 40 #&gt; [1] 50 #&gt; [1] 60 #&gt; [1] 70 #&gt; [1] 80 #&gt; [1] 90 #&gt; [1] 100 You can also use this with other kinds of vectors, e.g. a vector of strings. animals &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;horse&quot;, &quot;badger&quot;, &quot;unicorn&quot;) for (animal in animals){ print(animal) } #&gt; [1] &quot;cat&quot; #&gt; [1] &quot;dog&quot; #&gt; [1] &quot;horse&quot; #&gt; [1] &quot;badger&quot; #&gt; [1] &quot;unicorn&quot; Important concept: Use a for-loop to do the same operation over and over on the elements of your vector. The basic structure of a for-loop looks like this: for (variable in vector){ # do something } Tip: You may have noticed that I’ve called the variable that is changing for each iteration different things in all the examples, namely variable, element and animal. Actually, you can call this variable anything (within reason). All you have to remember is to call it the same thing within the loop as when starting it. In other words, this works16: for (whatever_you_want_to_call_the_variable in 1:10){ print(whatever_you_want_to_call_the_variable) } But this doesn’t: for (some_name in 1:10){ print(another_name) } Exercise: Create a vector containing the names of five countries. Use a for-loop to print the countries. Optional: use the paste() function to output “country is a country” for each element 3.1.3 Indexing with for-loops To solve our initial problem (and also for the things we will be doing later), we need to introduce one more concept: using the changing variable in your for-loop as an index for your vectors. If we look once more at our animals vector above, there are actually two ways of printing every element: # printing the element like we did earlier for (animal in animals){ print(animal) } # printing the element using an index for (index in 1:5){ print(animals[index]) } The execution of the latter for-loop looks like this: print(animals[1]) print(animals[2]) print(animals[3]) print(animals[4]) print(animals[5]) Note that rather than looping over the animals vector itself, we loop over a vector from 1 to 5, using those numbers to access the values inside animals. Here we made this vector by writing 1:5, but a better way would be writing 1:length(animals) so we can be sure that the index vector is the same length as the animals vector. Looping this way has the advantage that we can loop over several vectors at the same time: score &lt;- c(&quot;good&quot;, &quot;great&quot;, &quot;fine&quot;, &quot;best&quot;, &quot;probably not real&quot;) # looping over both animals and score # note the use of 1:length(animals) instead of 1:5 for (index in 1:length(animals)){ # paste together the current element in animals and score # grading will be overwritten every round of the loop grading &lt;- paste(animals[index], &quot;is&quot;, score[index]) print(grading) } #&gt; [1] &quot;cat is good&quot; #&gt; [1] &quot;dog is great&quot; #&gt; [1] &quot;horse is fine&quot; #&gt; [1] &quot;badger is best&quot; #&gt; [1] &quot;unicorn is probably not real&quot; We can also access more than one element of a vector at once, by using e.g. index - 1 to access the previous element. for (index in 2:length(animals)){ #note: starting on 2 friends &lt;- paste(animals[index], &quot;and&quot;, animals[index - 1], &quot;are friends&quot;) print(friends) } #&gt; [1] &quot;dog and cat are friends&quot; #&gt; [1] &quot;horse and dog are friends&quot; #&gt; [1] &quot;badger and horse are friends&quot; #&gt; [1] &quot;unicorn and badger are friends&quot; Show the code below to see how this would look if done manually. # first round, remember that writing animals[2 - 1] is # exactly the same as writing animals[1] friends &lt;- paste(animals[2], &quot;and&quot;, animals[2 - 1], &quot;are friends&quot;) print(friends) # second round friends &lt;- paste(animals[3], &quot;and&quot;, animals[3 - 1], &quot;are friends&quot;) print(friends) # third round friends &lt;- paste(animals[4], &quot;and&quot;, animals[4 - 1], &quot;are friends&quot;) print(friends) # fourth round friends &lt;- paste(animals[5], &quot;and&quot;, animals[5 - 1], &quot;are friends&quot;) print(friends) Now we know all we need to solve our initial problem, which we will return to in the next section. Exercise: In addition to your country vector from before, make a corresponding vector containing continents. Use indexing with for loops and the paste function to print “country is in continent” for each of the countries and continents in your vector. Important concept: Use for-loops with indexing when you want to access several elements of one or more vectors at once. Most of the time you will use indexing rather than looping over a vector directly. 3.1.4 Solving our problem To remind you of where we started: we want to add the adjacent elements of our vector x together in the smartest way possible. x &lt;- seq(2, 20, 2) Below is how this can be done with for-loops. I encourage you to try to solve it yourself before looking at the solution. Show hint Hint: Loop over the vector 2:length(x)17. For every round, add x[index] and x[index - 1] together and print this. for (index in 2:length(x)){ added &lt;- x[index] + x[index - 1] print(added) } #&gt; [1] 6 #&gt; [1] 10 #&gt; [1] 14 #&gt; [1] 18 #&gt; [1] 22 #&gt; [1] 26 #&gt; [1] 30 #&gt; [1] 34 #&gt; [1] 38 3.1.5 Storing values from a for-loop One final concept before going on to work with evolutionary biology. In the last section, we printed our results. What if we wanted to do work further with the results we got? We could copy the numbers from the console and create a new vector manually, but (like so many of my stupid suggestions throughout these tutorials) this is bothersome and doesn’t scale well. The best way to store values from a for loop is to create an empty vector before the loop, and fill in that vector as we loop. First we use the rep() function to create a vector containing NA. You can read the following rep(NA, 10) as “repeat NA, 10 times”, i.e. you get a vector of 10 NA. We use NA for our vector because our calculations within the loop will not produce any NA (unless something goes horribly wrong). This way, we can easily see if something went wrong in our loop (if there are any NA left in our results vector after our loop, something is probably off). # repeat 0, 10 times results &lt;- rep(NA, 10) # to ensure that results is the same length as x, we should instead write: results &lt;- rep(NA, length(x)) Then, we loop over the same index as before, but instead of printing our result, we store it as an element of our results vector. for (index in 2:length(x)){ results[index] &lt;- x[index] + x[index - 1] } This doesn’t print anything yet, but the results are now stored in the results vector. results #&gt; [1] NA 6 10 14 18 22 26 30 34 38 We see that the first element us still NA (since we started on the second element), which isn’t perfect, but good enough for now! That concludes this week’s R-focused part of the tutorial. Some of the Evolutionary biology-focused part will use the concepts you have learned here, so check back here if there is a part of the code you don’t understand. because it’s bothersome and there’s a great chance of making errors, and imagine how long it would take if our vector had 1000 or 10000 elements!↩︎ Note that to print your result to console inside a for-loop, you explicitly have to use print(), unlike what you’ve done so far. Something like for (element in 1:10){ element } won’t actually print anything.↩︎ but it’s kind of dumb, don’t do it↩︎ Something to think about: Why do we start the vector we’re looping over at 2, not 1? what would happen if we started at 1? (hint: what is 1 - 1, and does that correspond to an element in our vector?)↩︎ "],["evolutionary-biology.html", "3.2 Evolutionary biology", " 3.2 Evolutionary biology For this part of the tutorial, we need to load the tidyverse package. library(tidyverse) 3.2.1 The Hardy-Weinberg Model The HW model is a way of demonstrating the relationship between allele and genotype frequencies. It is mathematically basic but very important for understanding how different processes can alter these frequencies. As you might recall from the main text, the model has five major assumptions: All individuals in the population mate randomly. The population is infinitely large. No selection occurs. No mutation occurs. No gene flow occurs. Imagining our idealized population, we will focus on a single locus, \\(A\\) which has two alleles, \\(A_1\\) and \\(A_2\\). We can use \\(p\\) to denote the frequency of the \\(A_1\\) allele and \\(q\\) for \\(A_2\\). This means that if we know \\(p\\), we can derive \\(q\\) as \\(q = 1- p\\), since \\(p + q = 1\\). Remember that the HW-model is a means of representing the relationship between allele and genotype frequencies across generations. With our assumptions outlined above in place, the only thing that determines the frequency of genotypes is their probabilities, derived from the allele frequencies. It is quite simple to determine these probabilities. With 2 alleles \\(A_1\\) and \\(A_2\\), we have three possible genotypes and three ways to determine their probabilities as: \\(A_1A_1 = p^2\\) \\(A_1A_2 = 2pq\\) \\(A_2A_2 = q^2\\) Another way to look at these probabilities is as the expected frequencies of the three genotypes at Hardy-Weinberg equilibrium given the allele frequencies. Let’s use some R code to actually demonstrate how we might calculate these expected frequencies. For our test example, we will use a simple case where the frequency of \\(A_1\\), \\(p\\) is 0.8 and the frequency of \\(A_2\\), \\(q\\) is 0.2. # first we set the frequencies p &lt;- 0.8 q &lt;- 1 - p # check p and q are equal to 1 (q + p) == 1 # calculate the expected genotype frequencies (_e denotes expected) A1A1_e &lt;- p^2 A1A2_e &lt;- 2 * (p * q) A2A2_e &lt;- q^2 # show the genotype frequencies in the console c(A1A1_e, A1A2_e, A2A2_e) # since these are genotype frequencies, they should also sum to 1 - you can check this like so sum(c(A1A1_e, A1A2_e, A2A2_e)) So you can see it is quite easy to calculate the frequencies you need for the HW model in R. If you want to, you can play around with the initial frequencies defined at the start of the R code to see how it alters the genotype frequencies. This is one nice thing about using R for evolutionary genetics, it is pretty simple to demonstrate things mathematically to benefit your own understanding. However, it might be even more useful to demonstrate the relationships the HW model lays out graphically. That way you can see how the expected genotype frequencies are altered over a whole range of allele frequencies. Luckily for us, R is great for this sort of visualisation and this is an excellent opportunity to demonstrate some programming in R. 3.2.1.1 Plotting the expected genotype frequencies The first thing we need to do is generate a range of allele frequencies to visualize the expected genotype frequencies across. We’ll do this in the simplest way first, before going into how you might take a programming approach to the same problem. Either way, generating a range is simple - we just need to do it for a single allele, \\(A_1\\) and then we can very easily derive the frequency of \\(A_2\\). We will do this like so: # generate a range for p p &lt;- seq(0, 1, 0.01) # and also for q q &lt;- 1 - p Here we used the seq() function, which we first saw back in Chapter 1. Remember, we only have to do it once because \\(q\\) is the inverse of \\(p\\). Next we need to generate the expected frequencies. Because R is great at handling vectors, the code for this is identical for if we were using a single value. # generate the expected genotype frequencies A1A1_e &lt;- p^2 A1A2_e &lt;- 2 * (p * q) A2A2_e &lt;- q^2 So now we have the expected frequencies at HWE of our different genotypes across a range of allele frequencies with a little vector calculation. However, recalling from before that we often need to get our data into a data.frame in order to use ggplot2, we need to do a bit of work on this first. We’ll now get our data into a data.frame ready for working on… # arrange allele frequencies into a tibble/data.frame geno_freq &lt;- data.frame(p, q, A1A1_e, A1A2_e, A2A2_e) You also might remember from the previous session that we need to sometimes do some data manipulation to get the data into a form that we can easily plot. Luckily, this is quite straightforward in this case as we can use the pivot_longer function to do it quickly and easily. # Use pivot_longer to reshape the data.frame for straightforward plotting geno_freq &lt;- pivot_longer(geno_freq, c(A1A1_e, A1A2_e, A2A2_e), names_to = &quot;genotype&quot;, values_to = &quot;freq&quot;) pivot_longer() commands can take a while to get the hang of, the best thing is to continually practice them like this! If you’re unsure why we use pivot_longer() or how it works, you can always go back to the explanation from week 2. Now it is really straightforward to make a plot that shows the expected HW—frequencies across our allele frequencies. # plot the expected genotype frequencies a &lt;- ggplot(geno_freq, aes(p, freq, colour = genotype)) + geom_line() + labs(x = &quot;p frequency&quot;, y = &quot;Genotype frequency&quot;) # print the plot with some theme options to make it pretty a + theme_light() + theme(legend.position = &quot;bottom&quot;) Figure 3.1: Relationship between allele frequency and genotype frequency under Hardy-Weinberg equilibrium. From this graph, you can find the expected genotype frequencies for any value of \\(p\\). Note for instance how at \\(p=0.5\\), the frequencies are 0.25 for each of the homozygotes, and 0.5 for the heterozygote. At fixation of \\(A_1\\) (\\(p=1\\)), you can see that we (of course) only have A1A1 homozygotes, and vice versa for \\(p=0\\). 3.2.2 Testing for deviations from the Hardy Weinberg Expectation We’ve already learned that the HW model is essentially an idealised scenario where no demographic or evolutionary processes are shaping the relationship between allele and genotype frequencies. We can therefore use it as a null model against which to test our data. Basically, we can compare our observed genotype frequencies from those expected under HWE. To get the expected frequencies, we just generate them from the allele frequencies, as we did in the previous section of the tutorial. We can work through an example of this together. Once again we’ll assume a locus \\(A\\) with two alleles, \\(A_1\\) and \\(A_2\\). This means three genotypes, \\(A_1A_1\\), \\(A_1A_2\\) and \\(A_2A_2\\). We sample 150 individuals from a population. The next block of R code shows the numbers of each genotype we collected and also combines them into an observed vector - for later use. # numbers of genotypes A1A1 &lt;- 80 A1A2 &lt;- 15 A2A2 &lt;- 55 # make into a vector observed &lt;- c(A1A1, A1A2, A2A2) In order to go further here, we need to work out the allele frequencies from our observed data. This is quite straightforward - homozygotes have two copies of an allele whereas heterozygotes have only one. So we derive the number of each allele and divide it by the total: \\(\\displaystyle p = \\frac{2(A_1A_1) + A_1A_2}{n}\\) \\(\\displaystyle q = \\frac{2(A_2A_2) + A_1A_2}{n}\\) Where the genotype notation here represents the number of genotype class and \\(n\\) is the total number of alleles. We can calculate the frequency the alleles in R like so: # calculate total number of alleles n &lt;- 2*sum(observed) # calculate frequency of A1 or p p &lt;- (2*(A1A1) + A1A2)/n # calculate frequency of A2 or q q &lt;- (2*(A2A2) + A1A2)/n # print frequencies to screen c(p, q) Note that with the code above, we summed the number of observed genotypes and then multiplied by 2 as there are 2 alleles for each individual (i.e. there are 150 individuals and 300 alleles). We now have the allele frequencies, so we can use the code we worked out in the previous section to calculate the expected Hardy-Weinberg frequencies for this population. # generate the expected genotype frequencies A1A1_e &lt;- p^2 A1A2_e &lt;- 2 * (p * q) A2A2_e &lt;- q^2 # combine into a vector expected_freq &lt;- c(A1A1_e, A1A2_e, A2A2_e) So now that we have the expected genotype frequencies, the last thing we need to do before testing whether there is a deviation from HWE in our data is calculate the expected number of each genotype. This is easy - we just multiply the frequencies by the number of individuals we sampled - 150. # calculate observed genotype frequencies expected &lt;- expected_freq * 150 The next thing we’d like to do is see whether our observed genotypes deviate from the expected. We will test this formally, but first let’s just look at the differences between them. The simplest way to do this is combine them into a matrix: # combine observed and expected frequencies into a matrix mydata &lt;- cbind(observed, expected) # add rownames rownames(mydata) &lt;- c(&quot;A1A1&quot;, &quot;A1A2&quot;, &quot;A2A2&quot;) mydata #&gt; observed expected #&gt; A1A1 80 51.04167 #&gt; A1A2 15 72.91667 #&gt; A2A2 55 26.04167 Well, just from eyeballing this matrix, we see that there is a difference between the expected and the observed genotypes. Notably there seems to be a lower frequency of heterozygotes than we expect. Still, just looking at this is not enough, we need to test it formally. As you will probably remember from the main text, we can use a Chi-squared goodness of fit test for this purpose. When we use a statistical test like the Chi-squared test, we want to find out how likely it is that we would get our observed values when we assume that our null hypothesis is true. Here, our null hypothesis is the Hardy-Weinberg expectation (\\(p^2\\), \\(2pq\\), \\(q^2\\)). These are the genotype frequencies we would expect to see if the population was in Hardy-Weinberg equilibrium. If the difference between the observed data and the expectation from the null hypothesis is large enough, we can say that it is statistically significant. What is “large enough” is what our chi squared test will tell us – this is where the p-value comes in. If we get a low p-value, we can reject the null hypothesis and conclude that our population is not in Hardy-Weinberg equilibrium. p-values: A p-value tells us the probability of getting our observed result when we assume that the null hypothesis is true. For example, a p-value of 0.5 means that if we randomly sampled 150 individuals from a population in Hardy-Weinberg equilibrium, we would get results like ours or more extreme 50% of the time. We typically say that if the p-value is below 0.05, the results are statistically significant. If our p-value is 0.05, it means there is a probability of only 5% that we would get these results if our null hypothesis was true.18 Important: it may be confusing that both the probability in hypothesis testing and the proportion of the \\(A_1A_1\\) allele in a population is called p. These are two completely different statistics, so make sure you don’t get them mixed up! In the book, we calculated our chi-squared manually. In R, we can simply use the function chisq.test(): # perform a chi-squared test mychi &lt;- chisq.test(observed, p = expected_freq) Note that what we put into the function is the number of observed genotypes (stored in our observed variable) and the frequency of the expectation (stored in expected_freq). Then we can have a look at the results: mychi #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: observed #&gt; X-squared = 94.633, df = 2, p-value &lt; 2.2e-16 We see that the p-value is very low - well below our limit of 0.05. This means we have a significant deviation from HWE - in other words, some assumption of the Hardy-Weinberg model is violated. What process could cause this significant heterozygote deficit? And conversely, what about the excess of homozygotes? Maybe non-random mating could be a possible reason? 3.2.3 Simulating genetic drift One of the main assumptions of the HW model is that populations are infinite. This is obviously unrealistic! All populations are finite - but what does this mean for how evolution unfolds? An important concept in evolutionary genetics is how population size can influence the relative power of a process such as genetic drift. Drift essentially describes the random component of survival in a population - some individuals will survive and reproduce more than others by chance. An important factor contributing to such genetic drift is the random sampling of alleles that takes place at fertilization. For instance, just like some pairs produce an excess of sons and daughters, a heterozygous mating pair (\\(A_1A_2\\)) can produce offspring with an excess of the genotypes \\(A_1A_1\\) (or \\(A_2A_2\\)) thereby causing a random change in allele frequency in the population. When a population is large, the effects of drift will be small relative to other processes such as selection. However, when a population is small, drift as a result of random chance has a much bigger effect. One of the best ways to get your head around a concept like genetic drift and how it interacts with population size is to actually simulate, which is exactly what we are going to do here. We’ll first explain how we can simulate it over a single generation and then we are going to use a for-loop to do all the leg work for us. So we’ll not only be exercising your understanding in evolutionary genetics, but your R programming chops too. 3.2.3.1 A simple case of genetic drift over a single generation using the binomial distribution To demonstrate a simple case of drift, we’ll use the example from the book. We have a population of N = 8 individuals and we will focus on a single locus A with two alleles, \\(A_1\\) and \\(A_2\\). We will assume both alleles are equally common in the population - i.e. \\(p=q = 0.5\\). In the book you will recall we used a coin-flipping experiment to sample our alleles to form genotypes, since both alleles have a frequency of 0.5. Now we will do something similar, but instead of sampling alleles to form genotypes, we will sample the number of \\(A_1\\) alleles in the next generation directly (we call this \\(p’\\)). In the example in the book, we defined heads as \\(A_1\\) and tails as \\(A_2\\). The principle here is the same, we start with a population where \\(p = q\\), so there is an equal probabilty of sampling one allele or the other. You can imagine each coin flip representing the passing on of one allele from a parent to the next generation. If we do this twice for each individual in the population, the sum of the heads we get will be the number of A1 alleles in the next generation. This is a form of binomial sampling, and to do it ir R we can use the rbinom() function. rbinom() essentially simulates a coin flip. However, while in a regular coin flip the probability of heads and tails is always 0.5, rbinom() allows for changing these probabilities. Simplified, the following code says: “flip 1 coin (n) 16 times (size) with a probability of 0.5 (prob) for getting heads”. The output is the number of heads we got. # recreate coin flipping experiment using binomial sampling rbinom(n = 1, size = 16, prob = 0.5) Run this code a few times, and observe that the results are different each time, because the rbinom() function samples randomly. In our example, the output of rbinom could represent the number of \\(A_1\\) alleles in the next generation – so we could easily work out \\(p´\\) and \\(q´\\), (i.e. \\(p\\) and \\(q\\) in the next generation). We can use the following code to sample the number of \\(A_1\\) in generation 2: # set population size N &lt;- 8 # set allele frequency p &lt;- 0.5 # recreate coin flipping experiment using binomial sampling nA1 &lt;- rbinom(1, size = 2*N, prob = p) Remember since we are sampling alleles for diploid individuals, we need to multiply N by 2. The value of nA1 will differ for everyone since this is a random sample of the binomial distribution. We can now easily work out the value of \\(p´\\) - i.e. the new frequency of the \\(A_1\\) in the next generation. Note that R doesn’t allow the prime notation so for the actual R code, we will use p2. # calculate p in generation 2 p2 &lt;- nA1/(2*N) OK, so now we have a value for p2 – again it will differ for everyone because we performed random sampling. You will see though that p2 is likely different to p solely because of random sampling - i.e. we have demonstrated drift over a single generation. Then we can go on to calculate \\(p\\) in the third generation. Our new p2 will then be the prob argument of rbinom(), and everything else can be left unchanged: # draw a new random sample nA1 &lt;- rbinom(1, size = 2*N, prob = p2) # calculate p in generation 3 p3 &lt;- nA1/(2*N) p3 #&gt; [1] 0.3125 If we were to to properly understand drift over more than one generation, we could now have used our p3 to find p in the fourth generation, use the new \\(p\\) to calculate \\(p\\) in the next generation, and so on until we have simulated for all the number of generations we want. # draw a new random sample nA1 &lt;- rbinom(1, size = 2*N, prob = p3) # calculate p in generation 4 p4 &lt;- nA1/(2*N) p4 #&gt; [1] 0.125 # draw a new random sample nA1 &lt;- rbinom(1, size = 2*N, prob = p4) # calculate p in generation 5 p5 &lt;- nA1/(2*N) p5 #&gt; [1] 0.1875 # draw a new random sample nA1 &lt;- rbinom(1, size = 2*N, prob = p5) # calculate p in generation 5 p6 &lt;- nA1/(2*N) p6 #&gt; [1] 0.375 # and so on ... But hopefully, you’re thinking: “that sounds like a lot of work, maybe we can use a for-loop instead?”. We can! 3.2.3.2 Using a for-loop to simulate drift over multiple generations Let’s do try to wrap all that tedious code in a for loop instead. If we simplify our problem by a lot, what we want to do is: “use p from the previous generation to calculate p in this generation”. If your p-values are in a vector p, and you are using indexing like we learned in section 3.1.3, “p from the previous generation” is p[i-1], and “p in this generation” is p[i]. The contents of our for-loop would then be: nA1 &lt;- rbinom(1, size = 2*N, prob = p[i - 1]) p[i] &lt;- nA1/(2*N) All that remains then is to create the for-loop itself. See if you manage to make this yourself before looking at my code! Show hint Hint: start by creating a vector p to store your results. The length of this vector determines how many generations you use in your simulation. Then, set the first element of p to 0.5 Show another hint Hint: start the vector you’re looping over at 2, and end at the number of generations (i.e. the length of your p vector) # set population size N &lt;- 8 # set number of generations ngen &lt;- 100 # set initial frequency of A1 p_init &lt;- 0.5 # create vector for storing results p &lt;- rep(NA, ngen) # set first element to initial p p[1] &lt;- p_init for (i in 2:ngen){ # sample number of A1 alleles based on p in previous generation nA1 &lt;- rbinom(1, 2*N, p[i-1]) # set frequency of A1 as p in the current generation p[i] &lt;- nA1 / (2*N) } p #&gt; [1] 0.5000 0.6250 0.5625 0.6875 0.6875 0.6875 0.7500 0.6250 0.6875 0.6250 #&gt; [11] 0.6250 0.6250 0.7500 0.8750 0.8125 0.8125 0.8125 0.9375 0.7500 0.8750 #&gt; [21] 0.9375 0.7500 0.5625 0.2500 0.3750 0.5000 0.5625 0.5625 0.3750 0.2500 #&gt; [31] 0.1875 0.1250 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 #&gt; [41] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 #&gt; [51] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 #&gt; [61] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 #&gt; [71] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 #&gt; [81] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 #&gt; [91] 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Note that I assigned the population size N, the number of generations ngen and the initial frequency of \\(A_1\\) p_init to named objects. This way, if you want to change either of these, you only need to change them in one place! 3.2.3.3 Visualizing drift Now that we our for-loop to simulate data across generations, we can easily do this for different population sizes. Let’s simulate 1000 generations of drift for three populations of 10, 100 and 1000 individuals. We’ll keep the initial allele frequency, p at 0.5 in all cases. We will then combine them all into a data.frame alongside the number of generations. I will skip most of the comments in the code here so the code doesn’t take up too much space, but remember that you can look at the code in the previous section if you wonder what something does. First, for a population size of 10, storing the results to an object called n10 # Population size 10 N &lt;- 10 ngen &lt;- 1000 p_init &lt;- 0.5 n10 &lt;- rep(NA, ngen) n10[1] &lt;- p_init for (i in 2:ngen){ nA1 &lt;- rbinom(1, 2*N, n10[i-1]) n10[i] &lt;- nA1 / (2*N) } Then we do the same for a population size of 100. # Population size 100 N &lt;- 100 ngen &lt;- 1000 p_init &lt;- 0.5 n100 &lt;- rep(NA, ngen) n100[1] &lt;- p_init for (i in 2:ngen){ nA1 &lt;- rbinom(1, 2*N, n100[i-1]) n100[i] &lt;- nA1 / (2*N) } And finally for population size 1000. # Population size 1000 N &lt;- 1000 ngen &lt;- 1000 p_init &lt;- 0.5 n1000 &lt;- rep(NA, ngen) n1000[1] &lt;- p_init for (i in 2:ngen){ nA1 &lt;- rbinom(1, 2*N, n1000[i-1]) n1000[i] &lt;- nA1 / (2*N) } As you may remember, we need to make it into a data frame in order to plot it with ggplot2. We also add a column called g that contains the number of generations. # get number of generations g &lt;- seq(1, 1000, 1) # combine into a data.frame mydrift &lt;- data.frame(g, n10, n100, n1000) Anyway, what we ultimately want to do is visualise drift over time. So we need to get our data in a position to plot it using ggplot2 - once again, we turn to pivot_longer(). pivot_longer() in this case takes all our simulations (which at the moment are in different columns) and combines them into a single column p. A new column pop_size is made which indicates the name of the column where the data came from. # use pivot_longer to get data ready for plotting mydrift_long &lt;- pivot_longer(mydrift, -g, names_to = &quot;pop_size&quot;, values_to = &quot;p&quot;) Now we will plot our data using the following code: # plot data p &lt;- ggplot(mydrift_long, aes(g, p, colour = pop_size)) + geom_line() + labs(x = &quot;No. generations&quot;, y = &quot;Allele freq (p)&quot;) # print nicely with some theme options p + theme_bw() + theme(legend.position = &quot;bottom&quot;) Figure 3.2: A simulation of genetic drift over 1000 generations with different population sizes. And there we have it! A plot of our simulated data showing allele frequency changes over time as a result of genetic drift. Although the plot you get will differ from the one you see here (because of the random sampling), what you should be able to see is that with a low population size alleles are either lost or go to fixation very rapidly. This is because the effect of drift is much stronger here, whereas in larger populations it may take a long time for drift to fix or remove an allele - indeed in some cases it will not happen at all. Keep in mind that the value 0.05 is arbitrary, and that other (often lower) values are often used in other fields. Researchers simply decided that 5% is an acceptable degree of error. This also means that 5% of studies conducted (where the statistics are done correctly) will come to the wrong conclusion.↩︎ "],["study-questions-1.html", "3.3 Study questions", " 3.3 Study questions The study questions for week 3 are found here. Deliver them in Canvas before the deadline as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-2.html", "3.4 Going further", " 3.4 Going further There are a lot of great resources available on programming in R and evolutionary genetics. There are also some good further tutorials on understanding HWE and drift in R too. Below is a collection of resources you might find of use. The software carpentry github has an extensive set of tutorials for learning R programming Datacamp’s loop tutorial An excellent and very detailed primer on population genetics in R Graham Coop at UC Davis has extensive population genetic notes on HWE and also genetic drift "],["ch04.html", "Week 4 The Theory of Natural Selection", " Week 4 The Theory of Natural Selection Understanding natural selection is fundamental to properly understanding evolution. The basics of natural selection are relatively straightforward but surprisingly easily misunderstood. Our aim with this practical session is to use the R environment to actually demonstrate some models of selection and to also simulate evolution by natural selection. Thus we hope you’ll start to see how R can be useful for helping you grasp difficult theoretical concepts as well as handling data. What to expect In this section we will: Learn about creating your own functions in R recreate fitness functions in R to develop our understanding of natural selection use the one-locus viability model of selection to simulate evolution model overdominance and underdominance continue to practice your R skills Getting started The first thing we need to do is set up the R environment. We won’t be using anything other than base R and the tidyverse package today. So you’ll need to load the latter. library(tidyverse) "],["r-programming-making-custom-functions.html", "4.1 R programming: Making custom functions", " 4.1 R programming: Making custom functions 4.1.1 Motivation In last week’s tutorial and assignment, you used a for-loop to simulate genetic drift. If you did the assignment, you are probably tired of seeing code-snippets looking like this or similar: N &lt;- 10 ngen &lt;- 1000 p_init &lt;- 0.5 n10 &lt;- rep(NA, ngen) n10[1] &lt;- p_init for (i in 2:ngen){ nA1 &lt;- rbinom(1, 2*N, n10[i-1]) n10[i] &lt;- nA1 / (2*N) } Although the for-loop saved you thousands of lines of code compared to writing out the simulation manually, you still had to copy and paste the for loop itself many times. You had to write it 3 times for the tutorial and another 3 times for the assignment, just changing a single number each time! You may have thought that there has to be a better way of doing this, and there is! You can make this simpler by making your own custom functions, which you will learn about in the R-part of this tutorial. 4.1.2 Function basics You have already used a lot of functions in R, with names like mean(), sum() and ggplot(). Common for these is that they take some input—arguments—does something with that input and returns the result. For making your own function, you need three things: A name for your function. This could be anything, but it’s best to give them a sensible name. The arguments to your function. These are the values that you want to be able to change each time you run the function. What the function does with the arguments and what it returns. This is called the “body” of the function. A basic custom function typically looks like this: function_name &lt;- function(argument){ #the body of the function is inside curly brackets #here you do something with the arguments, for example: result &lt;- argument^2 # functions typically end with returning some value return(result) } The above function will take an input value, square it, and return the result. You can now use the function by writing function_name(somevalue), like you would with the functions you’ve encountered so far. The arguments go into the parentheses of function(), and the body is inside curly brackets. The value to return is wrapped inside return() Note that the name of the function and arguments can be whatever you’d like. You can also have as many arguments as you want, separated by comma. Important concept: Custom functions makes a piece of code reusable, either for doing something many times, or for using for other purposes later. A function takes one or more arguments, does something with the arguments in the body and returns the result. 4.1.3 A simple example Let’s make a simple function that allows you to see clearly what the function is doing. The following function takes an argument, and prints it in a sentence: print_arg &lt;- function(argument){ # make a string explaining what the argument is sentence &lt;- paste(&quot;the argument is:&quot;, argument) # return the string return(sentence) } When you run this code, nothing is printed, but the function has been saved. You can now call your new function with any argument. print_arg(5) #&gt; [1] &quot;the argument is: 5&quot; # you can store the returned value to an object and print it horse_sentence &lt;- print_arg(&quot;horse&quot;) horse_sentence #&gt; [1] &quot;the argument is: horse&quot; # you can put any expression as the argument, # for example a logical statement print_arg(pi &gt; exp(1)) #&gt; [1] &quot;the argument is: TRUE&quot; You can also reference the argument by name when calling the function, like you learned about in week 1. print_arg(argument = &quot;zebra&quot;) #&gt; [1] &quot;the argument is: zebra&quot; While this particular function is quite useless, notice how much typing you have saved already. We will move on to some more useful functions shortly, after an exercise. Exercise: create a new function called print_args() that takes two arguments as input, and returns the text “the first argument is [argument1] and the second is [argument2]”. print_args &lt;- function(argument1, argument2){ # make a string sentence &lt;- paste(&quot;the first argument is:&quot;, argument1, &quot;and the second is&quot;, argument2) # return the string return(sentence) } # test the code print_args(5, &quot;horse&quot;) #&gt; [1] &quot;the first argument is: 5 and the second is horse&quot; 4.1.4 Some important function properties One important thing about functions is that any objects you create only exist inside the function. This means that even though the above function creates an object named sentence, and we’ve run the function many times, there is no object called sentence in your R session: sentence #&gt; Error in eval(expr, envir, enclos): object &#39;sentence&#39; not found This means that you don’t need to worry about naming conflicts when running a function, which is convenient. However, it also means that you can’t access any objects that are made inside your function, only whatever you put inside return() Another important thing is that you can only return 1 thing from a function. This means that if you want to return multiple objects, you need to make these into a vector. In the following three examples, only the last function will work as intended: return_2 &lt;- function(x, y){ return(x, y) # will produce an error, you can only return a single object! } return_2(2, 4) #&gt; Error in return(x, y): multi-argument returns are not permitted return_twice &lt;- function(x, y){ return(x) # x is returned, and the function stops return(y) # the function stops before this line, so y is lost forever } return_twice(2, 4) #&gt; [1] 2 return_vector &lt;- function(x, y){ return_vec &lt;- c(x, y) return(return_vec) # this is OK, only one object is returned } return_vector(2, 4) #&gt; [1] 2 4 Important concept: Any variables you create inside a function only exists within that function. If you want to return more than 1 value, you need to bind the values together in e.g. a vector. 4.1.5 A slightly more useful example A more useful function is often one that does some calculations and returns the result. Here’s an example of a function that takes two arguments and divides the first by the other. divide &lt;- function(numerator, denominator){ result &lt;- numerator/denominator return(result) } divide(5, 3) #&gt; [1] 1.666667 divide(10, 0) #&gt; [1] Inf # oops Exercise: create a function that takes two arguments and returns the sum of those arguments. Tip: call it something other than sum (e.g. add) to avoid overwriting R’s built in sum function. add &lt;- function(x, y){ result &lt;- x + y return(result) } 4.1.6 Example: calculating genotype frequencies In last week’s tutorial, you used the following snippet of code to calculate genotype frequencies from allele frequencies: # first we set the frequencies p &lt;- 0.8 q &lt;- 1 - p # calculate the expected genotype frequencies (_e denotes expected) A1A1_e &lt;- p^2 A1A2_e &lt;- 2 * (p * q) A2A2_e &lt;- q^2 # show the genotype frequencies in the console c(A1A1_e, A1A2_e, A2A2_e) #&gt; [1] 0.64 0.32 0.04 This is something I imagine you will do often in this course and later in life if you pursue evolutionary biology, so it would make sense to make it into a function. I will let you try to do it on your own before showing you how to do this. Use the hints if you get stuck! Exercise: create a function that takes a single argument, p, and returns the expected genotype frequencies according to the Hardy-Weinberg expectation. Show hint Start by wrapping the whole code snippet above inside the curly brackets of a function. Show another hint Remember that you can only return 1 object, so you need to make the genotype frequencies into a vector. Also, remember that you should be able to change p, so you should not have the line p &lt;- 0.8 in the body of the function. calc_geno &lt;- function(p){ # calculate q from p q &lt;- 1 - p # calculate the expected genotype frequencies (_e denotes expected) A1A1_e &lt;- p^2 A1A2_e &lt;- 2 * (p * q) A2A2_e &lt;- q^2 # return the genotype frequencies geno_freq &lt;- c(A1A1_e, A1A2_e, A2A2_e) return(geno_freq) } When testing your function, it should show the following output: calc_geno(0.2) #&gt; [1] 0.04 0.32 0.64 calc_geno(0.5) #&gt; [1] 0.25 0.50 0.25 calc_geno(0.7) #&gt; [1] 0.49 0.42 0.09 Congratulations, you have now made your first truly useful function! Any time you need to calculate genotype frequencies from allele frequencies from now on, it will be a breeze! Important concept: Making a function isn’t all that hard when you get used to it. A general recipe for converting code into a function is: Paste your code inside the function body. Make any objects you want to be able to change into an argument of the function (p in this example). Remember to remove those objects from the body of the function to avoid overwriting your arguments. Wrap your result in return(). 4.1.7 Creating a function of the drift simulation Now we’re ready to make a function to simulate drift. Just to remind you, this is the code we used last week for the simulation: # set population size N &lt;- 8 # set number of generations ngen &lt;- 100 # set initial frequency of A1 p_init &lt;- 0.5 # create vector for storing results p &lt;- rep(NA, ngen) # set first element to initial p p[1] &lt;- p_init for (i in 2:ngen){ # sample number of A1 alleles based on p in previous generation nA1 &lt;- rbinom(1, 2*N, p[i-1]) # set frequency of A1 as p in the current generation p[i] &lt;- nA1 / (2*N) } p Notice that there are three parameters we will probably want to be able to change for each simulation: N, ngen and p_init. It makes sense, then, to make these into function arguments. Exercise: Make the above drift simulation into a function. It should take three arguments: N, ngen and p_init, and return a vector of p’s for each generation. Show hint Use the three steps from the previous section: paste the code into the body, turn N, ngen and p_init into arguments and return your result. drift_sim &lt;- function(N, ngen, p_init){ # create vector for storing results p &lt;- rep(NA, ngen) # set first element to initial p p[1] &lt;- p_init for (i in 2:ngen){ # sample number of A1 alleles based on p in previous generation nA1 &lt;- rbinom(1, 2*N, p[i-1]) # set frequency of A1 as p in the current generation p[i] &lt;- nA1 / (2*N) } return(p) } That concludes the R-part of this tutorial! The rest of the tutorial will be about fitness and selection, but will make use of functions for exploring this. Remember to check back here if you become unsure about how functions work! Exercise: when working through the rest of the tutorial, think once in a while: “Will I need to do this many times? Could I make this code into a function? How would I do that?” "],["evolutionary-biology-fitness.html", "4.2 Evolutionary biology: fitness", " 4.2 Evolutionary biology: fitness 4.2.1 Understanding fitness What do we mean by fitness? In its simplest form, fitness is defined as whether or not an organism is able to reproduce. Fitness is often mistaken as an individual attribute, but it is actually better explained as a difference in reproductive success among characters, traits or genotypes. If genotype \\(A_1A_1\\) produces more offspring than \\(A_2A_2\\) because of a trait the locus produces, we can say \\(A_1A_1\\) has a higher fitness. 4.2.1.1 Absolute, relative and marginal fitness So we might talk about absolute fitness - e.g. the expected reproductive success of \\(A_1A_1\\). In population genetic models though, we more often than not refer to relative fitness - i.e. how fit genotypes are relative to one another. To denote relative fitness, we will follow the notation in the main text - i.e. \\(w_{ij}\\) for \\(A_iA_j\\). Assuming locus \\(A\\) with two alleles, on average genotypes \\(A_1A_1\\) and \\(A_1A_2\\) produce 16 offspring each, whereas \\(A_2A_2\\) produces 11 offspring on average. We calculate relative fitness as follows: # define the number of offspring per genotype a &lt;- c(A1A1 = 16, A1A2 = 16, A2A2 = 11) # find the maximum fitness max_fit &lt;- max(a) # determine the relative fitness rel_fit &lt;- a/max_fit Note here that when defining our vector a, we also actually named each element of it. Names work a little differently in vectors to how they do in a data.frame. For example, a$A1A1 will not work. However a[\"A1A1\"] does. When calculating relative fitness, we define fitness relative to the maximum fitness. Since both \\(A_1A_1\\) and \\(A_1A_2\\) produce the highest number of offspring, their fitness is 1 whereas \\(A_2A_2\\) has a lower relative fitness. We might also want to calculate the mean population fitness, denoted as \\(\\overline{w}\\). This is essentially the sum of the relative fitness of each genotype multiplied by the genotype frequency. With R, calculating this is simple - we simply multiply a vector of genotype frequencies with the relative fitness and sum the result. We do this below: # define the genotype frequencies - note different way to define! geno_freq &lt;- c(A1A1 = 0.65, A1A2 = 0.15, A2A2 = 0.2) # calculate mean population fitness w_bar &lt;- sum(rel_fit * geno_freq) So far, we have dealt with fitness for genotypes. This makes sense because selection acts on genotypes (and the phenotypes they convey). But what if we want to define the fitness of a specific allele? In this case, things become a bit more complicated because allelic fitness depends on the genotype the allele finds itself in. In our previous example, \\(A_2\\) has a high fitness when it is in the heterozygous \\(A_1A_2\\) genotype, but not when it is homozygous. To account for this, we estimate marginal fitness for a given allele \\(i\\) as \\(w^{*}_i\\). So for two alleles, the marginal fitness is: \\(w^{*}_1 = pw_{11} + qw_{12}\\) \\(w^{*}_2 = pw_{12} + qw_{22}\\) Where \\(p\\) and \\(q\\) are the frequencies for \\(A_1\\) and \\(A_2\\) respectively. In other words, marginal fitness is a component of the fitness of the genotypes an allele occurs in AND the frequency of those genotypes. Let’s calculate the fitness of our alleles using R. ## first calculate the allele frequencies # define the total number of alleles n &lt;- 2*sum(geno_freq) # calculate p p &lt;- ((geno_freq[&quot;A1A1&quot;] * 2) + geno_freq[&quot;A1A2&quot;])/n # calculate q q &lt;- 1 - p ## now calculate the marginal fitness w1 &lt;- (p*rel_fit[&quot;A1A1&quot;]) + (q*rel_fit[&quot;A1A2&quot;]) w2 &lt;- (p*rel_fit[&quot;A1A2&quot;]) + (q*rel_fit[&quot;A2A2&quot;]) Note again that we are explicitly naming elements of the vector to make the mathematics here clearer to you. However this can cause some annoying names to follow around your data. Take a look at w1 and w2 - you should see they have genotype names. The names still hang about because we used them right from the start, so we also remove them here by assigning a NULL: # strip names names(w1) &lt;- NULL names(w2) &lt;- NULL names(p) &lt;- NULL names(q) &lt;- NULL Now that we have the basics in place in terms of calculating fitness, we can go on to create a one-locus model of viability selection. 4.2.2 One-locus model of viability selection When we talk about viability, we simply mean that individuals with different genotypes will vary in their probability of surviving until they are able to reproduce. Variation in viability will therefore effect the ability of individuals to reproduce and thus pass on their genes to the next generation. In other words, allele frequencies will change as a result of selection. To model this, we will use a locus \\(A\\) with alleles \\(A_1\\) and \\(A_2\\) - each with a frequency \\(p\\) and \\(q\\). From the last section, we have the relative fitness of the genotypes with \\(w_{11}\\), \\(w_{12}\\) and \\(w_{22}\\). For simplicity we will assume that fitness remains constant and that the frequency of the zygotes at each generation are in line with the Hardy-Weinberg expectation. Like in the book, we summarise the basics in the table below: Genotype Zygote frequency Fitness \\(A_1A_1\\) \\(p^2\\) \\(w_{11}\\) \\(A_1A_2\\) \\(2pq\\) \\(w_{12}\\) \\(A_2A_2\\) \\(q^2\\) \\(w_{22}\\) In our model, we will calculate \\(p_{t+1}\\) the frequency of allele \\(A_1\\) after a single generation of selection. This depends on three things: \\(p\\) - the allele frequency before selection, \\(w_1{^*}\\), the marginal fitness of the \\(A_1\\) allele and the difference between this and \\(\\overline{w}\\), the mean population fitness. So our model is basically: \\(P_{t+1} = \\displaystyle \\frac{pw_1^*}{\\overline{w}}\\) We will use our previous results to calculate how \\(p\\) changes after a round of selection: p_t &lt;- (p*w1)/w_bar If you compare p and p_t, you will see how the frequency in p changed as a result of selection. In fact, this is \\(\\Delta p\\). In R, we can easily calculate it as: delta_p &lt;- p_t - p If everything worked correctly, your delta_p should be 0.0372093. 4.2.2.1 Simulating selection under the one-locus model So far, we have recreated the model for a single generation to try and understand how it works. But the beauty of R is that we can easily change the parameters to see how this will vary the change in frequency of our allele. The easiest way to do this is if we write a function that neatly summarises the code we have already explored - then all we need to do to see how the parameters have an effect is simply alter the arguments we give our function. We will now write a function that takes the initial frequency of p and a vector consisting of the relative fitness of each genotype. This function will then calculate the allele frequencies, the mean population fitness and the marginal fitness of the alleles. Note how this is essentially all the code we have made so far pasted into the body of a function, and p and rel_fit is turned into arguments. # a simple function to demonstrate the one locus selection model selection_model_simple &lt;- function(p, rel_fit){ # define q q &lt;- 1 - p # calculate genotype frequencies (under HWE) gf &lt;- c(p^2, 2*(p*q), q^2) # calculate mean pop fitness w_bar &lt;- sum(rel_fit*gf) # calculate marginal allele frequencies w1 &lt;- (p*rel_fit[1]) + (q*rel_fit[2]) w2 &lt;- (p*rel_fit[2]) + (q*rel_fit[3]) # calculate freq of p in the next generation p_t &lt;- (p*w1)/w_bar # return the results return(p_t) } With this function, we can play around with the initial frequency of the \\(A_1\\) allele and the relative fitness of the 3 genotypes. Try a few different values for yourself to see. # keeping the initial frequency constant selection_model_simple(p = 0.5, rel_fit = c(1, 1, 0.75)) selection_model_simple(p = 0.5, rel_fit = c(1, 1, 0.5)) selection_model_simple(p = 0.5, rel_fit = c(1, 1, 0.3)) We now have a function that calculates the change in p after one generation. To get a better idea of how the model works, we want to see how p changes over multiple generations. We will initialise three values – the intial frequency p, the number of generations we want to simulate selection for, n_gen, and a vector of relative fitness. Like the simulation we made last week, this can be done with a for-loop. # first initialise the values p_init &lt;- 0.5 ngen &lt;- 100 rel_fit &lt;- c(1, 1, 0.75) # make an empty vector to store the values of p p &lt;- rep(NA, ngen) # set the first element of p p[1] &lt;- p_init for(i in 2:ngen){ p[i] &lt;- selection_model_simple(p = p[i-1], rel_fit = rel_fit) } p #&gt; [1] 0.5000000 0.5333333 0.5640423 0.5921796 0.6178702 0.6412807 0.6625963 #&gt; [8] 0.6820065 0.6996947 0.7158338 0.7305825 0.7440850 0.7564708 0.7678555 #&gt; [15] 0.7783419 0.7880212 0.7969742 0.8052725 0.8129793 0.8201508 0.8268370 #&gt; [22] 0.8330821 0.8389255 0.8444025 0.8495445 0.8543796 0.8589331 0.8632276 #&gt; [29] 0.8672836 0.8711195 0.8747520 0.8781961 0.8814655 0.8845726 0.8875289 #&gt; [36] 0.8903445 0.8930290 0.8955911 0.8980385 0.9003786 0.9026181 0.9047631 #&gt; [43] 0.9068193 0.9087920 0.9106860 0.9125058 0.9142555 0.9159390 0.9175599 #&gt; [50] 0.9191216 0.9206271 0.9220794 0.9234812 0.9248349 0.9261430 0.9274078 #&gt; [57] 0.9286312 0.9298152 0.9309616 0.9320723 0.9331487 0.9341924 0.9352049 #&gt; [64] 0.9361876 0.9371416 0.9380682 0.9389686 0.9398438 0.9406948 0.9415227 #&gt; [71] 0.9423282 0.9431125 0.9438761 0.9446200 0.9453448 0.9460513 0.9467402 #&gt; [78] 0.9474120 0.9480675 0.9487072 0.9493316 0.9499413 0.9505367 0.9511185 #&gt; [85] 0.9516870 0.9522427 0.9527859 0.9533172 0.9538369 0.9543453 0.9548429 #&gt; [92] 0.9553299 0.9558067 0.9562736 0.9567309 0.9571789 0.9576179 0.9580481 #&gt; [99] 0.9584699 0.9588833 The change in allele frequency is now stored in your object p. This can easily be plotted straight away with base R’s plot(): plot(p, type = &quot;l&quot;, xlab = &quot;Generation&quot;) 4.2.2.2 Visualising selection with different parameters. Now, let’s combine our programming and visualisation skills to demonstrate how varying the parameters really effect the outcome of our model. First things first, we will take our simulation from the previous section and make it into it’s own function selection_sim. Like before, paste the entire code into the body, and convert the relevant objects to arguments. ## make a simulator function selection_sim_simple &lt;- function(p_init, rel_fit, ngen){ p &lt;- rep(NA, ngen) p[1] &lt;- p_init for(i in 2:ngen){ p[i] &lt;- selection_model_simple(p = p[i-1], rel_fit = rel_fit) } return(p) } Note here how we have a custom function wrapped inside another custom function. This is a really useful concept, and allows for easier use and maintenance of your code. More on this in the assignment! Anyway, now we can easily simulate selection over multiple generations. For example: selection_sim_simple(p_init = 0.5, rel_fit = c(1, 1, 0.75), ngen = 1000) So, now we will perform 4 simulations for 200 generations, keeping our initial frequency of p at 0.5. However, we will alter the relative fitness of the \\(A_2A_2\\) genotype from 0.2 to 0.8. sim_02 &lt;- selection_sim_simple(p_init = 0.5, rel_fit = c(1, 1, 0.2), ngen = 200) sim_04 &lt;- selection_sim_simple(p_init = 0.5, rel_fit = c(1, 1, 0.4), ngen = 200) sim_06 &lt;- selection_sim_simple(p_init = 0.5, rel_fit = c(1, 1, 0.6), ngen = 200) sim_08 &lt;- selection_sim_simple(p_init = 0.5, rel_fit = c(1, 1, 0.8), ngen = 200) Now we have made 4 simulations. To plot these with ggplot(), we need to make them into a data frame and convert them to long form with pivot_longer(). See the tutorial from week 2 if you’re unsure about why we do this. sel_sims &lt;- data.frame( g = 1:200, # number of generations sim_02, sim_04, sim_06, sim_08 ) # use gather to rearrange for plotting # note the use of -g to select all the other columns sel_sims_l &lt;- pivot_longer(sel_sims, -g, names_to = &quot;rel_fit&quot;, values_to = &quot;p&quot;) Then we can plot the data. ggplot(sel_sims_l, aes(x = g, y = p, col = rel_fit)) + geom_line() + ylim(c(0,1)) So you can see from this plot that as the difference between the marginal fitness of \\(A_1\\) and the mean population fitness \\(\\overline{w}\\) decreases, the proportional increase in allele frequency per generation slows down. More plainly, we can see that when \\(w_{22}\\) is 0.8, the increase in p per generation is slower than when \\(w_{22}\\) is 0.2. 4.2.2.3 Getting more from our selection functions Let’s take another look at our selection_model_simple() function. # keeping the initial frequency constant selection_model_simple(p = 0.5, rel_fit = c(0.8, 1, 0.7)) You will recall that when we defined the code for this function, we actually calculated quite a lot of stuff inside it - the frequency of the \\(A_2\\) allele, the genotype frequencies, mean population fitness and marginal frequencies. But the only thing we used return to write out was the frequency of \\(p\\) in the next generation. What if we want to extend our function to give us everything we calculated? This will be very useful for the upcoming sections where we will need all these parameters. The code below creates two updated functions for simulating selection. The difference is that instead of outputting a single value, we get a lot of extra information about fitness parameters. The functions are a bit more complicated than what you’ve seen so far, however, so don’t worry if you don’t understand everything that’s happening. Advanced code: You need to run this code to complete the tutorial, but you don’t need to understand it. selection_model &lt;- function(p, rel_fit){ # define q q &lt;- 1 - p # calculate genotype frequencies (under HWE) gf &lt;- c(p^2, 2*(p*q), q^2) # calculate mean pop fitness w_bar &lt;- sum(rel_fit*gf) # calculate marginal allele frequencies w1 &lt;- (p*rel_fit[1]) + (q*rel_fit[2]) w2 &lt;- (p*rel_fit[2]) + (q*rel_fit[3]) # calculate freq of p in the next generation p_t &lt;- (p*w1)/w_bar # make vector for output output &lt;- c(p = p, q = q, w_bar = w_bar, w1 = w1, w2 = w2, p_t = p_t) # return the results return(output) } selection_sim &lt;- function(p_init, rel_fit, ngen){ # Set first generation mod_pars &lt;- t(selection_model(p = p_init, rel_fit = rel_fit)) # simulate for n generations for(i in 2:ngen){ mod_pars &lt;- rbind(mod_pars, selection_model(p = mod_pars[i-1, &quot;p_t&quot;], rel_fit = rel_fit)) } # make generations object g &lt;- 1:ngen # return the result as a data frame return(as.data.frame(cbind(g, mod_pars))) }   Our new selection_model() outputs a vector of useful values. The element p_t denotes \\(p\\) in the next generation, which was the output of the old selection_model_simple(). selection_model(p = 0.5, rel_fit = c(0.8, 1, 0.7)) #&gt; p q w_bar w1 w2 p_t #&gt; 0.5000000 0.5000000 0.8750000 0.9000000 0.8500000 0.5142857 The new selection_sim() returns a data frame, with 1 generation per row, and the fitness parameters in columns. selection_sim(p_init = 0.5, rel_fit = c(0.8, 1, 0.7), ngen = 5) #&gt; g p q w_bar w1 w2 p_t #&gt; 1 1 0.5000000 0.5000000 0.8750000 0.9000000 0.8500000 0.5142857 #&gt; 2 2 0.5142857 0.4857143 0.8763265 0.8971429 0.8542857 0.5265021 #&gt; 3 3 0.5265021 0.4734979 0.8772990 0.8946996 0.8579506 0.5369449 #&gt; 4 4 0.5369449 0.4630551 0.8780120 0.8926110 0.8610835 0.5458728 #&gt; 5 5 0.5458728 0.4541272 0.8785351 0.8908254 0.8637618 0.5535093 All these modeling results will be very useful in the next sections. 4.2.2.4 Can a rare mutant establish in a population? To understand a selection model like the one we have just developed, it can be useful to see whether a rare mutant is able to establish in a population where the alternative allele is nearly fixed. This is the basis of invasion fitness analysis. We can do this using a case where heterozygotes have a greater advantage than homozygotes. Using our newly modified selection_model function, we can test this by setting our relative fitness to show a higher relative fitness in heterozygotes and setting p to a high frequency, close to 1. # keeping the initial frequency constant selection_model(p = 0.99, rel_fit = c(0.7, 1, 0.8)) We see here that w_bar is around 0.7 - which we would expect given the frequency of \\(A_1\\) (i.e p) and the relative fitness. Invasion fitness of \\(A_2\\) is equivalent to the marginal fitness for the allele - so 0.99 here - close to 1, the relative fitness for the \\(A_1A_2\\) heterozygote. When invasion fitness is greater than resident mean population fitness, the model is not at a stable equilibrium as an allele can easily invade and increase in frequency. What would happen if the frequency of \\(A_2\\) was almost fixed? # keeping the initial frequency constant selection_model(p = 0.01, rel_fit = c(0.7, 1, 0.8)) In this case, w_bar (the resident fitness) is higher than when the \\(A_1\\) allele is fixed, however the marginal fitness of the \\(A_1\\) allele (the invasion fitness) is higher than this - so in this scenarion \\(A_1\\) could easily invade and increase in frequency too. In order for the equilibrium to be stable with heterozygote advantage, the marginal fitness of the two alleles should equal one another. We will return to this in a short while. 4.2.3 Over and underdominance Earlier on, we learned a bit about heterozygote advantage. This also referred to as overdominance - i.e. when the fitness of the heterozygote is higher than either homozygyote. However, we also touched upon the fact that this can only be stable in under certain conditions - i.e. \\(w{_1}^* = w{_2}^*\\). We also learned that when \\(p\\) or \\(q\\) were 0 (i.e. the population is fixed for either allele), these equilibria are unstable. So at what allele frequency is the population in a stable equilibria? Previously we simulated data but this time, we are going to run our selection model for a range of values of \\(p\\) and see where mean population fitness, \\(\\overline{w}\\) is maximised. Then we’ll visualise it to make it clearer to ourselves. As with the book, we will set relative fitness as 0.2, 1 and 0.4 for the \\(A_1A_1\\), \\(A_1A_2\\) and \\(A_2A_2\\) genotypes. We run the function for a range of values of \\(p\\) using a for-loop. Notice how the result for each \\(p\\) is added as a row of an empty data.frame. # set the range of p and relative fitnesses p_range &lt;- seq(0, 1, 0.01) rel_fit_overdom &lt;- c(0.2, 1, 0.4) # create empty data.frame with 6 columns for storing values # and rows corresponding to length of p_range overdom &lt;- data.frame(matrix(ncol = 6, nrow = length(p_range))) names(overdom) &lt;- c(&quot;p&quot;, &quot;q&quot;, &quot;w_bar&quot;, &quot;w1&quot;, &quot;w2&quot;, &quot;p_t&quot;) # loop over p_range and fill the data.frame with results for(i in 1:length(p_range)){ # assign result of current p to a row in the data frame overdom[i,] &lt;- selection_model(p = p_range[i], rel_fit = rel_fit_overdom) } # inspect the resulting data.frame head(overdom) #&gt; p q w_bar w1 w2 p_t #&gt; 1 0.00 1.00 0.40000 1.000 0.400 0.00000000 #&gt; 2 0.01 0.99 0.41186 0.992 0.406 0.02408585 #&gt; 3 0.02 0.98 0.42344 0.984 0.412 0.04647648 #&gt; 4 0.03 0.97 0.43474 0.976 0.418 0.06735060 #&gt; 5 0.04 0.96 0.44576 0.968 0.424 0.08686289 #&gt; 6 0.05 0.95 0.45650 0.960 0.430 0.10514786 Now this can be visualised with ggplot to see the stable equilibrium. # initialise plot a &lt;- ggplot(overdom, aes(p, w_bar)) + geom_line(colour = &quot;blue&quot;, size = 1.5) a &lt;- a + xlim(0, 1) + ylim(0, 1) a &lt;- a + xlab(&quot;Frequency of A1 - p&quot;) + ylab(&quot;Mean population fitness&quot;) a + theme_light() We can see mean population fitness is maximised at around 0.4. This is the stable point on our 2D adaptive landscape. Underdominance is the opposite of overdominance - i.e. it is heterozygote disadvantage. In short, relative fitness of the heterozygote is lower than either homozygote. Once again, we can visualise the stable equilibrium for a model of overdominance using more or less exactly the same code as before. All we really need to change is the relative fitness which we will set to 0.9, 0.3 and 1 for the \\(A_1A_1\\), \\(A_1A_2\\) and \\(A_2A_2\\) genotypes respectively. Exercise: Run selection_model() for different values of \\(p\\) in the underdominance scenario described above and plot the result. You can use the same code as above, only changing the relative fitnesses. You should also change the object names to reflect that you’re now investigating underdominance. # set the range of p and relative fitnesses p_range &lt;- seq(0, 1, 0.01) rel_fit_underdom &lt;- c(0.9, 0.3, 1) # create empty data.frame with 6 columns for storing values # and rows corresponding to length of p_range underdom &lt;- data.frame(matrix(ncol = 6, nrow = length(p_range))) names(underdom) &lt;- c(&quot;p&quot;, &quot;q&quot;, &quot;w_bar&quot;, &quot;w1&quot;, &quot;w2&quot;, &quot;p_t&quot;) # loop over p_range and fill the data.frame with results for(i in 1:length(p_range)){ # assign result of current p to a row in the data frame underdom[i,] &lt;- selection_model(p = p_range[i], rel_fit = rel_fit_underdom) } # inspect the resulting data.frame head(underdom) #&gt; p q w_bar w1 w2 p_t #&gt; 1 0.00 1.00 1.00000 0.300 1.000 0.000000000 #&gt; 2 0.01 0.99 0.98613 0.306 0.993 0.003103039 #&gt; 3 0.02 0.98 0.97252 0.312 0.986 0.006416320 #&gt; 4 0.03 0.97 0.95917 0.318 0.979 0.009946099 #&gt; 5 0.04 0.96 0.94608 0.324 0.972 0.013698630 #&gt; 6 0.05 0.95 0.93325 0.330 0.965 0.017680150 # then visualize # initialise plot a &lt;- ggplot(underdom, aes(p, w_bar)) + geom_line(colour = &quot;blue&quot;, size = 1.5) a &lt;- a + xlim(0, 1) + ylim(0, 1) a &lt;- a + xlab(&quot;Frequency of A1 - p&quot;) + ylab(&quot;Mean population fitness&quot;) a + theme_light() Here we see a scenario where underdominance is at an unstable equilibrium when \\(p\\) is slightly above 0.5. "],["study-questions-2.html", "4.3 Study questions", " 4.3 Study questions The study questions for week 4 are found here. Deliver them in Canvas before the deadline as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-3.html", "4.4 Going further", " 4.4 Going further More on writing your own functions from R for Data Science Graham Coop’s excellent notes on one-locus models of selection "],["ch05.html", "Week 5 Detecting Natural Selection", " Week 5 Detecting Natural Selection In the last section, we spent a lot of time on the theoretical basis of natural selection. In this section, we will once again learn about selection, but this time our focus will be much more empirically based. In particular, we are going to use R to demonstrate the utility of FST (i.e. genetic differentiation among populations) as a means to infer selection in the genome. This means we will be returning to some of the things you learned about in Chapter 3 in order to properly understand F-statistics. We’ll also return to the idea of empirical and statistical distributions in order to underline the basic concepts behind how we can do this. Finally, we will take some actual genome-wide FST data and visualise it in order to demonstrate just how a genome scan approach might identify genomic regions under selection. What to expect In this section we will: Learn more about functions in R Learn about the ifelse() function develop our understanding of F-statistics visualise FST across the genome and use it to detect potential divergent selection learn about empirical and statistical distributions in R Getting started The first thing we need to do is set up the R environment. We won’t be using anything other than base R and the tidyverse package today. So you’ll need to load the latter. library(tidyverse) "],["vectorisation.html", "5.1 More on functions: Vectorisation", " 5.1 More on functions: Vectorisation Last week you learned a bit about creating your own functions, and hopefully saw how useful they can be for simplifying your work flow. This week you will learn more about how they work, and some tricks for using functions efficiently. As an example, let’s use a simple function that takes radius and height as an input, and calculates the volume of a cylinder (\\(\\pi r^2h\\)). cyl_vol &lt;- function(r, h){ return(pi * r^2 * h) } First, let’s use it for a single value of r and h. # radius 3, height 8 r &lt;- 3 h &lt;- 8 cyl_vol(r, h) #&gt; [1] 226.1947 Let’s say we want to calculate the volume of 10 cylinders with radius 3, and heights ranging from 1 through 10. In R, we can supply a vector of heights as the second argument, and do these 10 calculations in a single operation! # radius 3, height 1:10 r &lt;- 3 h &lt;- 1:10 cyl_vol(r, h) #&gt; [1] 28.27433 56.54867 84.82300 113.09734 141.37167 169.64600 197.92034 #&gt; [8] 226.19467 254.46900 282.74334 This is part of what we call vectorisation, taking a single function and applying it to a vector of values. We can even supply two vectors, one for r and 1 for h if we want to vary both radius and height. r &lt;- 10:1 h &lt;- 1:10 cyl_vol(r, h) #&gt; [1] 314.15927 508.93801 603.18579 615.75216 565.48668 471.23890 351.85838 #&gt; [8] 226.19467 113.09734 31.41593 Note that this calculates volume for \\(r=10, h=1\\), then \\(r=9, h=2\\) and so on, not for all combinations of the vectors. Important concept: Many functions can be applied to a vector of values instead of a single value. This is a quick and simple way of running a lot of functions at the same time. Notice how the function call cyl_vol(r, h) is exactly the same for single values and vectors!   Tip: Notice that you could also have used a for loop to use the function on a range of values. If you have experience with other programming languages, e.g. Python, you’re probably used to do it that way! In R, however, vectorising functions is often faster than using for-loops if your vectors get really large. In some cases it’s also easier both to read and write. 5.1.1 The apply() function The apply() function is a tool to further vectorise your functions that works with matrices and data frames. apply() will apply a function to either each row or each column of your data frame/matrix. The general structure of using apply() is: apply(data, row_or_colwise, function_name) Where the second argument is either 1 for operating on each row or 2 for operating on each column19. The best way to show how it works is probably through an example. Consider if you have the following data frame of species counts, where each column contains counts for a species, and each row is a location. species &lt;- data.frame( sp_1 = c(0, 3, 2, 6, 7), sp_2 = c(4, 2, 3, 0, 1), sp_3 = c(2, 2, 0, 0, 1)) species #&gt; sp_1 sp_2 sp_3 #&gt; 1 0 4 2 #&gt; 2 3 2 2 #&gt; 3 2 3 0 #&gt; 4 6 0 0 #&gt; 5 7 1 1 What if you want to take the mean count for each location? I.e. calculate the mean of each row in the data set. You can’t simply use mean() on your data frame: mean(species) #&gt; [1] NA R doesn’t understand what to do in this case. However, this is a perfect opportunity for using apply()! Remember that the second argument should be 1 since we want to work on rows here. apply(species, 1, mean) #&gt; [1] 2.000000 2.333333 1.666667 2.000000 3.000000 Great! Notice how mean should be written without parentheses when using it inside apply(). This is something you unfortunately just have to remember. Exercise: use apply() calculate the mean count of each species, i.e. the means of the columns in your data. Show hint Use 2 as the second argument to work with columns instead of rows. apply(species, 2, mean) #&gt; sp_1 sp_2 sp_3 #&gt; 3.6 2.0 1.0 Important concept: Use apply() to use a function on either all rows (1) or all columns (2) of a data frame/matrix. This section has been a small taste on what R can do when you vectorise your functions. Vectorisation can be a bit tricky to wrap your head around in the beginning, but if you keep using it it eventually becomes a very useful tool for performing a lot of calculations simultaneously. 5.1.2 The ifelse() function Say that you have a vector with values, and you want to group them somehow. For example, you have a vector of numbers between 0 and 10,20 and want to label the values that are larger than 5. # generate 10 numbers between 0 and 10 set.seed(14) # ensure same result of random drawing each time x &lt;- sample(0:10, 10) # draw 10 random numbers between 0 and 10 x #&gt; [1] 8 10 3 2 5 6 9 1 7 0 Now, you may remember from the first week that you can check which numbers are more than 5 by using a logical statement: x &gt; 5 #&gt; [1] TRUE TRUE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE However, what if you want to label the values “high” and “low”, respectively? For this, you can use the ifelse() function. Conceptually, ifelse() works like this: ifelse(logical_statement, value_if_TRUE, value_if_FALSE) This means that to get “high” and “low” for our values, we can write: ifelse(x &gt; 5, &quot;high&quot;, &quot;low&quot;) #&gt; [1] &quot;high&quot; &quot;high&quot; &quot;low&quot; &quot;low&quot; &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;low&quot; &quot;high&quot; &quot;low&quot; 5.1.2.1 ifelse() examples ifelse() can be used for a lot of things, here are a couple of more examples: Check if a value is higher than the mean: ifelse(x &gt; mean(x), &quot;above mean&quot;, &quot;below_mean&quot;) #&gt; [1] &quot;above mean&quot; &quot;above mean&quot; &quot;below_mean&quot; &quot;below_mean&quot; &quot;below_mean&quot; #&gt; [6] &quot;above mean&quot; &quot;above mean&quot; &quot;below_mean&quot; &quot;above mean&quot; &quot;below_mean&quot; Convert a value from count to binary presence/absence: ifelse(x &gt; 0, 1, 0) #&gt; [1] 1 1 1 1 1 1 1 1 1 0 We can also use it for character vectors, looking for a specific word to make groups: animals &lt;- c(&quot;horse&quot;, &quot;donkey&quot;, &quot;zebra&quot;, &quot;horse&quot;, &quot;zebra&quot;, &quot;mule&quot;) ifelse(animals == &quot;horse&quot;, &quot;actual horse&quot;, &quot;almost horse&quot;) #&gt; [1] &quot;actual horse&quot; &quot;almost horse&quot; &quot;almost horse&quot; &quot;actual horse&quot; &quot;almost horse&quot; #&gt; [6] &quot;almost horse&quot; This usage can be convenient when plotting values, as you will see in the evolution-part of the tutorial. It can be tricky to remember which is which of these, so don’t worry if you find yourself looking at the help page with ?apply all the time, I sure do! Also, a general rule of thumb is that in R, rows always comes before columns, like when you extract values from a data frame with square brackets [].↩︎ Here, we create a vector of 10 random numbers between 0 and 10. The set.seed() function ensures that everyone will get the same random numbers (not so random after all then), just compare your result to the person beside you and to the output in the tutorial. If you change the seed, or run sample() multiple times without setting the seed every time, you will see the vector changing.↩︎ "],["understanding-fst---the-fixation-index.html", "5.2 Understanding FST - the fixation index", " 5.2 Understanding FST - the fixation index FST, also known as the fixation index, is an extremely important statistic in population genetics, molecular ecology and evolutionary biology. It is also arguably one of the most famous population genetic statistics you will encounter. FST essentially measures the level of genetic differentiation between two or more populations. It ranges from 0 (i.e. no genetic differentiation) to 1 (complete genetic differentiation) Ultimately, it is quite a simple statistic to understand but it can sometimes take time to properly grasp. So we will go over the basics properly here. One of the most confusing things about FST is that are several different ways to define it. For ease of understanding, we will use a simple formulation: \\(F_{ST} = \\displaystyle \\frac{H_T - H_S}{H_T}\\) For simplicity, imagine we are examining two populations only. With this formulate, \\(H_T\\) is the expected heterozygosity when the two populations are considered as one large meta-population. \\(H_S\\) is the average expected heterozygosity in the two populations. You might be thinking, hang on a minute… what do we mean by expected heterozygosity? To appreciate this, we need to think back to the Hardy-Weinberg expectation we learned about in Chapter 3. Remember that at a simple bi-allelic locus, \\(p\\) and \\(q\\) are the frequencies of the two alleles. We can calculate the expected frequency of heterozygotes with \\(2pq\\) - this is the expected heterozygosity. 5.2.1 A worked example of FST in humans As an illustrative example, we will calculate FST for the SNP rs4988235 associated with lactase persistence in humans. This SNP is located ~14 Kb upstream of the LCT gene on Chromosome 2 and is biallelic for C/T; a T at this position is strongly associated with the ability to digest milk in adulthood. We sample 80 people each from two populations which differ in the frequency of lactase persistence - Americans of European descent and Druze people from Israel. The counts of genotypes are shown in the table below. Note that these data are modified from Bersaglieri et al. 2002. Population TT CT CC American 48 28 4 Druze 0 3 77 Knowing these numbers, we will first calculate the allele frequences in each population. We will use \\(p\\) to denote the frequency of the T allele at this locus. # set up genotype counts a &lt;- c(48, 28, 4) # americans d &lt;- c(0, 3, 77) # druze # get the number of people sampled (same for both) n &lt;- sum(a) # calculate the frequency of the T allele - or p # for americans p_a &lt;- ((a[1]*2) + a[2])/(2*n) # for druze p_d &lt;- ((d[1]*2) + d[2])/(2*n) # calculating the frequency of C (or q) is then trivial q_a &lt;- 1 - p_a q_d &lt;- 1 - p_d Next we can calculate the allele frequencies for the metapopulation - i.e. Americans of European descent and Druze considered as a single population. This is as simple as taking the mean of the two allele frequencies. # calculate total allele frequency p_t &lt;- (p_a + p_d)/2 q_t &lt;- 1 - p_t With these allele frequencies calculated, we can very easily calculate expected heterozygosities - remember this is just \\(2pq\\). # first calculate expected heterozygosity for the two populations # americans hs_a &lt;- 2*p_a*q_a # druze hs_d &lt;- 2*p_d*q_d # then take the mean of this hs &lt;- (hs_a + hs_d)/2 # next calculate expected heterozygosity for the metapopulations ht &lt;- 2*p_t*q_t With all the relevant expected heterozygosities in place, we are now ready to calculate FST which we can do like so: # calculate fst fst &lt;- (ht - hs)/ht If your calculations were correct, then you should have an FST estimate of 0.59 - this is very high for between two human populations. One way to interpret the FST value we have here is that 59% of genetic variance we observe differs between populations. Since population can explain such a large difference in this case, we might expect selection to be responsible… 5.2.2 Writing a set of FST functions The code in the previous section was useful to demonstrate how we can calculate FST, but it would be a lot of work to run through this every single time we want estimate the statistic for a locus. This being R, we can of course easily create a function that will do all of the leg work for us! We will take the code we wrote out in the last section and use it here to write two functions that we can use when we want to calculate FST. Note that for simplicity, we will only write functions that work for two populations. First, we will write a function called calc_p which will take genotype counts from a population and calculate allele frequencies. This will probably be similar to the function you made in last week’s assignment. # a simple function to calculate allele frequencies in two populations calc_p &lt;- function(counts){ # get the number of samples n &lt;- sum(counts) # calculate frequency of 1st allele - p p &lt;- ((counts[1]*2) + counts[2])/(2*n) return(p) } Since it is very straightforward for use to calculate the frequency of the second allele once we have the frequency of the first (i.e. \\(q = 1- p\\)), our calc_p function will only calculate \\(p\\) for both populations. Let’s test it on the data from our previous example. # testing our function on the american/druze example af_american &lt;- calc_p(c(48, 28, 4)) af_druze &lt;- calc_p(c(0, 3, 77)) So now that we have a function that calculates allele frequencies in the two populations, we can write our calc_fst function to take these frequencies and calculate FST from them. # a function to calculate fst calc_fst &lt;- function(p_1, p_2){ # calculate q1 and q2 q_1 &lt;- 1 - p_1 q_2 &lt;- 1 - p_2 # calculate total allele frequency p_t &lt;- (p_1 + p_2)/2 q_t &lt;- 1 - p_t # calculate expected heterozygosity # first calculate expected heterozygosity for the two populations # pop1 hs_1 &lt;- 2*p_1*q_1 # pop2 hs_2 &lt;- 2*p_2*q_2 # then take the mean of this hs &lt;- (hs_1 + hs_2)/2 # next calculate expected heterozygosity for the metapopulations ht &lt;- 2*p_t*q_t # calculate fst fst &lt;- (ht - hs)/ht # return output return(fst) } Let’s test our function on the allele frequencies we calculated with our calc_p function. # testing our function on the american/druze example calc_fst(af_american, af_druze) This should be the same as you got before, but with a lot less work. Next, we’ll look at applying a function to a bigger data set with apply(). 5.2.3 Applying functions to matrices and data frames Extending our LCT and lactase persistence example, let’s get some data from multiple human populations. You can download the data here Exercise: Import the data into R using the read.table() function. If you’re unsure about how you do this, remember that you can go back and check in the tutorial from the first week Show hint The data is separated by tabulator (\"\\t\"), and has a header. lct_counts &lt;- read.table(&quot;lct_count.tsv&quot;, header = TRUE, sep = &quot;\\t&quot;) You should now have a data frame in your R environment with allele counts for the SNP rs4988235 for 53 populations. Again, these data are all from Bersaglieri et al. 2002. What we have is the counts of allleles but what we actually want is the allele frequency for T - that is how we can calculate FST. We can use our calc_p() function for this, so let’s try this function out on counts for a single population. We use indexing here to select the first row and only columns 2:4, since our function is only expecting the count data, not the population name. calc_p(lct_counts[1, 2:4]) Great! So this works well. Now let’s get \\(p\\) (the frequency of the T allele) for all the populations. We can do this using the apply() function that you learned about in the R-section. Note that the data needs to be columns 2 through 4 of the data, and the second argument needs to be 1 since we’re working on rows. p &lt;- apply(lct_counts[,2:4], 1, calc_p) We can now combine our vector of allele frequencies with the population names to create a data.frame of frequencies. Like so lct_freq &lt;- data.frame(pop = lct_counts$pop, p) Now we can calculate a pairwise FST with our calc_fst function. For example, let’s calculate FST for European Americans and East Asians. We will use dplyr commands for this. First we make subsets of the data with filter(). We then end up with a single row of our data. We provide the p-column of that subset to calc_fst using $21. # extract frequencies af_euram &lt;- filter(lct_freq, pop == &quot;European_American&quot;) af_eastasian &lt;- filter(lct_freq, pop == &quot;East_Asian&quot;) # calculate fst calc_fst(af_euram$p, af_eastasian$p) As with our previous example, we can see FST is actually pretty high between these populations for this SNP. What about if we compared East Asians with the Bedouin people from Israel? # extract frequencies af_bedouin &lt;- filter(lct_freq, pop == &quot;Bedouin_Negev_Israel&quot;) # calculate fst calc_fst(af_eastasian$p, af_bedouin$p) Here we see FST is substantially lower. Allele frequency differences are lower between these populations. Tip: to better understand this code, try printing the objects af_euram and af_eastasian, as well as the columns af_euram$p and af_eastasian$p. This way you can follow what the code is doing.↩︎ "],["visualising-fst-along-a-chromosome.html", "5.3 Visualising FST along a chromosome", " 5.3 Visualising FST along a chromosome Next, we will combine vectorisation and our custom functions to calculate FST for a series of SNPs in the vicinity of the LCT gene on chromosome 2. This is essentially a genome scan, an approach that can be used to detect signatures of selection in the genome. You can download the data here Exercise: Read in the file LCT_snps.tsv. lct_snps &lt;- read.table(&quot;LCT_snps.tsv&quot;, header = TRUE, sep = &quot;\\t&quot;) This data is also from Bersaglieri et al. 2002. It is the allele frequency in various human populations for one allele at a set of 101 biallelic SNP markers close to the LCT gene on chromosome 2 in the human gene. Each row is a SNP and there are three frequencies - one for North Americans of European descent, one for African Americans and one for East Asians. Since we have the allele frequencies, we can easily calculate FST for each of these SNPs. For our example here, we will do this between european_americans and east_asians. First of all, let’s use our calc_fst function on just a single SNP. calc_fst(lct_snps[1, &quot;european_americans&quot;], lct_snps[1, &quot;east_asians&quot;]) So it works great for a single row. Actually, calc_fst() will work with vectors of values as well, so to calculate the FST for all loci simultaneously, we can supply the entire columns as arguments with the $ operator. calc_fst(lct_snps$european_americans, lct_snps$east_asians) It works equally great for the entire data set! Let’s add these FSTs as a column in the lct_snps data frame. # make an fst column lct_snps$fst &lt;- calc_fst(lct_snps$european_americans, lct_snps$east_asians) Now that we have FST estimates for each of our SNPs, we can visualise the variation along the chromosome with ggplot2. a &lt;- ggplot(lct_snps, aes(coord, fst)) + geom_point() a &lt;- a + xlab(&quot;Position (Mb)&quot;) + ylab(expression(italic(F)[ST])) a &lt;- a + theme_light() a What are we seeing here? Quite clearly, there is a an increase in FST along the chromosome, with a few SNPs showing extremely high values. It might make things a bit clearer if we mark on our plot the midpoint of the LCT gene. We know the gene occurs between 136,261,885 bp and 136,311,220 bp on Chromsome 2 (from the UCSC Genome Browser). So first we will find the midpoint of the gene. # define the start and stop positions of the gene lct_start &lt;- 136261885 lct_stop &lt;- 136311220 # calculate the midpoint lct_mid &lt;- (lct_start + lct_stop)/2 All we need to do to add it to our plot is add the geom_vline() geom. a &lt;- a + geom_vline(xintercept = lct_mid, lty = 2, col = &quot;blue&quot;) a When the mid point of the gene is marked, it is clear that there is an increase in FST just upstream from the LCT gene. Perhaps we want to highlight the SNP that we calculated FST for in our first example? This is a perfect opportunity to use the ifelse() function we learned about in Section 5.1.2. We can make a new column in our data based on whether or not lct_snps$snp_id is exactly equal to “rs4988235”. Exercise: add a column named status to lct_snps using ifelse(). The column should contain “Yes” if the id is rs4988235, and “No” if it’s not. Show hint Start with lct_snps$status &lt;- to assign the result to a new column. Show another hint The logical statement you need to use is lct_snps$snp_id == \"rs4988235\" lct_snps$status &lt;- if_else(lct_snps$snp_id == &quot;rs4988235&quot;, &quot;Yes&quot;, &quot;No&quot;) Now to highlight the SNP on our plot. We can make it big and colored this by mapping it to both the col and size aesthetic. a &lt;- ggplot(lct_snps, aes(coord, fst, col = status, size = status)) + geom_point() a &lt;- a + xlab(&quot;Position (Mb)&quot;) + ylab(expression(italic(F)[ST])) a &lt;- a + geom_vline(xintercept = lct_mid, lty = 2, col = &quot;blue&quot;) a &lt;- a + theme_light() + theme(legend.position = &quot;none&quot;) a Now we see, our focal SNP is highlighted in the plot. We’ll change the colours to make it a little bit clearer. a + scale_colour_manual(values = c(&quot;black&quot;, &quot;red&quot;)) In the next section, we’ll demonstrate how we can use the distribution of FST to identify outliers as potential targets of selection. 5.3.1 Identifying outliers in our FST distribution How can we identify outliers in our FST data? First of all, we can look at the distribution of our data by making a histogram of fst. ggplot(lct_snps, aes(fst)) + geom_histogram(binwidth = 0.05) Now, it’s apparent that some values are way larger than the rest, but where do we set the threshold? One way to do it is to set some arbitrary value, and say that all values larger than this should be considered outliers. This can for instance be that we mark the highest 5% as outliers. In R, we can get this value with the quantile() function. In the following example, we make a vector of numbers from 0 to 200, and use the quantile function to find the highest 5% (or lowest 95%). x &lt;- 0:200 quantile(x, 0.95) #&gt; 95% #&gt; 190 The function returns 190, which means that 95% of the values are below 190, and 5% of the values are above 190 (which shouldn’t be all that surprising). Now we use the quantile() function on our data to set the outlier threshold. Note that this time we need to add na.rm = T in order to ignore some loci which have no FST estimates. # set threshold threshold &lt;- quantile(lct_snps$fst, 0.95, na.rm = T) # plot histogram with threshold marked a &lt;- ggplot(lct_snps, aes(fst)) + geom_histogram(binwidth = 0.05) a + geom_vline(xintercept = threshold, colour = &quot;red&quot;, lty = 2, size = 1) Now what if we want to visualise this on our chromosome-wide plot? Once again, we need to use the ifelse() function. lct_snps$outlier &lt;- ifelse(lct_snps$fst &gt; threshold, &quot;Outlier&quot;, &quot;Non-outlier&quot;) Take a look at the lct_snps data frame - you should now see an additional column which is a character vector with the status of each locus as either outlier or non-outlier. Next we can incorporate this into our plotting: a &lt;- ggplot(lct_snps, aes(coord, fst, colour = outlier)) + geom_point() a &lt;- a + xlab(&quot;Position (Mb)&quot;) + ylab(expression(italic(F)[ST])) a &lt;- a + geom_vline(xintercept = lct_mid, lty = 2, col = &quot;blue&quot;) a &lt;- a + theme_light() + theme(legend.position = &quot;bottom&quot;) a So now our potential outlier SNPs are marked on the figure. There are only 5 of them but they all occur just upstream from the LCT locus. We cannot say for certain that these SNPs have increased FST values because of selection - other processes such as genetic drift or demographic history (i.e. a bottleneck in one of the two populations) might be responsible. However, given our knowledge that LCT is involved in lactase persistence, we can at least hypothesise that this is the case. One important point to note here is that the threshold we set to identify a SNP as being potentially under selection is entirely arbitrary. In a way this line of thinking forces us to think of selection acting in some binary way on some SNPs and not others. This is obviously not the case. Still, for SNP data like this an FST scan can be a very useful tool. "],["study-questions-3.html", "5.4 Study questions", " 5.4 Study questions The study questions for week 5 are found here. Deliver them in Canvas before the deadline as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-4.html", "5.5 Going further", " 5.5 Going further Graham Coop’s notes on F statistics A detailed tutorial on calculating population differentiation with several R-based population genetic packages A nice, thorough exploration of the normal distribution using R functions and plotting "],["ch07.html", "Week 6 Inferring Evolutionary Processes from Sequence Data", " Week 6 Inferring Evolutionary Processes from Sequence Data In this session, we will be learning how to work with DNA sequence data in R. So far, much of the work we have done has used allelic based models where we have considered individual markers, rather than sequences as a whole. However, DNA sequence data allows us to take into account information at more than a single site - i.e. we also incorporate our understanding of invariant sites too. For this reason, a number of different statistics are often used when handling sequencing data. Today we will focus on inferring the number of segregating sites, nucleotide diversity and Tajima’s D. We focus on simpler statistics because the aim of today is also to familiarise you with working with large-scale, genome datasets. So we will begin with a relatively straightforward example with just a few sequences, before moving onto a larger dataset and then eventually looking at patterns of nucleotide diversity at the scale of an entire chromosome. This is an important precursor for what is to come in the following two sessions, where we will be focusing more and more on genomic data. What to expect Today we won’t introduce any new R-concepts, but jump straight into the action with some tools for handling sequence data. In this section we will: reinforce our understanding of basic population genetic statistics estimated from nucleotide data learn how to calculate these statistics on real data perform a genome scan analysis using these statistics Getting started The first thing we need to do is set up the R environment. Today we’ll be using tidyverse but also we will need three additional packages for this session - ape, pegas and PopGenome. To install these packages, use the following commands: install.packages(&quot;ape&quot;) install.packages(&quot;pegas&quot;) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;pievos101/PopGenome&quot;) Note that PopGenome is installed in a slightly different way than what you are used to. As long as you install the devtools package first, then PopGenome, you should have no issues. Once these packages are installed installed, we will clear the R environment with rm(list = ls()) and then load everything we need for this session. # clear the R environment rm(list = ls()) library(tidyverse) library(ape) library(pegas) library(PopGenome) Remember that clearing the R environment when you start a script is good practice to make sure you don’t have any conflicts with previously loaded data. All three packages ape, pegas and PopGenome are really useful for handling genetic data in R - follow the links for more information about each of them. Remember that you always can access the help page of any function with ?. "],["working-with-dna-sequence-data.html", "6.1 Working with DNA sequence data", " 6.1 Working with DNA sequence data For the first part of the tutorial, we learn about how to handle DNA sequence data in R. To keep things simple at this stage, we will use a relatively small dataset. Once we have some familiarity with sequence data and the statistics we can calculate from it, we will learn how to handle data derived from whole genome sequencing. For much of this first part of the tutorial, we will use a series of functions in ape and also some from pegas. 6.1.1 Reading sequence data into R. The first thing we will learn to do today is to read seqeunce data into R. Unlike in the previous tutorials, it doesn’t make much sense to store sequence data as a data.frame, so we need to use functions specifically designed for the purpose such as read.dna. We will be reading in a FASTA which is a standard file format for sequencing data. FASTA files can vary slightly, but the basic format is the same. The file we will use today can be downloaded here. Before we actually read this file into R, let’s take a look at it: file.show(&quot;example.fas&quot;) The file.show function is quite self-explanatory. You can also click on the FASTA in the Files pane in R Studio and have a look at it in the console. In this FASTA file, there are two lines for each sequence - the sequence name and then the sequence itself. The sequence name line starts with &gt; and then after it are the base calls. Again, it is worth checking out the wikipedia page for FASTA as it contains more information on how these files are formatted. Next we will actually read our FASTA file in. To do this, we will use read.dna() like so: mydna &lt;- read.dna(&quot;example.fas&quot;, format = &quot;fasta&quot;) This will create an object, mydna which is an ape specific structure called a DNAbin - i.e. DNA data stored in binary. You don’t need to worry too much about the specifics of this data structure although you might want to check it for yourself. Try class(mydna) and ?DNAbin to get more information. In the meantime, have a look at the mydna object and see what information is printed to the R console. 6.1.2 Exploring DNA sequence data As you will have just seen, when you call the mydna object, you only get a summary in the R console. What if you actually want to look at the sequences? One way to do this is to use the as.alignment function - like so: myalign &lt;- as.alignment(mydna) This function converts our DNAbin object into an alignment - which is essentially a list with different elements - i.e nb - the number of sequences, seq - the actual sequences themselves, nam - the names of the sequences. alignment object is convenient in that you can access information the way you are used to, e.g. with the $ operator. You can look at the sequences as a character vector using the following R code: myalign$seq However, if you want to compare sequences, using the alview() function may be a better method: alview(mydna) Note that in this case, we used alview directly on our mydna object - not on the myalign alignment object. This prints our alignment to the screen and it makes it immediately obvious where there are nucleotide polymorphisms in our data. You should note however that the N bases in the first sequence are bases which could not be called by the sequencing machine - they are not valid base calls. NOTE In this case, these sequences were read into R as an alignment already - meaning that each position in the sequences corresponds to one another. We did not actually align the sequences in R itself. 6.1.3 Calculating basic sequence statistics 6.1.3.1 Base composition Using a couple of standard pegas functions, we can get some more information about our sequences. For example, we can calculate the frequency of the four nucleotides in our dataset: base.freq(mydna) We can also calculate the GC content - i.e. the proportion of the sequence that is either a G or C nucleotide. GC.content(mydna) To break down GC content even further, this is equivalent to: sum(base.freq(mydna)[c(2, 3)]) 6.1.3.2 Segregating sites Looking at our aligned sequences, we can see that there are several positions whether there is a polymorphism. Using the seg.sites function, we can find the segregating sites: seg.sites(mydna) seg.sites returns the position in the sequence where polymorphisms occur in our 40 base pair sequence alignment. The positions are stored in a vector, and we can count the number of elements in the vector with the length() function to get the number of segregating sites. length(seg.sites(mydna)) As this function makes pretty clear, segregating sites is just the number of polymorphic positions in a sequence. However, it is also something that scales with sample size - i.e. the more sequences we add, the higher the probability of finding segregating sites in our data. This is a point we will return to later. One last thing about the segrating sites we calculated here - it is not standardised to the length of sequences (i.e., the capitalised \\(S\\) rather than \\(s\\)). To achieve that we need to do the following. # get segregating sites S &lt;- length(seg.sites(mydna)) # set sequence length L &lt;- 40 # standardise segregating sites by sequence length s &lt;- S/L 6.1.3.3 Nucleotide diversity Nucleotide diversity or \\(\\Pi\\) is the average number of pairwise differences between sequences in a population or sample. We can calculate it using the following formula: \\(\\Pi = \\displaystyle \\frac{1}{[n(n-1)]/2} \\sum_{i&lt;j}\\Pi_{ij}\\) Where \\(\\Pi_{ij}\\) is the number of nucleotide differences between sequence \\(i\\) and sequence \\(j\\) and \\({[n(n-1)]/2}\\) is the number of possible pairwise sequence comparisons from all \\(n\\) sequences. In other words, it is the sum of pairwise differences between each pair of sequences, divided by the number of pairs. From seg.sites(mydna), we know there are two polymorphic positions at sites 35 and 36 in our 40 bp alignment. Using alview(mydna) we can visualise these and also actually count the differences. There are 2 nucleotide differences between sequences No305 and No304, 1 between 304 and 306 and then finally, 1 again between 305 and 306. We can use R to calculate our nucleotide diversity by hand: # set n sequences n &lt;- 3 # set differences # 2 differences between 304 and 305, 1 difference in the other comparisons Pi_ij &lt;- c(2, 1, 1) # calculate the number of pairwise comparisons np &lt;- (n*(n-1))/2 # calculate the nucleotide diversity Pi &lt;- sum(Pi_ij)/np You can compare this to the calculations in Chapter 7 of the textbook - as they are essentially identical. What we have here is the average number of nucleotide differences between our three sequences. This is not standardised to the sequence length (i.e., \\(\\Pi\\), not \\(\\pi\\)), so we need to do that next. # calculate standarised nucleotide diversity L &lt;- 40 pi &lt;- Pi/L Of course, calculating nucleotide diversity by hand is not that useful when we have more than even a few sequences (as we will shortly). Luckily, we can also calculate nucleotide diversity using the function nuc.div from pegas. nuc.div(mydna) Unlike the number of segregating sites, this estimate is already standardised to the length of our sequence. Take a moment to compare the output previous command with our hand calculated value of pi. They are different! Why is this? Well using alview again we can see that there are actually two sites where in the first sequence, No305, the base call is N. As we learned earlier, this means we do not have a reliable call for this position in the sequence (perhaps a sequencing error or an ambigous base). The nuc.div function therefore corrects for these missing bases and shortens the total length of our sequence by two. This therefore changes the final estimate of \\(\\pi\\) it produces. Of course, manually counting differences between sequences and finding sequence lengths and adjusting for Ns is time-consuming and error-prone, so you will always be using the nuc.div() function directly when working with large data sets (doing it manually, however, may help you understand the theory behind). "],["working-with-a-larger-dataset.html", "6.2 Working with a larger dataset", " 6.2 Working with a larger dataset As we have seen in previous tutorials, R packages often come with nice, ready-to-use datasets. ape and pegas are no exception. We will turn to the woodmouse dataset next to learn how we can work with larger data than just 3, 40 bp sequences. To load the data, you need to do the following: data(woodmouse) woodmouse What is this data? It is 15 sequences the mitochondrial cytochrome b gene of the woodmouse (Apodemus sylvaticus), a very cute and widespread rodent in Europe. The data is a subset of a study by the authors of pegas. We can easily view the woodmouse sequences using alview. alview(woodmouse) However, you can see already that with this much data, looking at the alignment directly is not that practical - and we certainly wouldn’t ask you to count all the polymorphic sites here! Instead it makes much more sense to use the R functions we just covered to calculate the statistics of interest. With a larger dataset, we can also manipulate our woodmouse DNAbin object a bit more directly. Although we do not see a matrix of aligned DNA sequences when we call woodmouse, we can still treat it like one - including using R indices. In this sense, the rows of our matrix are our individuals and the columns are the number of base pairs in our sequence. So for example: woodmouse[1:10, ] This command will return only the first 10 sequences. Manipulating the columns will also change how much of the sequence we retain: woodmouse[, 1:100] In the next section, we will use this to demonstrate how sample size can influence our estimates of nucleotide diversity and segregating sites. 6.2.1 Sample size and sequence statistics First of all, let’s turn our attention to the number of segregating sites. How does using different numbers of sequences effect this? We can loop through different values of the maximum number of sequences. So we will first use 2 sequences, then 3, then 4 and so on, until we use all 15.22 # initialise vector for storing values # we start by comparing 2 sequences, then increase to 3 etc. # so we will end up with 14 comparisons ss &lt;- rep(NA, 14) # get segregating sites for an increasing number of sequences for(i in 1:(length(ss))){ # subset to only the first i+1 sequences woodmouse_sub &lt;- woodmouse[1:i+1,] # calculate n seg.sites in the subset ss[i] &lt;- length(seg.sites(woodmouse_sub)) } Have a look at the ss vector this creates - it is the number of segregating sites for each maximum number of sequences. However, the relationship is a lot easier if we plot it: # plot figure plot(2:15, ss, col = &quot;red&quot;, xlab = &quot;No of sequences&quot;, ylab = &quot;Segregating sites&quot;, las = 1) From the figure, we can see that the number of segregating sites grows as we increase the number of sequences, i.e., segregating sites are biased by sample size. It increases our probability of observing a polymorphism, and since all polymorphisms are given equal weighting in the calculation of the number of segregating sites, any polymorphism will increase the value by 1. So what do we see if we repeat the same code, but this time for nucleotide diversity or \\(\\pi\\)? nd &lt;- rep(NA, 14) # get segregating sites for an increasing number of sequences for(i in 1:(length(nd))){ # subset to only the first i+1 sequences woodmouse_sub &lt;- woodmouse[1:i+1,] # calculate nuc.div in the subset nd[i] &lt;- nuc.div(woodmouse_sub) } This loop is similar to the previous one, except this time we use the nuc.div function instead. We can also plot the relationship here: # plot figure plot(2:15, nd, col = &quot;blue&quot;, xlab = &quot;No of sequences&quot;, ylab = expression(pi), las = 1) There is no obvious relationship here - nucleotide diversity is an estimate of the average difference among sequences so it only becomes more precise with increased numbers of sequences - it is not so easily biased. Unlike segregating sites, polymorphisms are also not equally weighted; rare polymorphisms only contribute slightly to nucleotide diversity as most sequences will be similar at this position, whereas more common polymorphisms mean most sequences will differ at a position. 6.2.2 Inferring evolutionary processes using Tajima’s D The number of segregating sites and \\(\\pi\\) are essentially estimates of the population mutation rate \\(\\theta\\) - i.e. \\(4N_e\\mu\\). Under ideal conditions - i.e. neutrality, our two estimates of \\(\\theta\\) should be equivalent to one another. Differences between these estimates suggest either the action of selection or some kind of demographic change. Tajima’s D is a statistical test that allows us to actually investigate this. We can calculate it very easily in R using the tajima.test function in pegas. # calculate Tajima&#39;s D for the woodmouse data tajima.test(woodmouse) This produces a list with three elements. The first is an estimate of Tajima’s D and the other two are p-values assessing the significance of the statistic. Remember that to say whether a statistic is significant or not, the commonly used (arbitrary) threshold is p&lt;0.05. The two p-values are both similar here and also both show that there is no significant deviation from zero. 6.2.2.1 A bit more about \\(\\pi\\), \\(s\\) and Tajima’s D In the assignment, you will be asked to explain and interpret Tajima’s D. To get you started, we will briefly explain what we are testing for with a Tajima test, and how \\(\\pi\\) and \\(s\\) can differ relative to each other. You can read more about the interpretation of—and math behind—Tajima’s D in the course book. In a Tajima’s D test, we test if the population mutation rate parameter \\(\\theta\\) estimated using \\(\\pi\\) differs from \\(\\theta\\) estimated from \\(s\\). D is a measure of the difference between these two estimates. Our statistical null model is that the two estimates are the same, i.e. that only the neutral mutation rate and effective population size affects \\(\\pi\\) and \\(s\\) (see the book for details). Below is a very simple worked example of how different patterns in mutation can impact \\(\\pi\\) and \\(s\\) differently. Consider two populations of four individuals. In both populations, \\(S=1\\) and \\(s = 0.17\\): Population 1: AACAAG AACAAG AACAAG AATAAG Population 2: AACAAG AACAAG AATAAG AATAAG \\(\\pi\\), however, will differ. In population 1, the total number of pairwise differences is 3, and the number of possible comparisons are 6, thus \\(\\Pi = 0.5\\) and \\(\\pi = 0.08\\) (don’t take my word for it, check it yourself!). In population 2, we have 4 pairwise differences, so \\(\\Pi = 0.67\\) and \\(\\pi = 0.11\\). So even though \\(s\\) is the same in both populations, \\(\\pi\\) is different. This is just a quick example and does not go into the biological significance of these differences (which you need to understand for this week’s assignment). For that, you will need to consult the book and lectures for this course. Take a look at the code and see if you understand the following: why did we initialise the ss vector with 14 values, not 15? Why do we use i+1 for subsetting the woodmouse data? Are there other ways we could have done this?↩︎ "],["calculating-statistics-at-the-whole-genome-level.html", "6.3 Calculating statistics at the whole genome level", " 6.3 Calculating statistics at the whole genome level So far we have calculated descriptive statistics from two sets of sequence data. This is obviously useful to demonstrate how these statistics work and reinforcing our understanding of them. However, in modern evolutionary genetics, we are often working with much larger datasets than what we have dealt with so far. As sequencing technology becomes cheaper, faster and more accessible, we are regularly working with genome-scale data. This has implications for how we handle data and also how we interpret it and use it to test hypotheses or make some form of evolutionary inference. In this section of the tutorial, we will use a package called PopGenome to read variants called from whole genome resequencing data into R and then we will calculate nucleotide diversity within and among populations and also along a single chromosome. A word of caution: PopGenome is a very useful package, but it is not always the most user friendly, so we have done our best to simplify matters as much as possible here. 6.3.1 Reading in variant data The data we are going to be working with for this section is a set of SNP calls from whole-genome resequencing of Passer sparrows. It was originally used in a study on the evolution of human commensalism in the house sparrow by Ravinet et al. (2018). In the dataset there are SNPs from four sparrow species - the house sparrow, the Italian sparrow, the Spanish sparrow, the tree sparrow and also data from a house sparrow sub-species known as the Bactrianus sparrow. The data comes in two parts. We’ll deal with the actual SNP data first. This is stored in a file called a VCF, which stands for variant call format. VCF files can quickly become very very big, so the one we are using today is a much smaller, randomly sampled version of the true dataset from a single chromosome only. However it is still quite a large file size, so the file is compressed, and there are some preprocessing steps you will need to do before you can open in it in R. First, download the VCF Next, make a directory in your working directory (use getwd if you don’t know where that is) and call it sparrow_snps Move the downloaded VCF into this new directory and then uncompress it. If you do not have an program for this, you can either use the Unarchiver (Mac OS X) or 7zip (Windows). Make sure only the uncompressed file is present in the directory. With these steps carried out, you can then read this data in like so: ON MAC sparrows &lt;- readData(&quot;./sparrow_snps/&quot;, format = &quot;VCF&quot;, include.unknown = TRUE, FAST = TRUE) ON WINDOWS sparrows &lt;- readData(&quot;./sparrow_snps&quot;, format = &quot;VCF&quot;, include.unknown = TRUE, FAST = TRUE) We eventually want to investigate differences between populations, but the data does not currently contain information about the populations, only individuals. Download the population data and put it in your working directory. The following code reads in the population data, and updates the sparrows object. sparrow_info &lt;- read.table(&quot;./sparrow_pops.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) populations &lt;- split(sparrow_info$ind, sparrow_info$pop) sparrows &lt;- set.populations(sparrows, populations, diploid = T) So we just read a variant data from an entire chromosome into R. Before we move on, let’s take a look at what we have actually created with our sparrows object. If you call the sparrows object, you will not really see anything that informative, since it is a very complex data structure. However with the get.sum.data function, we can learn a bit more about it. get.sum.data(sparrows) This just gives us a quick summary of what we read in. A word of warning here - the n.sites value here is NOT the number of SNPs we read in - it is simply the position of the furthest SNP in our dataset on chromosome 8. We can essentially ignore this value. To get the number of variants or SNPs in our VCF, we need to add together n.biallelic.sites (i.e. SNP sites with only two alleles) and n.polyallelic.sites (i.e. SNP sites with three or four alleles). You can do this by hand or using R code like so: sparrows@n.biallelic.sites + sparrows@n.polyallelic.sites This also demonstrates one other important point about PopGenome, the sparrows object is a complicated structure called a GENOME object (use class(sparrows) to see this). Unlike other R structures we have seen so far, we need to use @ to access some parts of it. This is a little confusing, but you can think of it in acting in a similar way to the $ operator we used to access columns of a data.frame. 6.3.2 Calculating nucleotide diversity statistics So, let’s recap so far. We have over 90,000 SNPs from chromsome 8 of 129 sparrows from five different species. With the PopGenome package, we can now very quickly and easily calculate nucleotide diversity for every single one of these SNPs. We will do this like so: # calculate nucleotide diversity sparrows &lt;- diversity.stats(sparrows, pi = TRUE) A nice, simple function that just requires us to specify that we want to calculate \\(\\pi\\) using pi = TRUE. Let’s try and get at the data. Unfortunately, this is where things become a little more tricky since the GENOME object data structure is quite complicated. The following code extracts the nucleotide diversity data, and converts it into a data frame for plotting. # extract the pi data sparrow_nuc_div &lt;- t(sparrows@region.stats@nuc.diversity.within[[1]]) # get the SNP positions from the rownames - note we need to make them numeric here position &lt;- as.numeric(rownames(sparrow_nuc_div)) # rename the matrix columns after the species colnames(sparrow_nuc_div) &lt;- c(&quot;bactrianus&quot;, &quot;house&quot;, &quot;italian&quot;, &quot;spanish&quot;, &quot;tree&quot;) # combine into a data.frame and remove the row.names sparrow_nd &lt;- data.frame(position, sparrow_nuc_div, row.names = NULL) Take a look at the sparrows_nd data frame. One thing to note here: our sparrow_nd data.frame is 91,312 rows - this is the same as the number of biallelic SNPs, which tells us that PopGenome only calculates nucleotide diversity on these positions, not those with more than two alleles. 6.3.3 Visualising nucleotide diversity along the chromosome We can now visualise the nucleotide diversity for the house sparrow along the whole of chromosome 8. Since we set up a data.frame in the last section, this can readily be done with ggplot2. ggplot(sparrow_nd, aes(position, house)) + geom_point() Hmm - this is not the most visually appealing figure and it is also not particularly informative. Why is that? Well one of the issues here is that we have calculated nucleotide diversity for each biallelic SNP position so there is a lot of noise and the signal from the data is not clear. This is often the case when working with high-density genome data. One solution is to use a sliding window analyis in order to try and capture the average variation across a chromosome. We will learn how to do this in the next section. 6.3.4 Performing a sliding window analysis We will learn more about sliding windows next week, so in this section we will more or less rush through the code and focus on the results. A sliding window analysis groups the data into (overlapping) bins, or “windows”. We can then calculate some summary statistic—\\(\\pi\\) in our example—on each window instead of on each singe nucleotide. With our data, this means that instead of 90,000 plus estimates of nucleotide diversity, we will get as many as we have windows! This kind of summary significantly reduces noise in the data, so it is possible to visualise the trends along the genome. The following code creates windows that are 100,000 bp long, with a distance of 25,000 bp between the start of each window (i.e., two adjacent windows overlaps with 75,000 bp). It then calculates \\(\\Pi\\) in each window, and divides it with window length to obtain the length standardised measure \\(\\pi\\). Finally, we transform the data to be ready for plotting # generate sliding windows sparrows_sw &lt;- sliding.window.transform(sparrows, width = 100000, jump = 25000, type = 2) # calculate pi in each window sparrows_sw &lt;- diversity.stats(sparrows_sw, pi = TRUE) # extract diversity stats and divide by window length sparrow_nuc_div_sw &lt;- sparrows_sw@nuc.diversity.within sparrow_nuc_div_sw &lt;- sparrow_nuc_div_sw/100000 # get midpoint of each window (for plotting) position &lt;- seq(from = 1, to = 49575001, by = 25000) + 50000 # rename the matrix columns after the species colnames(sparrow_nuc_div_sw) &lt;- c(&quot;bactrianus&quot;, &quot;house&quot;, &quot;italian&quot;, &quot;spanish&quot;, &quot;tree&quot;) # combine into a data.frame and remove the row.names sparrow_nd_sw &lt;- data.frame(position, sparrow_nuc_div_sw, row.names = NULL) Look at the generated data frame. This time, the number of rows in the data frame corresponds to number of windows, rather than number of SNPs. Now we can make a more informative visualisation than the previous one, using ggplot2. We choose the house column (house sparrow population) for this visualisation, but feel free to test out with the other populations as well23. ggplot(sparrow_nd_sw, aes(position, house)) + geom_line(colour = &quot;blue&quot;) + theme_light() This is much more informative than our per SNP figure from before. What is more, we can see clearly there are several regions on this chromosome where there is a signficant reduction in nucleotide diversity, particularly around 30 Mb. We cannot say exactly what might be causing this without inferring other statistics or examining other data, but one possibility is that this is a region of reduced recombination where selection has led to a reduction in diversity. If it is shared with other sparrow species, it might suggest some kind of genome structure such as a centromere - where recombination rates are usually lower. The point is that sliding window information like this can be extremely informative for evolutionary analysis. Although we only got a quick introduction to genomic data in today’s tutorial, we will return to this sort of dataset again in the next session and explore more fully how we can use it to infer processes that might shape the distribution of these sorts of statistics in the genome. Next week you will learn how to visualise all at once! Yay!↩︎ "],["study-questions-4.html", "6.4 Study questions", " 6.4 Study questions The study questions for week 6-7 can be found here. Deliver them in Canvas before the deadline after week 7 as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-5.html", "6.5 Going further", " 6.5 Going further A simple tutorial on using PopGenome for inference in R Some more information on performing whole-genome analyses in PopGenome More examples with PopGenome and also other R packages for sequence data "],["ch08.html", "Week 7 Speciation Genomics", " Week 7 Speciation Genomics Genomic data has revolutionised the way we conduct speciation research over the past decade. With high-throughput sequencing it is now possible to examine variation at thousands of markers from across the genome. Genome-wide studies of genetic differentiation, particularly measured using FST have been used to identify regions of the genome that might be involved in speciation. The rationale is relatively simple, FST is a measure of genetic differentiation and when species diverge in the presence of gene flow, we might expect that genome regions underlying traits that prevent gene flow between species will show a higher level of FST than those that do not. In other words, genome scan analyses can, in principle, be used to identify barrier loci involved in the speciation process. This approach became extremely popular in many early speciation genomic studies but it overlooked a crucial point - that other processes, not related to speciation can produce the same patterns in the genome. In this session, we will leverage our ability to handle high-throughput, whole genome resequencing data to investigate patterns of nucleotide diversity, genetic differentiation and genetic divergence across a chromosome. We will examine what might explain some of the patterns we observe and learn that while genome scans can be a powerful tool for speciation research, they must be used with caution. What to expect In this section we will: Learn tools for visualizing data with many dimensions contrast and compare genome-wide measures of FST among species examine variation in recombination rate and it’s influence on differentiation Getting started The first thing we need to do is set up the R environment. Today we’ll be using tidyverse and the PopGenome package that we installed and loaded in the last session. # clear the R environment rm(list = ls()) library(tidyverse) library(PopGenome) "],["visualizing-complex-data.html", "7.1 Visualizing complex data", " 7.1 Visualizing complex data You have previously learned how to use aesthetics in ggplot to show many variables in a single plot (e.g., coloring points by group). Today you will learn a bit more about this. We will be working with the 2020 population data from the very first week, so make sure that is in your working directory. We start by reading in the data (see if you manage to do this yourself before looking at my code): d &lt;- read.table(&quot;worlddata.csv&quot;, header = TRUE, sep = &quot;,&quot;) For this tutorial, we will briefly investigate the correlation between fertility rate and life expectancy in the countries of the world. We can start by making a simple scatterplot. Exercise: make a scatterplot of fertility rate against life expectancy using the world data g &lt;- ggplot(d, aes(Total_Fertility_Rate, Life_Expectancy_at_Birth)) # note: adding alpha = 0.5 to better see all points g + geom_point(alpha = 0.5) This shows a negative correlation between the variables: in countries with lower life expectancy, more children are born per woman. We can investigate this further by dividing the data by continent, and gain some additional perspective by showing the population sizes of each country. Exercise: Update the plot by mapping colour to continent, and point size to population size. h &lt;- g + geom_point(aes(col = Continent, size = Population), alpha = 0.5) h 7.1.1 Faceting This is already quite a good plot, showing a lot of data at once. However, it can be a bit difficult to see trends within each continent (if that’s what we want to investigate), and what if we want to use colour for something else while still showing differences between continents? One way to do this is to divide the plot into several windows based on a variable, known as faceting. Faceting in ggplot can be done by adding the function facet_wrap(). The syntax is a bit weird: facet_wrap(~Variable). It uses the ~ (tilde) character24, and is an exception to the “all variables go inside aes()”-rule that we have emphasized earlier. For our plot it looks like this: h + facet_wrap(~Continent) You can control the layout by using the nrow and ncol arguments to specify numbers of rows and/or columns: h + facet_wrap(~Continent, nrow = 3) h + facet_wrap(~Continent, ncol = 1) Note that all axes are the same across the facets. This can be changed with the argument scales, where you can specify “free”, “free_y” or “free_x”. Be aware that this can be misleading in some cases (like this one, I would argue), so use it with caution! “free_x” is shown below, but try the others yourself to see what happens! h + facet_wrap(~Continent, scales = &quot;free_x&quot;) Now you have learned some tools for visualising various statistics across the sparrow genome for later. Let’s jump into the evolutionary biology part! Which can be read as “modeled by” or simply “by”, i.e., “facet by Variable”.↩︎ "],["returning-to-the-sparrow-dataset.html", "7.2 Returning to the sparrow dataset", " 7.2 Returning to the sparrow dataset In the last session, we used the PopGenome package to calculate sliding window estimates of nucleotide diversity across chromosome 8 of the house, Italian and Spanish sparrows with data from Ravinet et al. (2018). We will now return to this example and use it to demonstrate why we must interpret the genomic landscape of differentiation with caution. 7.2.1 Reading in the sparrow vcf The first step we need to take is to read our VCF of the sparrow chromosome 8 into the R environment. This is exactly the same procedure as the last session but just in case you missed those steps, here they are again. Remember that becasue the VCF is large, the file is compressed and there are some preprocessing steps you will need to do before you can open in it in R. First, download the VCF Next, make a directory in your working directory (use getwd if you don’t know where that is) and call it sparrow_snps Move the downloaded VCF into this new directory and then uncompress it. If you do not have an program for this, you can either use the Unarchiver (Mac OS X) or 7zip (Windows). Make sure only the uncompressed file is present in the directory. With these steps carried out, you can read this data in like so: MAC sparrows &lt;- readData(&quot;./sparrow_snps/&quot;, format = &quot;VCF&quot;, include.unknown = TRUE, FAST = TRUE) WINDOWS sparrows &lt;- readData(&quot;./sparrow_snps&quot;, format = &quot;VCF&quot;, include.unknown = TRUE, FAST = TRUE) Like last time, we then need to read the file with population information, and attach that to our sparrows object. You should have the file available from last week’s tutorial, otherwise it can be downloaded here. sparrow_info &lt;- read.table(&quot;./sparrow_pops.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) populations &lt;- split(sparrow_info$ind, sparrow_info$pop) sparrows &lt;- set.populations(sparrows, populations, diploid = T) 7.2.2 Examining the variant data Remember, you can look at the data we have read in using the following command: get.sum.data(sparrows) In this case, you can see that from the n.sites that the final site is at position 49,693,117. The actual chromosome is 49,693,984 long - so this confirms variants span the entire chromosome. Note that n.sites is a bit counter-intuitive here, it would only make sense as the number of sites if we had called nucleotides at every single position in the genome - but since this is a variant call format, only containing polymorphic positions then obviously this is not the case. Furthermore, the data has actually been subset in order to make it more manageable for our purposes today. Nonetheless, it is still substantial, from the n.biallelic.sites we can see there are 91,312 bilallelic SNPs and from n.polyallelic.sites, there are 1092 positions with more than two alleles. So in total we have: sparrows@n.biallelic.sites + sparrows@n.polyallelic.sites A total of 92,404 SNPs - a big dataset which requires some specific approaches to handling the data. "],["setting-up-sliding-windows.html", "7.3 Setting up sliding windows", " 7.3 Setting up sliding windows So far, this will start to seem quite familiar! We learned in the last session that per-SNP estimates of statistics such as \\(\\pi\\) can often be extremely noisy when you are calculating them on very large numbers of markers. As well as this, there are issues with the fact that SNP positions in close proximity are not always independent due to recombination - this is a theme we will return too shortly. So for this reason, it is often better to use a sliding-window approach - i.e. split the genome into windows of a particular size and then calculate the mean for a statistic within that window. We know already that chromosome 8 is 49,693,984 bp long, so we can get an idea of how many sliding windows we would generate by using some R code. We’ll set our sliding window to be 100,000 bp wide - or 100 Kb. We will also set a step or jump for our window of 25,000 bp - or 25Kb. # set chromosome size chr8 &lt;- 49693984 # set window size and window jump window_size &lt;- 100000 window_jump &lt;- 25000 We use these values to set up our sliding windows for our sparrows dataset using the PopGenome function, sliding.window.transform # make a sliding window dataset sparrows_sw &lt;- sliding.window.transform(sparrows, width = window_size, jump = window_jump, type = 2) Last week we calculated the window positions along the chromosome for you (inside a yellow box). This week, however, we will show you how you can use basic R commands to find these. We begin by making a sequence from 1 to the length of chromosome 8, with steps equal to our window size using seq(). # use seq to find the start points of each window window_start &lt;- seq(from = 1, to = chr8, by = window_jump) Then we can find the end point of each window by adding the window size to each element of the vector. # add the size of the window to each start point window_stop &lt;- window_start + window_size Now we have generated two vectors: window_start and window_stop. The windows in the chromosome run from the positions in window_start to the corresponding position in window_stop For example, the first window runs from 1 to 100 Kb (i.e., window_start[1] to window_stop[1]), the second window from 25 Kb to 125 Kb (window_start[2] to window_stop[2]) and so on. However, there is an issue here. Some of the windows stop after the end of the chromosome (compare e.g. chr8 with the final stop position window_stop[length(window_stop)]25), so we need to remove these. You can use the following code and logical operations to see that all windows start before the end of the chromosome but that because of how we generated the stop windows, this is not the case for the stop positions26. # no windows start after the end of chromosome 8 sum(window_start &gt; chr8) # but some window stop positions do occur past the final point sum(window_stop &gt; chr8) In fact, there are 4 windows that are beyond the end of the chromosome. To remove them, we can use the same logical operations as above, just this time within square brackets to drop those positions. # remove windows from the start and stop vectors window_start &lt;- window_start[window_stop &lt; chr8] window_stop &lt;- window_stop[window_stop &lt; chr8] Here we wrapped our logical operation window_stop &lt; chr8 in square brackets, which tells R to return the elements of the vector where this condition is TRUE. Also note that we have to remove the start windows that meet this condition too. Why? Well because we are using a sliding window and our window size is 100 kb, the window starting at 49,675,001 will come close to the end of the chromosome. Actually, this highlights an important point, our final window actually falls short of the end of the chromosome. You can check this like so: chr8 - window_stop[length(window_stop)] This is something to be aware of, since our final window falls short of the end of the chromosome, we may not be including all our variants. This is not necessarily wrong, but it is important to note. Anyway, although a little long-winded, this sliding window section is important as it will be useful for plotting later. For now, we will save our sliding window start/stop positions as a data.frame. We’ll also calculate the midpoint for each window. # save as a data.frame windows &lt;- data.frame(start = window_start, stop = window_stop, mid = window_start + (window_stop-window_start)/2) 7.3.1 Calculating sliding window estimates of nucleotide diversity and differentiation Now that we have set up the data, the population information and the sliding windows, it is quite straightforward for us to calculate some statistics we are interested in. In this case, we are going to calculate nucleotide diversity (i.e. \\(\\pi\\)) and FST. We will also generate a third statistic, dXY, which is the absolute nucleotide divergence between two populations. First we will calculate \\(\\pi\\). Handily, the following command also sets up what we need for dXY. # calculate diversity statistics sparrows_sw &lt;- diversity.stats(sparrows_sw, pi = TRUE) Next we will calculate FST, which again is very straight forward with a single command. # calculate diversity statistics sparrows_sw &lt;- F_ST.stats(sparrows_sw, mode = &quot;nucleotide&quot;) Note that here we use mode = \"nucleotide\" to specify we want it to be calculated sliding averages of nucleotides, rather than using haplotype data, which is the alternative. And that’s it for calculating the statistics! As you will see in the next section, extracting them from the sparrows_sw object is actually more difficult than generating them… 7.3.2 Extracting statistics for visualisation Since we ran our analysis on a sliding-window basis, we should have estimates of \\(\\pi\\), FST and dXY for each window. What we want to do now is extract all our statistics and place them in a single data.frame for easier downstream visualisation - this will let us identify how these statistics are interrelated. The extraction process involves extracting data and manipulating strings to label things correctly. This is a bit too much to go into in this tutorial, but string tools can be very useful for biological data. If you’re interested, see if you can understand how sub() and paste0() is used below, you’ve come a long way if you do! # extract nucleotide diversity and correct for window size nd &lt;- sparrows_sw@nuc.diversity.within/100000 # make population name vector pops &lt;- c(&quot;bactrianus&quot;, &quot;house&quot;, &quot;italian&quot;, &quot;spanish&quot;, &quot;tree&quot;) # set population names colnames(nd) &lt;- paste0(pops, &quot;_pi&quot;) # extract fst values fst &lt;- t(sparrows_sw@nuc.F_ST.pairwise) # extract dxy - pairwise absolute nucleotide diversity dxy &lt;- get.diversity(sparrows_sw, between = T)[[2]]/100000 ## Name the fst and dxy columns properly # get column names x &lt;- colnames(fst) # replace &quot;pop&quot; with the proper names of the populations for(i in 1:length(pops)){ x &lt;- sub(paste0(&quot;pop&quot;, i), pops[i], x) } # replace forward slash with underline x &lt;- sub(&quot;/&quot;, &quot;_&quot;, x) # set column names of fst and dxy colnames(fst) &lt;- paste0(x, &quot;_fst&quot;) colnames(dxy) &lt;- paste0(x, &quot;_dxy&quot;) ## Combine all data with the windows object we made earlier sparrow_data &lt;- as_tibble(data.frame(windows, nd, fst, dxy)) We now have the data frame sparrow_data, take a look at it to ensure that it looks correct (be aware of any errors you might get while running the code in yellow above). The data contain window positions as well as nucleotide diversity \\(\\pi\\) and the two pairwise measures FST and dxy for all pairs of populations. Now we can finally investigate differences between the populations! Note that here, length(window_stop) in the square brackets means we are evaluating the final value in the window_stop vector.↩︎ remember that window_stop &gt; chr8 gives a vector of the same length as window_stop, containing TRUE if the corresponding element of window_stop is larger thanchr8, and FALSE if it is smaller. sum() treats each TRUE as 1 and each FALSE as 0, so it can effectively be used to count the number of TRUEs in a vector, as we do here.↩︎ "],["sparrow-viz.html", "7.4 Visualising the data", " 7.4 Visualising the data For the purposes of this session, we will focus mainly on the difference between house and spanish sparrows. However, since we now have all our data in a tidy data.frame, it is very easy to calculate things like the mean values of our statistics among all the different species. For example, let’s say we want to look at mean nucleotide diversity, we can do that like so: # select nucleotide diversity data and calculate means sparrow_data %&gt;% select(contains(&quot;pi&quot;)) %&gt;% summarise_all(mean) A lot of this will be familiar from before but to clarify, we used select() and contains() to select columns from our main dataset that contain the string \"pi\"—i.e. nucleotide diversity columns. We then used summarise_all and mean to calculate the mean value for all of the columns we selected. From the output above, we can see that the house and the Italian sparrow have the highest levels of nucleotide diversity. We could also quite easily plot if we wanted to. However, to do this, we need to use pivot_longer on the data to get all species in one column, and all \\(\\pi\\) values in another27. Note how we can supply contains(\"pi\") inside pivot_longer() to specify which columns to include. # gather the data pi_g &lt;- sparrow_data %&gt;% pivot_longer(contains(&quot;pi&quot;), names_to = &quot;species&quot;, values_to = &quot;pi&quot;) # make a boxplot a &lt;- ggplot(pi_g, aes(species, pi)) + geom_boxplot() + theme_light() + xlab(NULL) a This makes it much clearer how nucleotide diversity differs among the species. 7.4.1 Visualising patterns along the chromosome Let’s have a look at how FST between house and spanish sparrows varies along chromosome 8. Note that we plot the mid-point of each window, and divide the position by \\(10^6\\) to get megabases on the x-axis. a &lt;- ggplot(sparrow_data, aes(mid/10^6, house_spanish_fst)) + geom_line(colour = &quot;red&quot;) a &lt;- a + xlab(&quot;Position (Mb)&quot;) + ylab(expression(italic(F)[ST])) a + theme_light() From this plot, it is clear there is a huge peak in FST around 30 Mb. Actually, there are several large peaks on this genome but is this one a potential region that might harbour a speciation gene? Well you might recall from the previous session that there is a drop in nucleotide diversity in this region… How can we investigate this? The easiest thing to do is to plot \\(\\pi\\), FST and dXY to examine how they co-vary along the genome. This requires a bit of data manipulation, which we will break it down into steps. First, let’s get the data we are interested in: # select data of interest hs &lt;- sparrow_data %&gt;% select(mid, house_pi, spanish_pi, house_spanish_fst, house_spanish_dxy) To keep things simple, we’ve thrown everything out we don’t need. Next, we need to use pivot_longer in order to rearrange our data.frame so that we can plot it properly. # use pivot_longer to rearrange everything hs_g &lt;- pivot_longer(hs, -mid, names_to = &quot;stat&quot;, values_to = &quot;value&quot;) Here, we use -mid to tell the function we want to leave this out of the pivoting and use names_to = \"stat\" to make it clear we are arranging our data by the statistics we have calculated, values_to = \"value\" is just a name for the values of each of our statistics. Now we can plot everything together: a &lt;- ggplot(hs_g, aes(mid/10^6, value, colour = stat)) + geom_line() a &lt;- a + xlab(&quot;Position (Mb)&quot;) a + theme_light() OK so it should be immediately obvious that this plot is really unhelpful. We see the FST data again, but since that is on such a different scale to estimates of \\(\\pi\\) and dXY, we can’t see anything! Instead, it would make a lot more sense to split our plot into facets - i.e. a plot panel for each statistic. Lucky for us, we learned to facet plots with facet_grid in the beginning of this tutorial! Remember that we can specify independent y-axes with scales = \"free_y\", and set ncol = 1 to get all plots below each other.28 # construct a plot with facets a &lt;- ggplot(hs_g, aes(mid/10^6, value, colour = stat)) + geom_line() a &lt;- a + facet_wrap(~stat, scales = &quot;free_y&quot;, ncol = 1) a &lt;- a + xlab(&quot;Position (Mb)&quot;) a + theme_light() + theme(legend.position = &quot;none&quot;) Examining the plot we created, it is pretty clear that the large peak in FST on our chromosome is matched by two regions of low nucleotide diversity in the house and Spanish sparrow, dXY is also very low in the same region. The signal here is quite clear - what could explain it? Low recombination regions in the genome are one potential explanation. The reason for this is that in a low recombination region, background selection and also selective sweeps can remove variation at polymorphic positions that are closely linked to the target of selection. Selection of this kind in either the house or the Spanish lineages AFTER they have split into different species will reduce \\(\\pi\\) in these regions and since FST is a relative measure of differentiation, it will potentially be inflated. This is an important issue as it means that we cannot reliably use FST to identify genes involved in reproductive isolation from a genome scan. By comparing FST to dXY here, we see the latter is also reduced in this region, which again suggests it is likely that some sort of genome structure might be responsible for the peak in FST we see. One way to investigate this is examine the recombination rate variation along chromosome 8—which we will do next, after a short interlude. 7.4.2 Interlude: relative vs. absolute measures of nucleotide diversity We have mentioned a couple of times that FST is a relative measure of nucleotide differences, while dxy is an absolute measure. But what does this mean? According to Wikipedia, a simple way of estimating FST (there are many ways to do this) is using the formula: \\[ F_{ST} = \\frac{\\pi_{between} - \\pi_{within}}{\\pi_{between}} \\] Thus, FST takes into account not only the nucleotide diversity between the populations, but also the diversity within the populations. So, what if the diversity within is very low, for example 0? Putting that into the formula above, we see that no matter what \\(\\pi_{between}\\) is, FST will be 1. In other words, FST is sensitive to the variation within each population, which is something you should be aware of before drawing conclusions. dxy does not take the diversity within into account, and is thus an absolute measure of differences. 7.4.3 Investigating recombination rate variation To check whether variation in recombination might explain the pattern we observed, we will read in the recombination rate estimated for 100 Kb windows with a 25 Kb step on chromosme 8. This was originally estimated from a house sparrow linkage map, published by Elgvin et al (2018) and you can download the data here. We will read the data in like normal rrate &lt;- read.table(&quot;chr8_recomb.tsv&quot;, sep = &quot;\\t&quot;, header = TRUE) Since the recombination rate is the same number of rows as our main dataset, we can just add it as a column. # assign recombination rate to full sparrow dataset sparrow_data$recomb &lt;- rrate$recomb Now we are ready to see whether the variation in nucleotide diversity and FST can be explained by recombination rate. Let’s plot how it varies along the genome. # construct a plot for recombination rate a &lt;- ggplot(sparrow_data, aes(mid/10^6, recomb)) + geom_line() a &lt;- a + xlab(&quot;Position (Mb)&quot;) + ylab(&quot;Recombination rate (cM/Mb)&quot;) a + theme_light() To explain this a little, we have plotted recombination rate in centiMorgans per Megabase - i.e. essentially the probability that a recombination event can occur. The higher this value is, the higher the probability of recombination. The first obvious point to take home from this figure is that our recombination rate varies quite significantly across the genome. Secondly, we see quite a drastic reduction in recombination rate between about 23 Mb and 30 Mb. This is exactly where our FST peak occurs. to confirm this, we will plot both statistics together. # subset data and gather hr &lt;- sparrow_data %&gt;% select(mid, house_spanish_fst, recomb) %&gt;% pivot_longer(-mid, names_to = &quot;stat&quot;, values_to = &quot;value&quot;) # make a facet plot a &lt;- ggplot(hr, aes(mid/10^6, value)) + geom_line() a &lt;- a + facet_wrap(~stat, scales = &quot;free_y&quot;, ncol = 1) a &lt;- a + xlab(&quot;Position (Mb)&quot;) + ylab(&quot;Recombination rate (cM/Mb)&quot;) a + theme_light() When we plot our data like this, it is actually more clear that perhaps both of the large peaks on chromosome 8 occur in an area of very low recombination. What could be causing such low recombination? Well one possibility is the centromere is likely to be present here. Now that we have recombination data read into R, we can also explore the relationships between recombination rate and other statistics in more detail. To demonstrate this, we will plot the joint distribution of recombination rate and FST between house and Spanish sparrows. # plot recombination rate and fst a &lt;- ggplot(sparrow_data, aes(recomb, house_spanish_fst)) + geom_point() a &lt;- a + xlab(&quot;Recombination rate (cM/Mb)&quot;) + ylab(expression(italic(F[ST]))) a + theme_light() Clearly there is a bias here - higher FST values are found in regions of low recombination. Although this doesn’t completely invalidate the use of FST in speciation genomics, it does mean we must be cautious when using it to identify genes involved in speciation. If we had not done so here, it would have been quite easy to mistake the peak on chromosome 8 as having an important role in maintaining reproductive isolation between house and Spanish sparrows. It may have been a while since the last time you saw pivot_longer(). If you have forgotten how it works or why we use it, remember that you can always go back to Week 2.↩︎ In this case, the plot might be easier to interpret if we rearranged everything so FST came at the top, \\(\\pi\\) beneath it and then finally, d_XY_. We can use the function fct_relevel() for manually reordering the factors to achieve this: new_order &lt;- c(&quot;house_spanish_fst&quot;, &quot;house_pi&quot;, &quot;spanish_pi&quot;, &quot;house_spanish_dxy&quot;) hs_g$stat &lt;- fct_relevel(hs_g$stat, new_order) We can now replot our figure with the new order: # construct a plot with facets a &lt;- ggplot(hs_g, aes(mid/10^6, value, colour = stat)) + geom_line() a &lt;- a + facet_wrap(~stat, scales = &quot;free_y&quot;, ncol = 1) a &lt;- a + xlab(&quot;Position (Mb)&quot;) a + theme_light() + theme(legend.position = &quot;none&quot;) ↩︎ "],["study-questions-5.html", "7.5 Study questions", " 7.5 Study questions The study questions for week 6-7 can be found here. Deliver them in Canvas before the deadline after week 7 as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-6.html", "7.6 Going further", " 7.6 Going further A nice review on what we mean by speciation genomics Some more information on performing whole-genome analyses in PopGenome Additional learning resources on speciation genomics using Python, R and Unix "],["ch09.html", "Week 8 Reconstructing the Past", " Week 8 Reconstructing the Past How can we use genetics and genomics to understand the evolutionary history of organisms? Although evolutionary change can be rapid, it mainly occurs on a timescale that extends far beyond the average human lifespan. For this reason, we must turn to other means in order to reconstruct a picture of the evolutionary history of species and populations. In this tutorial, we will focus on two such methods. The first of these is phylogenetic analysis as a means to visualise the evolutionary relationships among species. We will then turn to principal components analysis, a popular method for examining the variation we see in genomic data and a first step for gaining insight into the processes that might have led to the evolution of such structure within or between species. What to expect In this section we will: learn some tools for visualising phylogenetic trees learn how to create phylogenies perform a PCA on genomic data Getting started As always, we need to set up our R environment. We’ll load tidyverse as usual, but we will also need a few more packages today to help us handle different types of data. Chief among these is ape which is the basis for a lot of phylogenetic analysis in R. We will also load another phylogenetic package, phangorn (which has an extremely geeky reference in its name). The package adegenet will also be used to perform some population genomic analyses and since these are quite computationally intensive, we will also install and load parallel - a package that allows R to run computations in parallel to speed up analysis. # clear the R environment rm(list = ls()) # install new packages install.packages(&quot;ape&quot;) install.packages(&quot;phangorn&quot;) install.packages(&quot;adegenet&quot;) install.packages(&quot;parallel&quot;) # load packages library(ape) library(phangorn) library(adegenet) library(tidyverse) library(parallel) With these packages installed, we are ready to begin! "],["phylogenetics-in-r.html", "8.1 Phylogenetics in R", " 8.1 Phylogenetics in R R has a number of extremely powerful packages for performing phylogenetic analysis, from plotting trees to testing comparative models of evolution. You can see here for more information if you are interested in learning about what sort of things are possible. For today’s session, we will learn how to handle and visualise phylogenetic trees in R. We will also construct a series of trees from a sequence alignment. First, let’s familiarise ourselves with how R handles phylogenetic data. 8.1.1 Storing trees in R The backbone of most phylogenetic analysis in R comes from the functions that are part of the ape package. ape stores trees as phylo objects, which are easy to access and manipulate. The easiest way to understand this is to have a look at a simple phylogeny, so we’ll create a random tree now. # set seed to ensure the same tree is produced set.seed(32) # generate a tree tree &lt;- rtree(n = 4, tip.label = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) What have we done here? First, the set.seed function just sets a seed for our random simulation of a tree. You won’t need to worry about this for the majority of the time, here we are using it to make sure that when we randomly create a tree, we all create the same one. What you need to focus on is the second line of code that uses the rtree function. This is simply a means to generate a random tree. With the n = 4 argument, we are simply stating our tree will have four taxa and we are already specifying what they should be called with the tip.label argument. Let’s take a closer look at our tree object. It is a phylo object - you can demonstrate this to yourself with class(tree). tree #&gt; #&gt; Phylogenetic tree with 4 tips and 3 internal nodes. #&gt; #&gt; Tip labels: #&gt; c, a, d, b #&gt; #&gt; Rooted; includes branch lengths. By printing tree to the console, we see it is a tree with 4 tips and 3 internal nodes, a set of tip labels. We also see it is rooted and that the branch lengths are stored in this object too. You can actually look more deeply into the data stored within the tree object if you want to. Try the following code and see what is inside. str(tree) objects(tree) tree$edge tree$edge.length It is of course, much easier to understand a tree when we visualise it. Luckily this is easy in R. plot(tree) In the next section, we will learn more about how to plot trees. 8.1.2 Plotting trees We can do a lot with our trees in R using a few simple plot commands. We will use some of these later in the tutorial and assignment, so here’s a quick introduction of some of the options you have. First, let’s generate another random tree, this time with 5 taxa. # set seed to ensure the same tree is produced set.seed(32) # generate a tree tree &lt;- rtree(n = 5, tip.label = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;)) Now, try modifying the appearance of the tree using some of these arguments to plot(): use.edge.length (TRUE (default) or FALSE): should branch length be used to represent evolutionary distance? type: the type of tree to plot. Options include “phylogram” (default), “cladogram”, “unrooted” and “fan”. edge.width: sets the thickness of the branches edge.color: sets the color of the branches See ?plot.phylo for a comprehensive list of arguments. You can also manipulate the contents of your tree: drop.tip() removes a tip from the tree rotate() switches places of two tips in the visualisation of the tree (without altering the evolutionary relationship among taxa) extract.clade() subsets the tree to a given clade See the help pages for the functions to find out more about how they work. Now, let’s use some of the options we’ve learned here for looking at some real data. 8.1.3 A simple example with real data - avian phylogenetics So far, we have only looked at randomly generated trees. Let’s have a look at some data stored within ape—a phylogeny of birds at the order level. # get bird order data data(&quot;bird.orders&quot;) Let’s plot the phylogeny to have a look at it. We will also add some annotation to make sense of the phylogeny. # no.margin = TRUE gives prettier plots plot(bird.orders, no.margin = TRUE) segments(38, 1, 38, 5, lwd = 2) text(39, 3, &quot;Proaves&quot;, srt = 270) segments(38, 6, 38, 23, lwd = 2) text(39, 14.5, &quot;Neoaves&quot;, srt = 270) Here, the segments and text functions specify the bars and names of the two major groups in our avian phylogeny. We are just using them for display purposes here, but if you’d like to know more about them, you can look at the R help with ?segments and ?text commands. Let’s focus on the Neoaves clade for now. Perhaps we want to test whether certain families within Neoaves form a monophyletic group? We can do this with the is.monophyletic function. # Parrots and Passerines? is.monophyletic(bird.orders, c(&quot;Passeriformes&quot;, &quot;Psittaciformes&quot;)) #&gt; [1] FALSE # hummingbirds and swifts? is.monophyletic(bird.orders, c(&quot;Trochiliformes&quot;, &quot;Apodiformes&quot;)) #&gt; [1] TRUE If we want to look at just the Neoaves, we can subset our tree using extract.clade(). We need to supply a node from our tree to extract.clade, so let’s find the correct node first. The nodes in the tree can be found by running the nodelabels() function after using plot(): plot(bird.orders, no.margin = TRUE) segments(38, 1, 38, 5, lwd = 2) text(39, 3, &quot;Proaves&quot;, srt = 270) segments(38, 6, 38, 23, lwd = 2) text(39, 14.5, &quot;Neoaves&quot;, srt = 270) nodelabels() We can see that the Neoaves start at node 29, so let’s extract that one. # extract clade neoaves &lt;- extract.clade(bird.orders, 29) # plot plot(neoaves, no.margin = TRUE) The functions provided by ape make it quite easy to handle phylogenies in R, feel free to experiment further to find out what you can do! 8.1.4 Constructing trees with R So far, we have only looked at examples of trees that are already constructed in some way. However, if you are working with your own data, this is not the case - you need to actually make the tree yourself. Luckily, phangorn is ideally suited for this. We will use some data, bundled with the package, for the next steps. The following code loads the data: # get phangorn primates data fdir &lt;- system.file(&quot;extdata/trees&quot;, package = &quot;phangorn&quot;) primates &lt;- read.dna(file.path(fdir, &quot;primates.dna&quot;), format = &quot;interleaved&quot;) This is a set of 14 mitochondrial DNA sequences from 12 primate species and 2 outgroups - a mouse and a cow. The sequences are 232 basepairs long. The data is originally from this paper and is a well-known example dataset in phylogenetics. We have seen the structure this data is stored in before - it is a DNA.bin object like we worked with in Chapter 7. Print primates to your screen and have a look at it. For the next section, we will use just four species - the hominidae (i.e. Orangutan, Gorilla, Chimpanzee and Human). Let’s subset our data in order to do that. # subset data to get hominidae hominidae &lt;- primates[11:14, ] We also need to convert our dataset so that phangorn is able to use it properly. The package uses a data structure called phyDAT. Luckily conversion is very easy indeed: # convert data hominidae &lt;- as.phyDat(hominidae) We are going to create two types of trees - UPGMA and Neighbour Joining. These are distance based measures and so we must first make a distance matrix among our taxa, which requires a substitution model. The default substitution model is the Jukes &amp; Cantor model, but we can also use Felsenstein’s 1981 model. Which is the best to apply here? To find that out, we should first test the different models using modelTest: # perform model selection hominidae_mt &lt;- modelTest(hominidae, model = c(&quot;JC&quot;, &quot;F81&quot;), G = FALSE, I = FALSE) #&gt; Model df logLik AIC BIC #&gt; JC 5 -862.0267 1734.053 1751.287 #&gt; F81 8 -787.2579 1590.516 1618.09 Take a look at the hominidae_mt table. What we have done here is performed a maximum likelihood analysis and a form of model selection to determine which of the two models we tested - JC69 and F81 (specified by model = c(\"JC\", \"F81\")) best fits our data. We also set G and I to false in order to simplify the output. Don’t worry too much about what these are for now, but feel free to use ?modelTest if you wish to learn more. Anyway, how can we interpret this table? Well, we are looking for the model with the log likelihood closest to zero and also the lowest value of AIC (Akaike information criterion - see here for more information). In this case, it is clear that F81 is a better fit for the data than the JC model, so we will calculate our distance matrix with this model instead. We can now calculate evolutionary distance using dist.ml - a function that compares pairwise distances among sequences the substitution model we chose. # first generate a distance matrix hominidae_dist &lt;- dist.ml(hominidae, model = &quot;F81&quot;) Take a look at hominidae_dist. You will see it is a matrix of the distance or difference between the sequences. The distances are based on the number of nucleotide substitutions, and the actual values depend on the model we use—here, the F81. It is not straightforward to interpret how much the groups differ from the numbers directly, but in general, the larger the number the greater the genetic distance. Next we can create our trees. For an UPGMA tree, we use the upgma function: # upgma tree hom_upgma &lt;- upgma(hominidae_dist) Next we will make a neighbour joining tree. This is easily done with the NJ function. # NJ tree hom_nj &lt;- NJ(hominidae_dist) Now that we have created both of our trees, we should plot them to have a look at them. # plot them both par(mfrow = c(2, 1)) # 2 plots in same window plot(hom_upgma, no.margin = TRUE) plot(hom_nj, type = &quot;unrooted&quot;, no.margin = TRUE) par(mfrow = c(1,1)) # reset mfrow Note that when we plot the NJ tree, we add an extra argument to get an unrooted tree. The default in R is to plot rooted trees, but since the neighbour joining algorithm produces an unrooted phylogeny, the correct way to plot it is unrooted. We can verify that the tree is unrooted (compared to the UPGMA tree) using the is.rooted() function. # check whether the tree is rooted is.rooted(hom_nj) is.rooted(hom_upgma) We can also set a root on our tree, if we know what we should set the outgroup to. In this case, we can set our outgroup to Orangutan, because we know it is the most divergent from the clade that consists of humans, chimps and gorillas. We will set the root of our neighbour joining tree below using the root function and we’ll then plot it to see how it looks. # plot nj rooted hom_nj_r &lt;- root(hom_nj, &quot;Orang&quot;) plot(hom_nj_r, no.margin = TRUE) In this case, it hasn’t actually made a huge difference to our tree topology, but with a larger dataset, it might do. As a final point here, we might want to try and compare our two trees and see which we should accept as the best model for the evolutionary relationships among our taxa. One way to do this is to use the parsimony score of a phylogeny. Essentially, the lower the parsimony score is for a tree, the more parsimonious explanation of the data it might be. This is very easy to achieve with the parsimony function. # calculate parsimony parsimony(hom_upgma, hominidae) parsimony(hom_nj_r, hominidae) For the parsimony function, the first argument is the tree, the second is the data. Here we can see that both parsimony scores are equal for the two trees, suggesting that they are both equivalent models of the evolutionary relationships among the taxa we are studying here. If you test the parsimony score for the rooted and the unrooted NJ tree, you will see that they are the same. It is important to note that this is not usually the case! Choosing an outgroup will normally change the tree length, and therefore the parsimony score. "],["population-structure.html", "8.2 Population structure", " 8.2 Population structure Examining population structure can give us a great deal of insight into the history and origin of populations. Model-free methods for examining population structure and ancestry, such as principal components analysis (PCA), are extremely popular in population genomic research. This is because it is typically simple to apply and relatively easy to interpret when you have learned how. Essentially, PCA aims to identify the main axes of variation in a dataset with each axis being independent of the next (i.e. there should be no correlation between them). Here, we will do a PCA analysis, and then walk you through the interpretation of the PCA, as it can be a bit tricky to wrap your head around the first time you see it. 8.2.1 Village dogs as an insight to dog domestication To demonstrate how a PCA can help visualise and interpret population structure, we will use a dataset adapted from that originally used by Shannon et al. (2015) to examine the genetic diversity in a worldwide sample of domestic dogs. All of us are familiar with domestic dogs as breeds and pets, but it is easy to overlook the fact that the majority of dogs on earth are in fact free-roaming, human commensals. Rather than being pets or working animals, they just live alongside humans and are equally charming. In their study, Shannon et al. (2015) surveyed hundreds of dogs from across the world, focusing mainly on village dogs in developing countries. Since domestic dog breeds are often characterised by severe bottlenecks and inbreeding, they lack a lot of the diversity that would have been present when they first became a domestic species. In contrast, village dogs are unlikely to have undergone such bottlenecks and so might represent a more broad sample of the true genetic diversity present in dogs. The researchers used a SNP chip and previously published data to collate variant calls from over 5,406 dogs at 185,805 SNP markers. Of the 5,406 dogs, 549 were village dogs. It is these free-roaming dogs we will focus on today. 8.2.2 Reading the data into R In order to run our PCA analysis, we will need to use adegenet. However, the full dataset is much too large to read, so instead we will use a smaller subsetted dataset. We will read in a special format of SNP data produced by a program called PLINK. Don’t worry too much about the data format for now - our main aim is to get it into R. However, feel free to explore the PLINK website if you are interested. We will need a plink raw file and also a plink map file for our dog data. Follow the links to download the data and then use the read.PLINK function below to read them in. (if you’re on a mac, and think this goes too slowly, try changing the parallel argument to TRUE) # read in the dog data dogs &lt;- read.PLINK(file = &quot;./village_subsample.raw&quot;, map.file = &quot;./village_subsample.map&quot;, parallel = FALSE, chunkSize = 2000) Running the function will create a genlight object - a special data structure for adegenet. If you call the dogs object, you will see some summary information and also the number of individuals and markers. As you will see, we have subsampled this data to make it more feasible to run an analysis in R. 8.2.3 Performing a PCA With adegenet, we can perform PCA on our genomic data with the glPCA function. # perform pca on dogs dogs_pca &lt;- glPca(dogs, parallel = TRUE, nf = 20) Here again Mac and Linux users can benefit from parallel processing with parallel = TRUE. We also use nf = 20 in order to tell the function we want to retain 20 principal components. Let’s take a moment to look at the output of our PCA analysis. # look at pca object objects(dogs_pca) Our dogs_pca object is a list with four elements. We can ignore call - that is just the call to the function we performed above. eig returns the eigenvalues for all the principal components calculated (more on this later). loadings is a matrix of how the SNPs load onto the PC scores - i.e. how their changes in allele frequency effect the position of the data points along the axis. Finally the scores matrix is the actual principal component scores for each individual, allowing us to actually see how the invidivudals are distributed in our analysis. 8.2.4 Visualising the PCA Plotting a PCA is the best way to properly interpret it, so we will do this now. The first thing we should do is extract the principal component scores from the data. Remember that ggplot needs data to be in a data frame, so we will begin by converting dogs_pca$scores to a data frame. The ID of each dog is stored as row names, but we want it in a regular column, so we have to add an id column as well. pca_df &lt;- data.frame(dogs_pca$scores) pca_df$id &lt;- rownames(dogs_pca$scores) We can then plot the first 2 axes using ggplot()29. # plot with ggplot2 ggplot(pca_df, aes(PC1, PC2)) + geom_point() + theme_light() + coord_fixed() So what does this plot tell us? Let’s explain a bit about how to interpret PCA plots. Interpreting PCA plots A PCA plot is a bit different from other plots you may have encountered before. One key difference is that the value on the axes cannot be translated into anything concrete. The principal components (PC1, PC2, etc.) simply arranges the points (individual dogs in our case) along the axis depending on how similar they are to one another. This means that the further away two points are, the more different they are in some undefined aspect30. The simplest way to read a PCA plot is: points that are close together are similar to one another points that are far apart are different to one another consequently, if points form a cluster (e.g., the group of points in the bottom right of the plot above), they are similar to one another and different from everything else Another thing to note is that the principal components are ordered after how much variation they explain. For example, PC1 is the component explaining most variation in the data, PC2 the second most, PC3 the third most and so on. So two points being far apart on PC1 means that they are “more different” than two points far apart on PC2. OK, so the above plot looks interesting. We can see that one group of points in particular forms a cluster in the bottom right corner. But which dogs form this cluster? Are they from the same population? To investigate this we need to add some more information to our plot, namely the population each dog belongs to. We have prepared that information for you and you can download it here. Then read it in like so: # read in village dog data village_data &lt;- read.table(&quot;./village_dogs.tsv&quot;, sep = &quot;\\t&quot;, header = TRUE) Take a moment to look at this. It has three columns, id, breed and location. Our my_pca object also has an id column, so we need to join the two datasets. We can do this with a dplyr function called left_join() (which we explain more about next week): # join pca and village dog data village_pca &lt;- left_join(pca_df, village_data, by = &quot;id&quot;) Now we have added location to our data set. This means we can plot the PCA using ggplot() and at the same time colour the points by the location they were sampled in (try doing this without looking at my code first!). # plot with ggplot2 ggplot(village_pca, aes(PC1, PC2, col = location)) + geom_point() + theme_light() + coord_fixed() So from this PCA, what can we deduce? First of all, the cluster we identified earlier are dogs from East Asia. Similarly, Central Asian, African and European dogs seem to form their own clusters. In the original paper, Shannon et al. (2015) suggest that the origin of dog domestication might actually be in Central Asia. This is hard to deduce from the PCA but it is clear that there is geographical structure among village dogs. 8.2.5 Eigenvalues How much of the variance in the data set is captured by the PCA? For this, we can use the eigenvalues of the principal components, stored in dogs_pca$eig. Have a look at this vector, where PC1 is the first element, PC2 the second and so on, and notice that the numbers are decreasing. This means that PC1 explains more variation than the other axes, as we mentioned previously. These numbers are not easy to interpret by themselves, but are useful if we view them as a fraction of the total. For example, we can see how much of the total variation is explained by PC1 like this: dogs_pca$eig[1] / sum(dogs_pca$eig) #&gt; [1] 0.04596527 And see that PC1 explains around 4.6 % of the variation of the data. Due to R treating vectors as it would single numbers in many cases, we can calculate variance explained for all principal components in a single operation like this: eig &lt;- (dogs_pca$eig / sum(dogs_pca$eig)) eig From eig, we can see that if we sum the first two elements (i.e., PC1 and PC2), we find that around 7.63% of variance explained by the first two principal components (and by extension, the patterns we wee in our plot). A total of 7.63% for the first two vectors sounds small, but it is actually quite an appreciable amount of variance. Typically, we would concentrate on the principal components that together account for at least 10% of the variance. 8.2.6 The full data set It is important to remember that the analyses we have done have been on a small subsample. Working with the full data, however, could make patterns clearer. Therefore, you will be working with the complete data in the assignment. Since the full data set is very large, we cannot perform PCA on this in R. However, we have conducted this for you and you can find the full PCA data set here. You can also find the eigenvectors here. More on this in the assignment! since distances between points matter, as we explain below, we use coord_fixed() to make sure that the distance between values are the same on both axes↩︎ There are ways to try to determine which sources of variation is explained by the axes, but that is too much to go through here.↩︎ "],["study-questions-6.html", "8.3 Study questions", " 8.3 Study questions The study questions for week 8 are found here. Deliver them in Canvas before the deadline as a word or pdf document. See the appendix for some important points on how the assignments should be delivered. There, you will also find an introduction to R Markdown, a good way to combine code, output and text for a report. "],["going-further-7.html", "8.4 Going further", " 8.4 Going further A short tutorial on phylogenetics in R and also with some comparative phylogenetic functions Going further with phylogenetics in R - plus links to other tutorials ggTree - a package for more elegant tree drawing in R A series of tutorials for running analyses (including PCA) in adegenet "],["ch10.html", "Week 9 Advancing Further in R", " Week 9 Advancing Further in R Throughout these tutorials, we have introduced you to evolutionary genetic concepts using R. By now, you should have some familiarity with the versatility of R for data analysis and what is possible with it. We have gone from manipulating vectors and dataframes to processing genome-resequencing data and calculating population genomic statistics. With a course as broad as this, we understand it is difficult to feel like you fully understand every aspect of the analyses you are conducting - there is obviously a lot to learn! Ultimately, it is impossible for you to achieve this within a single course, using R properly takes experience and practice. Indeed, we’d argue that there is no real mastery of R, it is a tool with which you are always able to learn new things. We learn all the time in R and we have been using it for quite some time! Nonetheless, there are obviously basics that you can master and that you can build upon in your own work, research and analysis. You are already familiar with many of the most important of these - data structures, how to use functions, how to manipulate and visualise data. How can we go further, to giving you an introduction to some more advanced techniques in R? In this final session, will take a step back from population and evolutionary genetics to focus once more on how R itself works, except this time we will focus mainly on programming. We will return to some R programming topics you have actually already encountered but in more detail, with more of a focus on explaining them piece-by-piece. What to expect In this section we will: learn about some advanced features of RStudio learn about joining data sets learn more about vectorisation, and how to use lapply() and sapply() Getting started As always, we need to set up our R environment. We’ll load tidyverse as usual and that’s the only package we will use today. # clear the R environment rm(list = ls()) # load packages library(tidyverse) "],["advanced-features-of-rstudio.html", "9.1 Advanced features of RStudio", " 9.1 Advanced features of RStudio Back in the introductory session, we learned that RStudio is a front-end for R. More specifically, RStudio is an integrated development environment that makes it more straightforward to do a lot more than just interact with the console. Much of these features are specifically designed with programming in mind and so we will give a short introduction to them. 9.1.1 Projects, projects, projects One of the most useful features of RStudio is it’s ability to separate your work into different projects. Throughout the tutorials, you might have amassed a large number of files (i.e. datasets, scripts and so on) with little obvious organisation. You might have also had to set your working directory repeatedly. This can become very tedious when you are working on multiple things at once in R. Instead, you can very easily subdivide your work into different RStudio projects, which are essentially separate working directories. You can make brand new ones or associate them with existing work. You can see here for a more detailed explanation on how to manage projects in RStudio. 9.1.2 Everyone has a history Another useful feature of RStudio (and also the standard R distribution) is the fact that every command, function and input you type to the console is stored as a history. You can access this very easily using the function history. For example: # show recent command history history() In RStudio, this command will actually open the history pane and show you previous commands that you have run. You can even use the buttons at the top of this pane to reread commands into the console (i.e. To Console) or copy them into an R script (To Source). There is also an even easier way for you to quickly access the history or previously run commands when you are in the console. You can simply press the up key on your keyboard to rerun the previous lines. This can save you a lot of time and retyping. 9.1.3 Tab complete and other hotkeys When typing code into a script or the R console, RStudio allows you to do something called tab completion. This simply means if you hit the tab key while typing a function or object name, it will give you a list of options. For example, try typing pl into the R console and then pressing tab. You will see a list of available functions and you can then select the function you want from them. Tab completion also works within a function too. Try using tab complete to call the function plot and then pressing it again inside the brackets. You will see the arguments the function requires and if you hover over them, a small pop-up will give you a brief explanation of what they are. There are actually a large number of keyboard based shortcuts you can make use of with Rstudio - these include useful things like Ctrl + 1 or Ctrl + 2 to switch between the console and script panes. You can see a whole list of them here or by selecting Tools &gt; Keyboard Shortcuts Help from the menu bar at the top of the program. "],["more-on-data-handling-categorising-and-joining.html", "9.2 More on data handling: categorising and joining", " 9.2 More on data handling: categorising and joining Handling large amounts of data with few lines of code is one of R’s strong points. In this section we will show how you can use ifelse() and the left_join() function from dplyr to make categories and add information to your data 9.2.1 ifelse() for making categories Let’s first review what the ifelse() function does. As you learned back in chapter 5, ifelse() takes a logical statement, and returns something different depending on whether the condition is TRUE or FALSE: # make a small vector y &lt;- c(20, 30, 50) # use ifelse to evaluate it ifelse(y &gt; 25, &quot;Greater than 25&quot;, &quot;Not greater than 25&quot;) This function can be extremely useful for creating new variables in datasets. Let’s return to the familiar starwars data from dplyr in order to use the function in this way. starwars #&gt; # A tibble: 87 × 14 #&gt; name height mass hair_color skin_color eye_color birth_year sex gender #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Luke Sk… 172 77 blond fair blue 19 male mascu… #&gt; 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… #&gt; 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… #&gt; 4 Darth V… 202 136 none white yellow 41.9 male mascu… #&gt; 5 Leia Or… 150 49 brown light brown 19 fema… femin… #&gt; 6 Owen La… 178 120 brown, gr… light blue 52 male mascu… #&gt; 7 Beru Wh… 165 75 brown light blue 47 fema… femin… #&gt; 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… #&gt; 9 Biggs D… 183 84 black light brown 24 male mascu… #&gt; 10 Obi-Wan… 182 77 auburn, w… fair blue-gray 57 male mascu… #&gt; # ℹ 77 more rows #&gt; # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, #&gt; # vehicles &lt;list&gt;, starships &lt;list&gt; Now we can take a look at the starwars$species vector. There are a lot of different species, so what if we wanted to create a vector that simply states whether an individual is a droid or not? # if else to identify droids and non-droids ifelse(starwars$species == &quot;Droid&quot;, &quot;Droid&quot;, &quot;Non-Droid&quot;) This can be useful e.g. for comparing droids and non-droids in a plot or table. Say we want to label our species based on whether they are a droid, human or neither of the two. A useful thing with ifelse() is that the third argument can be another ifelse() function! So we can actually chain ifelse() commands like this: ifelse(starwars$species == &quot;Droid&quot;, &quot;Droid&quot;, ifelse(starwars$species == &quot;Human&quot;, &quot;Human&quot;, &quot;Neither human nor droid&quot;)) This is useful, but quickly becomes convoluted. Imagine how the code would look if we threw in a third and fourth category there! In cases like this, remember to use linebreaks to make the code more readable. You can have a linebreak anywhere after starting a function, and R will still understand that it’s part of the same function. A suggestion for better formatting than above: ifelse( starwars$species == &quot;Droid&quot;, &quot;Droid&quot;, ifelse( starwars$species == &quot;Human&quot;, &quot;Human&quot;, &quot;Neither human nor droid&quot; ) ) Still, if you have more than, say, four-five categories, this becomes difficult to read and time-consuming. For e.g. adding more information to a data frame, joining may be a better alternative, which we will go through next. 9.2.2 Joining For this section we will revisit the copepods.txt data that we encountered way back in week 2. Start by reading in this data. You should know enough by now to do this by yourself, so we won’t show you how. copepods #&gt; depth acartia calanus harpacticoida oithona oncaea temora #&gt; 1 0 0 3 0 2 0 0 #&gt; 2 2 1 0 0 6 1 0 #&gt; 3 4 1 0 0 7 0 1 #&gt; 4 6 27 0 1 0 0 2 #&gt; 5 8 11 0 2 6 0 3 #&gt; 6 10 17 0 3 0 0 2 #&gt; 7 12 13 0 1 0 0 1 #&gt; 8 14 7 0 13 0 0 0 #&gt; 9 16 6 0 6 0 0 1 Next, we use pivot_longer() to get all taxa in a single column, i.e., convert to long format. See if you manage to do this yourself before looking at my code below. copepods_long &lt;- pivot_longer(copepods, -depth, names_to = &quot;taxon&quot;, values_to = &quot;count&quot;) copepods_long #&gt; # A tibble: 54 × 3 #&gt; depth taxon count #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 0 acartia 0 #&gt; 2 0 calanus 3 #&gt; 3 0 harpacticoida 0 #&gt; 4 0 oithona 2 #&gt; 5 0 oncaea 0 #&gt; 6 0 temora 0 #&gt; 7 2 acartia 1 #&gt; 8 2 calanus 0 #&gt; 9 2 harpacticoida 0 #&gt; 10 2 oithona 6 #&gt; # ℹ 44 more rows Now, say that you have recorded the temperature at each depth, and want to add that information to your copepod data. How would you go about doing that? First, here is the data in a data frame: temps &lt;- data.frame( depth = c(0,2,4,6,8,10,12,14,16), temp = c(15.5, 15.4, 15.2, 14.7, 11.4, 8.3, 7.6, 7.0, 6.8) ) temps #&gt; depth temp #&gt; 1 0 15.5 #&gt; 2 2 15.4 #&gt; 3 4 15.2 #&gt; 4 6 14.7 #&gt; 5 8 11.4 #&gt; 6 10 8.3 #&gt; 7 12 7.6 #&gt; 8 14 7.0 #&gt; 9 16 6.8 One way would be using nested ifelse() functions, like we learned in the previous section. This is a lot of work and doesn’t look good, but it’s written it out below just to show you it’s possible: copepods_long$depthtemp &lt;- ifelse( copepods_long$depth == 0, 15.5, ifelse( copepods_long$depth == 2, 15.4, ifelse( copepods_long$depth == 4, 15.2, ifelse( copepods_long$depth == 6, 14.7, ifelse( copepods_long$depth == 8, 11.4, ifelse( copepods_long$depth == 10, 8.3, ifelse( copepods_long$depth == 12, 7.6, ifelse( copepods_long$depth == 14, 7.0, ifelse( copepods_long$depth == 16, 6.8, NA ) ) ) ) ) ) ) ) ) Instead, you can use the left_join() function from dplyr. You have to supply it the original data, the data you want to join with, and a vector of column names to join by (here “depth”). copepods_temp &lt;- left_join(copepods_long, temps, by = &quot;depth&quot;) copepods_temp #&gt; # A tibble: 54 × 5 #&gt; depth taxon count depthtemp temp #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 acartia 0 15.5 15.5 #&gt; 2 0 calanus 3 15.5 15.5 #&gt; 3 0 harpacticoida 0 15.5 15.5 #&gt; 4 0 oithona 2 15.5 15.5 #&gt; 5 0 oncaea 0 15.5 15.5 #&gt; 6 0 temora 0 15.5 15.5 #&gt; 7 2 acartia 1 15.4 15.4 #&gt; 8 2 calanus 0 15.4 15.4 #&gt; 9 2 harpacticoida 0 15.4 15.4 #&gt; 10 2 oithona 6 15.4 15.4 #&gt; # ℹ 44 more rows left_join() matches one or more columns in your two data sets, and add rows from the second data set into the first data set in the correct place. You see that the temp column is equal to the depthtemp we created earlier, but it’s so much easier to work with! Keep in mind that it is this simple in our case because depth has the exact same name in both data frames. Remember this when recording data in the future! Important concept: If you have your data spread out over multiple files, remember to name columns appropriately. All columns that contain the same kind of data should have the exact same name across all data sets. Similarly, the data should be entered in the same way in both data sets (e.g., don’t record depth as “2” in one data set and “2m” in the other). If you do this, you can easily join data sets with the left_join() function. "],["lists.html", "9.3 Lists", " 9.3 Lists In this course, we have stored large amounts of data in two types of objects: vectors and data frames. These have some inherent limitations: A vector can only contain one type of value. This means, for example, that one vector can’t contain both numeric values AND character values. In a data frame, all columns need to be of the same length. In addition, a column can only contain a single type of value. A list in R, on the other hand, has none of these limitations. It can contain values of multiple types and different lengths. This makes it ideal for storing more complex objects. In fact, much of the complex data you have encountered in these tutorials, like the phylogenetic trees last week, are based on lists. You can make a list with the list() function: my_list &lt;- list(names = c(&quot;Ask&quot;, &quot;Embla&quot;), numbers = c(11, 15, 6, 8), condition = TRUE) my_list #&gt; $names #&gt; [1] &quot;Ask&quot; &quot;Embla&quot; #&gt; #&gt; $numbers #&gt; [1] 11 15 6 8 #&gt; #&gt; $condition #&gt; [1] TRUE You can access list elements with $, or with double square brackets [[]]: # access list element 1 my_list$names #&gt; [1] &quot;Ask&quot; &quot;Embla&quot; my_list[[1]] #&gt; [1] &quot;Ask&quot; &quot;Embla&quot; You can access elements within the vector within the list by adding a second set of (single) square brackets. # access element 1 of the vector in list element 1 my_list[[1]][1] #&gt; [1] &quot;Ask&quot; Since a list can contain any type of data (even data frames and other lists), there are times when they are very useful. We won’t go into much more detail about lists here, but it is important that you know that this structure exists and a bit about how to work with them. "],["vectorisation2.html", "9.4 Vectorisation", " 9.4 Vectorisation During the tutorials in this course, you have encountered the term “vectorisation” a few times(e.g. chapter 5). In short, this is the concept of using functions and doing operations on a vector of values, in contrast to looping over each value and calculating each time. In many cases, it can make calculations faster and more readable. Just to remind you, here is the example we used in chapter 5: # define function to calculate volume of a cylinder cyl_vol &lt;- function(r, h){ return(pi * r^2 * h) } # use the function on single values r &lt;- 3 h &lt;- 8 cyl_vol(r, h) #&gt; [1] 226.1947 # use the function on vectors of values r &lt;- 10:1 h &lt;- 1:10 cyl_vol(r, h) #&gt; [1] 314.15927 508.93801 603.18579 615.75216 565.48668 471.23890 351.85838 #&gt; [8] 226.19467 113.09734 31.41593 An equivalent for-loop to the last example would be: # create empty vector vols &lt;- rep(NA, length(r)) # do calculations in loop for (i in 1:length(r)){ vols[i] &lt;- cyl_vol(r[i], h[i]) } vols #&gt; [1] 314.15927 508.93801 603.18579 615.75216 565.48668 471.23890 351.85838 #&gt; [8] 226.19467 113.09734 31.41593 Notice how much simpler the vectorised solution looks. For simple operations like these, vectorising instead of looping is a no-brainer. Now, we will look at some more ways to vectorise, with the sapply() and lapply() functions. 9.4.1 Using lapply() to vectorise In chapter 5, you learned about the apply() function, for applying a function to a data frame or matrix. The lapply() function work similarly, but on vectors and lists instead. Say you have a simple function that takes a vector as input, and rescales each element to the proportion of the total by dividing on the sum of the vector. In other words, calculate and return \\(\\frac{x}{sum(x)}\\). Exercise: Make a function that does the above, and test it with a couple of different vectors. # define function calc_proportion &lt;- function(x){ return(x/sum(x)) } # test it calc_proportion(c(1, 3, 6)) #&gt; [1] 0.1 0.3 0.6 calc_proportion(c(17, 36, 24, 55)) #&gt; [1] 0.1287879 0.2727273 0.1818182 0.4166667 Now, what if we want to do this with several vectors? We may for example have a list of vectors, and want to apply this function to all of them: numbers_list &lt;- list( c(1, 3, 6), c(17, 36, 24, 55), c(100, 500, 400, 38, 75) ) numbers_list #&gt; [[1]] #&gt; [1] 1 3 6 #&gt; #&gt; [[2]] #&gt; [1] 17 36 24 55 #&gt; #&gt; [[3]] #&gt; [1] 100 500 400 38 75 If we simply try to use the function on the list directly, we get an error. R has no way of using sum() on a list! calc_proportion(numbers_list) #&gt; Error in sum(x): invalid &#39;type&#39; (list) of argument This is where lapply() comes in. lapply() (or “list apply”) takes a list or vector as input, and applies a function to each element of the list/vector. The syntax is lapply(list, function). Like with apply(), the function should not have parentheses after its name. Let’s use this to apply calc_proportion() to numbers_list: lapply(numbers_list, calc_proportion) #&gt; [[1]] #&gt; [1] 0.1 0.3 0.6 #&gt; #&gt; [[2]] #&gt; [1] 0.1287879 0.2727273 0.1818182 0.4166667 #&gt; #&gt; [[3]] #&gt; [1] 0.08984726 0.44923630 0.35938904 0.03414196 0.06738544 Now we get a list of our rescaled vectors! Note that lapply() always returns a list (hence “list apply”). Like earlier examples we’ve seen, it doesn’t matter if numbers_list has a single value or a million, the code would still look the same31. Important concept: lapply() allows for fast and simple vectorisation when your function cannot be applied to your data directly. It takes a list/vector and a function as arguments, and returns a list of values. 9.4.2 sapply() sapply() (or “simplified apply”) is very similar to lapply() in that it takes a list or vector and a function as arguments, and applies the function to the list/vector. The key difference is that where lapply() always returns a list, sapply() returns the simplest possible object. For example, say we want to use sum() on each vector in numbers_list. With lapply() we get: lapply(numbers_list, sum) #&gt; [[1]] #&gt; [1] 10 #&gt; #&gt; [[2]] #&gt; [1] 132 #&gt; #&gt; [[3]] #&gt; [1] 1113 However, each list element only contains one value. Wouldn’t it be easier to have it stored in a vector? This is what we get with sapply(): sapply(numbers_list, sum) #&gt; [1] 10 132 1113 Important concept: sapply() can be convenient to simplify the output of a vectorised operation. Use sapply() if you want the simplest data structure possible, and lapply() when you want to be sure that your operation returns a list. 9.4.3 Anonymous functions A concept that is often used together with lapply() and sapply() is anonymous functions. Like the term implies, this is a function without a name. For example, instead of creating the calc_proportion() function above, we could have defined the function inside our lapply() function like this32: lapply(numbers_list, function(x) x/sum(x)) #&gt; [[1]] #&gt; [1] 0.1 0.3 0.6 #&gt; #&gt; [[2]] #&gt; [1] 0.1287879 0.2727273 0.1818182 0.4166667 #&gt; #&gt; [[3]] #&gt; [1] 0.08984726 0.44923630 0.35938904 0.03414196 0.06738544 It works in the exact same way, but you don’t have to define a function beforehand, and is easy to understand for the reader right away. Only use this for very simple functions that you are only going to use once. Otherwise, defining a named function is better. It is good to know about anonymous functions anyway, as they are frequently used. Of course, this can also be achieved with a for-loop, which looks like this: # create empty list, confusingly with the vector() function ... proportion_list &lt;- vector(mode = &quot;list&quot;, length = length(numbers_list)) # loop and calculate for (i in 1:length(numbers_list)){ proportion_list[[i]] &lt;- calc_proportion(numbers_list[[i]]) } proportion_list #&gt; [[1]] #&gt; [1] 0.1 0.3 0.6 #&gt; #&gt; [[2]] #&gt; [1] 0.1287879 0.2727273 0.1818182 0.4166667 #&gt; #&gt; [[3]] #&gt; [1] 0.08984726 0.44923630 0.35938904 0.03414196 0.06738544  ↩︎ Note that the function here is defined without the use of curly brackets and return(). This is not unique to anonymous functions (but most often used there), but an equivalent way to define a function in R. This means that all these four functions are exactly equal: f1 &lt;- function(x) x/sum(x) f2 &lt;- function(x) return(x/sum(x)) f3 &lt;- function(x){ x/sum(x) } f4 &lt;- function(x){ return(x/sum(x)) } When the function is more than a single line, you should always use the curly brackets. I personally prefer to use an explicit return() in more complicated functions, but you will encounter both in code you read.↩︎ "],["concluding-remarks.html", "9.5 Concluding remarks", " 9.5 Concluding remarks We hope the more indepth focus on programming and more advanced use of R in this tutorial has given you more insight into how these aspects of R work. Ultimately, the best way to become familiar and experienced with these methods is to repeatedly use them, often on your own data, to solve your own issues. Programming is a challenge for many people in any language - and it can be especially challenging for biologists. The key thing to keep in mind is that you are now familiar with the basics. You might still feel daunted at the prospect of sitting down and writing a function or set of code completely from scratch. However as you have seen from these tutorials and also those linked at the end of each section, there is a wealth of resources available online that can demystify many of these concepts further. The most important thing for your own future work is that you can grapple with these tutorials and then take things a step further each time. Even for experienced users, it takes time and thought to code in R - we hope that these tutorials have set you on the trajectory to becoming more advanced users! Good luck! "],["study-questions-7.html", "9.6 Study questions", " 9.6 Study questions The study questions for week 9 are found here. These are meant only to test yourself. Hand-in is not required and will not count on the required number of approved hands-in. We will, however, provide a hand-in folder for those who would like feedback from one of the group teachers. "],["going-further-8.html", "9.7 Going further", " 9.7 Going further There are many, many resources out there for learning more about programming in R. We have collated a few good ones here: A series of tutorials exploring aspects of R Studio useful for coding Some good examples of control flow in R A short description of control flow, functions and vectorisation in R A more advanced look at why vectorisation is worth pursuing A software carpentry course on creating functions in R "],["w01.html", "Week 1-2 assignment 1", " Week 1-2 assignment 1 Solve all the problems below using R. Some questions have optional parts that are a little more difficult. We encourage you to do these, but feel free to skip them if the mandatory tasks offers sufficient challenge. You can solve the problems in any way you want (as long as you’re using R!) unless otherwise stated. 1. Basic R 1.1 Assign a variable with the value 6 to an object named after your favorite animal. Assign a variable with the value 1 to an object named after your least favorite animal. Calculate the mean of your two objects, and assign that number to an object named after an animal you think is mediocre at best. Combine all your scores in a vector and assign it to an object. Add names to each element of the vector so that the name corresponds to the score. Optional: Combine your scores into a data frame with one column for the name and one for the score. Add a column with the average weight of your animals, and another with their top speeds. grevling &lt;- 6 rur &lt;- 1 torsk &lt;- mean(c(grevling, rur)) dyr &lt;- c(grevling, rur, torsk) names(dyr) &lt;- c(&quot;grevling&quot;, &quot;rur&quot;, &quot;torsk&quot;) #dyr 1.2 The formula for the volume of a sphere is \\(\\frac{4}{3}\\pi r^3\\). Calculate the volume of a sphere with a radius of 4.5 cm Create a vector of radiuses from 1 to 10, in steps of 0.5. Calculate the volume of spheres with all these radiuses. (hint: in R the easiest way to do this is in a single operation without the use of any loops, see if you can figure it out!) Optional: make a function that takes one or more radiuses as input, and outputs the volume of a sphere with those radiuses. 1.3 You should use logical operators (==, &gt; etc.) to solve the following tasks. Check if the number \\(e\\) is larger than the number \\(\\pi\\). Create a vector of random numbers with the following code: set.seed(645) rnumbers &lt;- rpois(100, lambda = 2) How many of the numbers are equal to or larger than 2? Hint: you can use the sum() function on a vector of TRUE/FALSE, it will then treat each TRUE as 1 and each FALSE as 0. Example: sum(c(TRUE, FALSE, TRUE)) returns 2 How many of the numbers are exactly 1? Optional: Check if R considers \\((\\sqrt2 )^2\\) to be exactly equal to 2. What is the result? Why is it this way? 2. Data import and plotting For this section you will use the dataset vitruvian.txt. The data set consists of body measurements from biology students from earlier years in the course BIO2150. Make sure you download it, and take note of where it’s stored on your computer. For this part of the exercise, in addition to the course’s computing exercises, these short tutorials may be helpful. Go through them before continuing if you’d like, or use them as reference material when you’re unsure about something. 2.1 Importing and inspecting the data Open the file “vitruvian.txt” in a plain text editor (like notepad for PC or TextEdit for mac). What kind of file is this? How are the data entries separated? Import the file into R. Use tools in R to figure out the following: are there any missing values (NAs) in this data set? how many observations (rows) and variables (columns) are there? which years did the measuring take place? what is the sex ratio for biology students in this period? Remove any rows that contain NA in your data set, leaving you with only complete cases. How many rows did you lose? 2.2 Summarizing the data What is the mean body length of all bio students from all years? What is the standard deviation? What is the median? A popular claim/myth is that the ratio of body length to the length from foot to navel is approximately the golden ratio (\\(\\approx\\) 1.618) Check if this is true for the bio students by getting the mean and standard deviation of the ratio between body.length and foot.navel. Calculate the mean height of each of the sexes from all years. Subset your data to everyone taller than the average for their respective sex. How many males and females are taller than average? Optional: use R’s lm() function to model body.length from foot.navel. What is the slope of the regression? What is the intercept, and how do you interpret that? 2.3 Before plotting the vitruvian data, we take a detour with some basic plot exercises. Make a vector of the numbers 1 through 50, and one with the numbers 51 through 100, call them x and y respectively. Plot them against each other as points. Plot x against x2 as a line with any color other than black. Plot a line of x against \\(\\frac{y^2}{4}\\) on top of your previous plot in a different color. Add a title to your plot, stating how pretty it is. 2.4 Now we return to our vitruvian data set. We will more or less be plotting the same things we explored in task 2.2. Make a histogram of body.length. What can you say about the resulting distribution? Plot foot.navel against body.length (so that body length is on the y-axis), and color the points by sex. Add a line with a slope of 1.618. How well does the data fit the golden ratio? Add a title, legend and informative axis labels to your plot. Make a boxplot of students’ heights for the two sexes. Are there any outliers? Make a subset of your data that only contains observations from 2012. Make a boxplot of neck size for this year, grouped by sex. Optional Use your model from 2.2 e. to add a line of best fit to your plot of foot.navel against body.length. How does it compare to the golden ratio line? Add a confidence interval if you figure out how. 3. Interpreting code and errors 3.1 Look at the code below: a = 5 a == 6 In your own words, what is the difference between = and == in the code above? What will the code output? Why? What is the value of a after executing this code? 3.2 You tried to import the file “example.csv”, and got the following error: ## Warning in file(file, &quot;rt&quot;): cannot open file &#39;example.csv&#39;: No such file or ## directory ## Error in file(file, &quot;rt&quot;): cannot open the connection What is the most probable cause of the error message? How can you fix it? 3.4 You tried to extract the Sepal.Length column from the iris data set as follows, but got an error: iris[,Sepal.Length] ## Error in eval(expr, envir, enclos): object &#39;Sepal.Length&#39; not found What caused the error? How can you fix it? "],["w03.html", "Week 3 Assignment 2", " Week 3 Assignment 2 1. for-loops Create one vector containing names of 4 animals you feel strongly about. Create another vector that contains what you feel about the animal. Use a for-loop to print the animal and the feeling together (like in section 1.3 of the tutorial). Example output: &quot;Pangolins are amazing&quot; # and similarly for 3 more animals 2. Hardy Weinberg Equilibrium You genotype three populations of grasshoppers along a north south transect across the European Alps. Near Munich, Germany, north of the Alps you sample 120 individuals; near Innsbruck, Austria, within the Alps you sample 122 individuals; near Verona, Italy, south of the Alps you sample 118 individuals. you find the following number of each genotype: Munich: 6 (a1a1), 33 (a1a2), 81 (a2a2) Innsbruck: 20 (a1a1), 59 (a1a2), 43 (a2a2) Verona: 65 (a1a1), 39 (a1a2), 14 (a2a2) Using the r code we learnt during the tutorial, calculate the allele frequencies in each population. Then test (statistically) whether there is a deviation from Hardy-Weinberg equilibrium in each of them. If any of the populations deviate, what could be the cause of the deviation? In the statistical test in the previous question we use the Hardy Weinberg model as a null model. Explain what this means. Why is it that when an allele goes to fixation in a population, there are no heterozygotes but there is also no deviation from the Hardy-Weinberg expectation? Hint: calculate the expected heterozygote frequency when \\(p = 1, q = 0\\). 3. Genetic drift You want to model what will happen to the grashoppers’ alleles over time, assuming no other forces than genetic drift are changing allele frequencies. Using the for-loop from the tutorial as a starting point, simulate drift over 2000 generations for each of the three populations in the previous exercise. Make sure that the \\(N\\) and initial \\(p\\) match those of the populations. You should make 3 simulations in total, and plot your results. How does the initial p alter the outcome of the simulations? Which population is more likely to go to fixation for either allele? Tip: Since this exercise uses random sampling, results will be different every time you run your simulations. You may want to run the simulations several times to get a feeling for what the results generally will be. "],["w04.html", "Week 4 assignment 3", " Week 4 assignment 3 1. Functions You need to define the calc_geno() function from the tutorial to complete this part of the assignment. calc_geno &lt;- function(p){ # calculate q from p q &lt;- 1 - p # calculate the expected genotype frequencies (_e denotes expected) A1A1_e &lt;- p^2 A1A2_e &lt;- 2 * (p * q) A2A2_e &lt;- q^2 # return the genotype frequencies geno_freq &lt;- c(A1A1_e, A1A2_e, A2A2_e) return(geno_freq) } Create a function with two arguments, x and y, that checks whether x is larger than y. You can decide yourself if you want the function to return TRUE/FALSE or a string of text. The goal of functions is to make life easier for yourself, by having to repeat yourself less, and making your code more readable and maintainable. In the R-part of the tutorial, we created the function calc_geno, which uses \\(p\\) to calculate expected genotype frequencies under Hardy-Weinberg equilibrium. If you want to calculate this, but only have the genotype counts (number of individuals \\(A_1 A_1\\), \\(A_1 A_2\\) and \\(A_2A_2\\)), not \\(p\\), it makes sense to create a function that does this for you. If you do that, you can simply write: p &lt;- calc_p(geno_counts) geno_freq &lt;- calc_geno(p) Or even calc_geno(calc_p(geno_counts)) if you prefer! Notice how easy it is to follow what happens in the code above. This adheres to a principle called modular programming, where larger tasks are separated into smaller, more manageable tasks. Now it’s your turn to actually turn the above into working code by creatng the calc_p function. Create a function calc_p() that takes a vector of genotype counts as an argument and returns the allele frequency of \\(A_1\\) (i.e., \\(p\\)). To test your code, run the following: geno &lt;- c(6, 33, 81) p &lt;- calc_p(geno) geno_freq &lt;- calc_geno(p) p should be 0.1875, and geno_freq should be 0.03515625 0.30468750 0.66015625 Optional: create a function called geno_from_counts() that takes a vector of genotype counts as an argument, and then calls first calc_p(), then calc_geno() to get expected genotype frequencies directly from genotype counts. For the rest of the assignment, feel free to use these functions, and create new functions if you like. Note: be aware that calc_geno() calculates expected genotype frequencies, and cannot be used where you need observed genotype frequencies. 2. Fitness What are absolute and relative fitness? What do we mean by marginal fitness? We sample a 325 individuals from a population of snails which have a brown/yellow polymorphism. This is controlled by a single locus B. B1B1 individuals are brown, so are B1B2 individuals but B2B2 is yellow. In our sample, 184 snails are B1B1, 60 are B1B2 and 81 are B2B2. We know from our experimental work that on average, brown individuals have 33 offspring, whereas yellow individuals have 24. Calculate the following: the frequency of the B1 and B2 alleles the relative fitnesses the marginal fitness the mean population fitness. 3. Selection You need to define the functions selection_model() and selection_sim() to complete this part of the assignment. selection_model &lt;- function(p, rel_fit){ # define q q &lt;- 1 - p # calculate genotype frequencies (under HWE) gf &lt;- c(p^2, 2*(p*q), q^2) # calculate mean pop fitness w_bar &lt;- sum(rel_fit*gf) # calculate marginal allele frequencies w1 &lt;- (p*rel_fit[1]) + (q*rel_fit[2]) w2 &lt;- (p*rel_fit[2]) + (q*rel_fit[3]) # calculate freq of p in the next generation p_t &lt;- (p*w1)/w_bar # make vector for output output &lt;- c(p = p, q = q, w_bar = w_bar, w1 = w1, w2 = w2, p_t = p_t) # return the results return(output) } selection_sim &lt;- function(p_init, rel_fit, ngen){ # Set first generation mod_pars &lt;- t(selection_model(p = p_init, rel_fit = rel_fit)) # simulate for n generations for(i in 2:ngen){ mod_pars &lt;- rbind(mod_pars, selection_model(p = mod_pars[i-1, &quot;p_t&quot;], rel_fit = rel_fit)) } # make generations object g &lt;- 1:ngen # return the result as a data frame return(as.data.frame(cbind(g, mod_pars))) }   Using R code we learned during the tutorial, simulate selection over 100 generations for a case where A1A1 has the highest fitness and the relative fitness of the genotypes A1A2 and A2A2 is 0.4. The initial frequency of the A1 allele is 0.01. Plot the frequency of p over the generations. Is A1 dominant, additive or recessive? What can we infer from the plot? What is the difference between directional selection and balancing selection? "],["w05.html", "Week 5 assignment 4", " Week 5 assignment 4 1. Vectorising functions To complete this part of the assignment, you need to define the calc_p() function from the tutorial: calc_p &lt;- function(counts){ # get the number of samples n &lt;- sum(counts) # calculate frequency of 1st allele - p p &lt;- ((counts[1]*2) + counts[2])/(2*n) return(p) } You have the following data frame containing genotype counts from four populations: counts &lt;- data.frame( A1A1 = c(10, 0, 84, 32), A1A2 = c(30, 48, 13, 15), A2A2 = c(75, 60, 3, 28), location = c(&quot;loc1&quot;, &quot;loc2&quot;, &quot;loc3&quot;, &quot;loc4&quot;) ) Use calc_p() together with apply() to calculate \\(p\\) for each population. Add the values to counts as a column. hint: you have to select only the numeric columns of the data frame to use apply() on it. Use ifelse() to create a column that says “above 0.5” if p in the population is larger than 0.5, and “below 0.5” if it’s not. You should print the data frame in the end and show the output in the hand-in. 2. A worked example of \\(F_{ST}\\) We sample two fish populations - one in the lake and the other in a stream. We genotype them at locus B. In the lake, the genotype counts are - B1B1 = 32, B1B2 = 12 and B2B2 = 6. In the stream, the genotype counts are B1B1 =10, B1B2 = 16, B2B2 = 43. Calculate the average expected heterozygosity for the two populations. Optional: make a function to calculate average expected heterozygosity. Calculate the expected heterozygosity for the two populations as a metapopulation. Optional: make a function to calculate expected heterozygosity in the metapopulation. Calculate \\(F_{st}\\) between the lake and stream fish. How do you interpret this value? 3. More on \\(F_{ST}\\) To complete this part of the assignment, you need to define the calc_fst() function, in addition to the calc_p() function defined in question 1. calc_fst &lt;- function(p_1, p_2){ # calculate q1 and q2 q_1 &lt;- 1 - p_1 q_2 &lt;- 1 - p_2 # calculate total allele frequency p_t &lt;- (p_1 + p_2)/2 q_t &lt;- 1 - p_t # calculate expected heterozygosity # first calculate expected heterozygosity for the two populations # pop1 hs_1 &lt;- 2*p_1*q_1 # pop2 hs_2 &lt;- 2*p_2*q_2 # then take the mean of this hs &lt;- (hs_1 + hs_2)/2 # next calculate expected heterozygosity for the metapopulations ht &lt;- 2*p_t*q_t # calculate fst fst &lt;- (ht - hs)/ht # return output return(fst) } Using the calc_p() and calc_fst() functions we developed during the tutorial and the lct_freq data, calculate \\(F_{ST}\\) between the Han_China and the Swedish_and_Finnish_Scandinavia populations. What might be a biological explanation of the \\(F_{ST}\\) value you calculate? Hint: think about what the LCT gene does, and the geographical patterns of lactose intolerance. Using the functions we developed in the tutorial, calculate \\(F_{ST}\\) around the LCT gene between Americans of European descent and also between African Americans. Plot this like we plotted the \\(F_{ST}\\) between European Americans and East Asians. What is the highest value of \\(F_{ST}\\)? How does this compare with the highest \\(F_{ST}\\) between European Americans and East Asians that we investigated in the tutorial? "],["w07.html", "Week 6-7 assignment 5", " Week 6-7 assignment 5 1. Visualising complex data This part of the assignment uses the iris data set which is already available in R. Make a scatterplot of Sepal.Length against Sepal.Width. Facet the plot by Species, and arrange the plots so that they are below each other. Color the points by Petal.Length Optional Let’s try to visualise all the iris data at once. Use pivot_longer() to get all measurements (i.e. all columns except Species) in a single column. Make a boxplot of species against measurement value, and facet the plot by measurement type. 2. Working with DNA sequence data For this part of the assignment, you need to download the assignment-7.fasta file here. Import the data and view the sequences in R. How long are the sequences, and what are the base frequencies? Calculate standardised segregating sites and nucleotide diversity of the data. Describe the two file formats: FASTA and VCF 3. The woodmouse data How many segregating sites (i.e. actual number) are there if we subset the woodmouse dataset to 12 individuals and 500 basepairs? What is the nucleotide diversity? The Tajima’s D test we performed on the woodmouse data is not significantly different from zero. However, for our purposes here, imagine that it is. What was the value of Tajima’s D, and how do you interpret this? There are several factors influencing the sign of Tajima’s D, and you need to argue for which you think is most likely in this specific case. Recall that the data is from mitochondrial DNA (believed to be neutral) and was used in a study of the woodmouse’s demographic history since the last ice age, where it seems likely that they survived in a refugia in Southern Europe and then recolonised Northern Europe following ice retreat. 4. Sparrows For this part of the assignment you will work with the sparrow dataset that you have already worked with during the tutorial, you can continue with the sparrows dataframe that you have in your environment or download the data here(VCF) and here(population data). See section XX in the tutorial on how to download and import the data. In your own words, what is a sliding window analysis? Why did we need to use a sliding window analysis to visualise this data? Plot the sliding window nucleotide diversity for both the spanish sparrow (using red) and the italian sparrow (using gold). Do you see the same region of reduced diversity as with the house sparrow? If there is a shared pattern, what might explain it? Calculate the mean pairwise Fst and also the mean pairwise dxy for all of the different species comparisons. N.B. if you use a tidyverse solution, it may be easier to use t() to transpose and see the final result Use a boxplot to visualise the distribution of \\(F_{ST}\\) and \\(d_{XY}\\) for all the different species comparisons. Hint: You need to use pivot_longer on the columns containing \\(F_{ST}\\) and \\(d_{XY}\\) data respectively. Draw a composite plot (fst, nucleotide diversity, dxy) for the house versus italian comparison. Is there a similar pattern to the house/spanish data we examined in the tutorial? Plot the relationship between \\(d_{XY}\\) for house vs spanish and recombination rate (similar to the very last plot in the tutorial (NOT the one in the footnotes!), but with \\(d_{XY}\\) instead of \\(F_{ST}\\)). Is it the same as that for Fst? Explain why/why not. "],["w09.html", "Week 8 assignment 6", " Week 8 assignment 6 1. Working with trees Draw a random tree with 23 taxa. Make it a radial (or “fan”) tree, with an edge width of 1.5. It must be any colour other than black. Tip: for naming the tree tips, check out the letters object that is built into R Extract the Proaves clade from the bird.orders dataset and plot it. Then, test whether ducks and geese (Anseriformes) and chickens and turkeys (Galliformes) are monophyletic. Hint: Find out which node that represents the last common ancestor of the Proaves. 2. Primates In the tutorial, we worked with a subset (hominidae) of the primates data set. In this part of the assignment you will do the same analyses with the full data set. Using the full primates dataset, calculate the pairwise distance matrix using the most suitable sequence model. Then, construct both UPGMA and NJ trees and plot them. Root the NJ tree with Mouse as an outgroup. What is the parsimony of the rooted NJ tree and the UPGMA tree, and which is most parsimonous? Look at the placement of Bovine in the rooted NJ tree. Is it what you would expect? What can explain this placement? 3. Village dogs PCA From the eigenvalues generated by our dogs_pca (i.e. the subset we worked with in the tutorial), how many principal components should we consider to account for approximately 10% of the variance? For the rest of the assignment, we will be working with the PCA results and eigenvalues from the full village dogs data set. The PCA can be found here. The eigenvalues can be found here. Plot the PCA for the full village dogs data set, colouring the points by location. Which group is the most divergent? What other patterns can you see in the PCA? To read in the eigenvalues, try using the scan() function. If you open the file, you can see that it’s just a plain text file with a single column of values, so read.table() could also work. What proportion of the variance do the first two principal components explain? "],["w10.html", "Week 9 assignment 7", " Week 9 assignment 7 These study questions are meant only to test yourself. Hand-in is not required and will not count on the required number of approved hands-in. We will, however, provide a hand-in folder for those who would like feedback from one of the group teachers This assignment makes you apply a lot of your R knowledge to analyse data. Some parts may be challenging, and the most important thing this week is that you try your best! So take this as a learning opportunity, and don’t fret if you’re stuck on an exercise. All honest attempts at solving this assignment will be approved. Good luck! 1. Categorising and joining Once again, we will be using the data in worlddata.csv. Read in the data first, and do the exercises below. The world is sometimes divided into “The West” (Europe and North America) and “The Rest” (which is not necessarily constructive nor useful). In this part of the assignment we will investigate this division for a bit. Use (nested) ifelse() statements to create a new column that says “The West” if the country is a part of Europe or North America, and “The Rest” if not. Plot life expectancy at birth against total fertility rate, and color the points by the column you created in a. If you weren’t able to complete a., color by continent instead. The Gapminder foundation aims to debunk common misconceptions about the world using a fact-based worldview and excellent interactive data visualisations. The foundation collects a lot of population data on their website, which we will use some of here. Download the file gdp_per_capita.csv and import it into R (Tip: some country names have commas in them, which might confuse read.table() a bit. Try using read.csv() instead). The file contains GDP per capita (in inflation-adjusted dollars) for the countries of the world for 2020. Note that the GDP data has slightly fewer countries than the world data, and that some countries may not be named in exactly the same way. We’ll ignore this for the purpose of this exercise and let R handle the mismatches. Use left_join() to join the GDP data to the world data. Remember that since you’re adding to the world data, that should be the first argument. Print your new data frame to check that it worked. Make a plot of life expectancy against GDP, color the points by continent, map size to population size, and map shape to the “West” and “Rest” column you made in a. Is the West/Rest division helpful for determining patterns? Tip: add scale_x_log10() to your ggplot to get GDP on a logarithmic axis, the patterns may be easier to see that way. If you weren’t able to complete the previous exercises, download this file and plot that instead (the columns are called “gdp” and “Division”). If you are interested in learning more, take a look at the previously mentioned interactive visualisations. Look how much the world has changed the past 200 years! 2. Vectorisation In this part of the assignment, we will use vectorisation to investigate the development of the word’s countries since 1900 until today. But first a few vectorisation assignments! Create a function that takes a vector as an argument, and returns the last element of the vector divided by the first. For example, when using the function on the vector c(2, 4, 10), it should return 5, because \\(\\frac{10}{2} = 5\\). Hint: use x[length(x)] to get the last element of a vector x . For the next assignment, run the following code to create an example list: numbers &lt;- list( c(1,2,3), c(14, 36, 60, 78), c(40, 700, 1000) ) Use lapply() or sapply() to apply the function you created in a. to numbers. For the next part of the assignment, download the file gdp_year.rds. This data is also from Gapminder, and shows the development in GDP per capita from 1900 to 2020 by country. Load the data with readRDS(): data &lt;- readRDS(&quot;gdp_year.rds&quot;) If you look at the data, you will see that it is a list, with each list element representing a country, and containing a vector of GDPs per capita, one for each year. Use sapply() to apply the function you made in a. to the data you just loaded, to get each country’s relative GDP growth from 1900 until today. The result should be a named vector with a single number per country. Next, convert your vector to a data frame with the following code (assuming your vector is named gdp_change): gdp_df &lt;- data.frame(gdp_change) %&gt;% rownames_to_column(&quot;Country&quot;) Use left_join() to join gdp_df with the world data. Make a boxplot of GDP change by continent. Is there anything in particular you find interesting or surprising with this plot? if you aren’t able to join, download this data and plot that instead. "],["rmarkdown.html", "A About assignments and RMarkdown", " A About assignments and RMarkdown There are some important things to consider when handing in the assignments: All answers, plots and code should be contained in a single document. This needs to be either a word or a pdf document. The document should contain all the code you used to complete the assignment. It should also contain all relevant output from the code. This should be in a font that’s suitable for programming (i.e., where all characters have the same width), for example “source code pro”. You are allowed to work together, but identical hand-ins are not allowed. Similarly, all the text should be your own, copying from the book, tutorials or any other sources is not allowed. If you really want to quote someone, you should make it clear that it’s a quote, and cite it properly. It is OK to use the code in the tutorials without citing that. One simple way of combining code, plots and text is to write in a word document where you also paste your code and figures that you’ve exported from R. Another way is to use RMarkdown, which will be introduced in the next section. A simple template for handing in assignments in R Markdown can be found here R Markdown R Markdown is a way to work where you can write, code and show output in the same document. It can be a very convenient way to communicate science, and to write comprehensive data journals detailing how you have analysed your data. In fact, these tutorials are all written in R Markdown! Here we will briefly introduce R Markdown so you will be able to use it for the assignments if you want to. For a more comprehensive introduction, see the introduction tutorials. Before using R Markdown, you need to install it. You install it like you would any normal R package. install.packages(&quot;rmarkdown&quot;) When you write R Markdown documents, you write within RStudio, before converting your text and code to either a HTML, PDF or Word document (known as “knitting”). For this course you need to deliver assignments either in PDF or Word format. PDF requires \\(\\LaTeX\\), and can thus be a bit tricky sometimes. The simplest way to download Latex is by installing TinyTex with the following code: install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() Now you have all you need for creating PDF documents with R Markdown! Getting started Create a new R Markdown document by navigating to File &gt; New File &gt; R Markdown .... You will then be prompted to provide a title, an author and an output format (you can change these later). Choose PDF as the format and proceed. You have now created an R Markdown file! Notice that it already contains some usage examples. You don’t need these for the assignments, so you can safely delete everything from ## R Markdown and to the end of the document. Here, we will only provide the very basics needed to hand in the assignments. If you’re interested, you can learn more about the basics here. Anything you write in the document will be rendered as normal text. You can write R code by inserting a code chunk. A code chunk looks like this (hotkey ctrl/cmd+alt+i): ```{r} ``` Inside the code chunk, you can write regular R code that you can execute normally with ctrl/cmd+enter. You can get a top-level header by starting a line with # (outside of a code chunk). You get a second level header with ##, third level with ### and so on. You get bold text by flanking the text with two asterisks like this: **bold text** and italic text with a single asterisk: *italic text* When you’re done with the document, you can press the tiny yarnball at the top of the document that says “knit” to render your document (or press ctrl/cmd+shift+k). This will execute all code that is in the code chunks from a clean environment, and include any plots that are produced by your code. If everything worked, you now have a beautiful document containing code, text and plots! Some notes about R Markdown: All your code is run from a clean slate. This means that all objects must be defined within the document to make it work. If you even have a single error in the code, the document will not knit. On the plus side, this can be a good way of checking that all your code works! When working with data files, the data file has to be contained in the same folder as the R Markdown document. Setting working directory within the document will not work most of the time. Plots will appear inline instead of in the plot window. Disable this by navigating to the cog wheel at the top, and select “chunk output in console”. "],["tech.html", "B Technical information", " B Technical information These tutorials were made using Rmarkdown version 2.27 and Bookdown version 0.39. All code was executed in R programming language version 4.4.1. The following packages and versions were used for the current tutorials: Table B.1: Packages needed to run all the tutorials, and the package versions used to knit the current version of the tutorials. Package Version adegenet 2.1.10 ape 5.8 parallel 4.4.1 pegas 1.3 phangorn 2.11.1 PopGenome 2.7.5 tidyverse 2.0.0 The specific versions of the tidyverse packages are outlined below: Table B.2: Package versions of core Tidyverse packages Package Version ggplot2 3.5.1 dplyr 1.1.4 tidyr 1.3.1 readr 2.1.5 purrr 1.0.2 tibble 3.2.1 stringr 1.5.1 forcats 1.0.0 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
